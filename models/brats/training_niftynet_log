INFO:niftynet:2019-02-12 13:01:03,779: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 13:01:03,779: starting segmentation application
INFO:niftynet:2019-02-12 13:01:03,779: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:01:03,785: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:01:03,789: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:01:03,793: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 13:01:03,796: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 13:01:03,799: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 13:01:24,451: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:01:24,451: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:01:26,435: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:01:26,436: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:01:26,442: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:01:26,442: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:01:26,442: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:01:26,443: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:01:28,529: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 13:01:28,570: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 13:01:28,652: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 13:01:28,652: using DenseVNet
INFO:niftynet:2019-02-12 13:01:28,658: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 13:01:28,699: Initialising Dataset from 29 subjects...
INFO:niftynet:2019-02-12 13:05:14,721: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 13:05:14,721: starting segmentation application
INFO:niftynet:2019-02-12 13:05:14,722: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:05:14,727: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:05:14,732: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:05:14,735: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 13:05:14,739: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 13:05:14,741: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 13:06:43,452: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 13:06:43,452: starting segmentation application
INFO:niftynet:2019-02-12 13:06:43,452: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:06:43,458: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:06:43,462: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:06:43,466: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 13:06:43,469: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 13:06:43,472: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 13:07:02,637: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:07:02,637: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:07:05,436: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:07:05,436: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:07:05,439: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:07:05,439: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:07:05,439: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:07:05,439: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:07:07,542: initialised uniform sampler {'image': (1, 128, 128, 16, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 16, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 13:07:07,598: initialised uniform sampler {'image': (1, 128, 128, 16, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 16, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 13:07:07,715: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 13:07:07,716: using DenseVNet
INFO:niftynet:2019-02-12 13:07:07,723: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 13:07:07,774: Initialising Dataset from 29 subjects...
INFO:niftynet:2019-02-12 13:07:17,890: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 13:09:35,394: training iter 1, loss=0.8526108860969543 (137.503414s)
INFO:niftynet:2019-02-12 13:09:53,796: training iter 2, loss=0.7362685799598694 (18.401984s)
INFO:niftynet:2019-02-12 13:10:06,059: training iter 3, loss=0.7749817371368408 (12.262263s)
INFO:niftynet:2019-02-12 13:10:23,881: training iter 4, loss=0.7782140970230103 (17.821567s)
INFO:niftynet:2019-02-12 13:10:38,563: training iter 5, loss=0.7633333206176758 (14.681520s)
INFO:niftynet:2019-02-12 13:10:55,929: training iter 6, loss=0.7658332586288452 (17.363111s)
INFO:niftynet:2019-02-12 13:11:10,875: training iter 7, loss=0.7609305381774902 (14.945404s)
INFO:niftynet:2019-02-12 13:11:24,714: training iter 8, loss=0.7508697509765625 (13.838788s)
INFO:niftynet:2019-02-12 13:11:41,226: training iter 9, loss=0.7668816447257996 (16.511718s)
INFO:niftynet:2019-02-12 13:11:55,491: training iter 10, loss=0.7435734868049622 (14.263677s)
INFO:niftynet:2019-02-12 13:12:48,559:     validation iter 10, loss=0.7540467977523804 (53.066835s)
INFO:niftynet:2019-02-12 13:12:49,372: training iter 11, loss=0.7520679831504822 (0.811494s)
INFO:niftynet:2019-02-12 13:12:50,001: training iter 12, loss=0.7658065557479858 (0.629082s)
INFO:niftynet:2019-02-12 13:13:02,517: training iter 13, loss=0.7412770986557007 (12.515388s)
INFO:niftynet:2019-02-12 13:13:22,847: training iter 14, loss=0.7634403109550476 (20.329701s)
INFO:niftynet:2019-02-12 13:13:45,184: training iter 15, loss=0.711129903793335 (22.336597s)
INFO:niftynet:2019-02-12 13:13:57,816: training iter 16, loss=0.7259126901626587 (12.631244s)
INFO:niftynet:2019-02-12 13:14:16,476: training iter 17, loss=0.7740017175674438 (18.657519s)
INFO:niftynet:2019-02-12 13:14:29,812: training iter 18, loss=0.7447811365127563 (13.335354s)
INFO:niftynet:2019-02-12 13:14:49,343: training iter 19, loss=0.7230576276779175 (19.530118s)
INFO:niftynet:2019-02-12 13:15:02,380: training iter 20, loss=0.76850825548172 (13.036724s)
INFO:niftynet:2019-02-12 13:15:02,888:     validation iter 20, loss=0.7712270021438599 (0.506670s)
INFO:niftynet:2019-02-12 13:15:23,471: training iter 21, loss=0.7796862721443176 (20.582284s)
INFO:niftynet:2019-02-12 13:15:37,820: training iter 22, loss=0.7270865440368652 (14.349535s)
INFO:niftynet:2019-02-12 13:15:55,902: training iter 23, loss=0.730498194694519 (18.081213s)
INFO:niftynet:2019-02-12 13:16:11,465: training iter 24, loss=0.7363908290863037 (15.562531s)
INFO:niftynet:2019-02-12 13:16:26,393: iter 25 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 13:16:26,393: training iter 25, loss=0.7381641268730164 (13.859755s)
INFO:niftynet:2019-02-12 13:16:44,498: training iter 26, loss=0.7370638251304626 (18.104540s)
INFO:niftynet:2019-02-12 13:16:58,829: training iter 27, loss=0.7126725912094116 (14.331396s)
INFO:niftynet:2019-02-12 13:17:18,496: training iter 28, loss=0.6962343454360962 (19.666021s)
INFO:niftynet:2019-02-12 13:17:32,633: training iter 29, loss=0.7359472513198853 (14.136835s)
INFO:niftynet:2019-02-12 13:17:47,524: training iter 30, loss=0.7179832458496094 (14.889889s)
INFO:niftynet:2019-02-12 13:17:48,559:     validation iter 30, loss=0.7408786416053772 (1.033846s)
INFO:niftynet:2019-02-12 13:18:05,010: training iter 31, loss=0.6680735349655151 (16.449749s)
INFO:niftynet:2019-02-12 13:18:22,520: training iter 32, loss=0.7076650261878967 (17.508898s)
INFO:niftynet:2019-02-12 13:18:36,228: training iter 33, loss=0.7521389126777649 (13.706910s)
INFO:niftynet:2019-02-12 13:18:54,971: training iter 34, loss=0.7845412492752075 (18.742098s)
INFO:niftynet:2019-02-12 13:19:07,956: training iter 35, loss=0.7190318703651428 (12.984348s)
INFO:niftynet:2019-02-12 13:19:26,250: training iter 36, loss=0.6854739785194397 (18.292371s)
INFO:niftynet:2019-02-12 13:19:40,419: training iter 37, loss=0.7273181676864624 (14.169143s)
INFO:niftynet:2019-02-12 13:19:57,194: training iter 38, loss=0.7249377965927124 (16.774935s)
INFO:niftynet:2019-02-12 13:20:14,851: training iter 39, loss=0.6850706338882446 (17.656232s)
INFO:niftynet:2019-02-12 13:20:27,514: training iter 40, loss=0.7159008383750916 (12.662514s)
INFO:niftynet:2019-02-12 13:20:27,982:     validation iter 40, loss=0.6576014757156372 (0.465516s)
INFO:niftynet:2019-02-12 13:20:47,762: training iter 41, loss=0.7528911232948303 (19.779444s)
INFO:niftynet:2019-02-12 13:21:00,584: training iter 42, loss=0.7178981304168701 (12.821384s)
INFO:niftynet:2019-02-12 13:21:19,696: training iter 43, loss=0.7453035116195679 (19.111848s)
INFO:niftynet:2019-02-12 13:21:32,690: training iter 44, loss=0.6922754645347595 (12.993960s)
INFO:niftynet:2019-02-12 13:21:48,521: training iter 45, loss=0.7740108370780945 (15.828479s)
INFO:niftynet:2019-02-12 13:22:02,294: training iter 46, loss=0.7528854608535767 (13.768444s)
INFO:niftynet:2019-02-12 13:22:18,839: training iter 47, loss=0.7014317512512207 (16.544797s)
INFO:niftynet:2019-02-12 13:22:32,792: training iter 48, loss=0.7391937971115112 (13.952893s)
INFO:niftynet:2019-02-12 13:22:48,434: training iter 49, loss=0.6624129414558411 (15.641161s)
INFO:niftynet:2019-02-12 13:23:03,083: iter 50 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 13:23:03,083: training iter 50, loss=0.6960006356239319 (13.784008s)
INFO:niftynet:2019-02-12 13:23:03,497:     validation iter 50, loss=0.694789707660675 (0.412808s)
INFO:niftynet:2019-02-12 13:23:19,519: training iter 51, loss=0.7103945016860962 (16.021306s)
INFO:niftynet:2019-02-12 13:23:33,620: training iter 52, loss=0.7573009729385376 (14.100936s)
INFO:niftynet:2019-02-12 13:23:51,537: training iter 53, loss=0.7249051928520203 (17.917342s)
INFO:niftynet:2019-02-12 13:24:04,909: training iter 54, loss=0.7204445004463196 (13.371310s)
INFO:niftynet:2019-02-12 13:24:22,938: training iter 55, loss=0.7769712209701538 (18.028722s)
INFO:niftynet:2019-02-12 13:24:35,892: training iter 56, loss=0.7151964902877808 (12.952411s)
INFO:niftynet:2019-02-12 13:24:53,377: training iter 57, loss=0.741553008556366 (17.485118s)
INFO:niftynet:2019-02-12 13:25:06,595: training iter 58, loss=0.7705320715904236 (13.217345s)
INFO:niftynet:2019-02-12 13:25:26,206: training iter 59, loss=0.7627586126327515 (19.611363s)
INFO:niftynet:2019-02-12 13:25:39,030: training iter 60, loss=0.7087891697883606 (12.822536s)
INFO:niftynet:2019-02-12 13:25:39,407:     validation iter 60, loss=0.7365139126777649 (0.374634s)
INFO:niftynet:2019-02-12 13:25:59,890: training iter 61, loss=0.6883839964866638 (20.481236s)
INFO:niftynet:2019-02-12 13:26:15,196: training iter 62, loss=0.715344250202179 (15.305806s)
INFO:niftynet:2019-02-12 13:26:33,619: training iter 63, loss=0.7349950671195984 (18.421968s)
INFO:niftynet:2019-02-12 13:26:48,479: training iter 64, loss=0.698898434638977 (14.860157s)
INFO:niftynet:2019-02-12 13:27:06,158: training iter 65, loss=0.7180558443069458 (17.678040s)
INFO:niftynet:2019-02-12 13:27:19,426: training iter 66, loss=0.7636580467224121 (13.266146s)
INFO:niftynet:2019-02-12 13:27:38,488: training iter 67, loss=0.668933629989624 (19.062508s)
INFO:niftynet:2019-02-12 13:27:53,741: training iter 68, loss=0.7478839755058289 (15.251802s)
INFO:niftynet:2019-02-12 13:28:08,601: training iter 69, loss=0.717467188835144 (14.858767s)
INFO:niftynet:2019-02-12 13:28:25,076: training iter 70, loss=0.6671239137649536 (16.474003s)
INFO:niftynet:2019-02-12 13:28:25,575:     validation iter 70, loss=0.7072035670280457 (0.497018s)
INFO:niftynet:2019-02-12 13:28:41,658: training iter 71, loss=0.7047722339630127 (16.081014s)
INFO:niftynet:2019-02-12 13:28:59,945: training iter 72, loss=0.7101766467094421 (18.287378s)
INFO:niftynet:2019-02-12 13:29:15,271: training iter 73, loss=0.656624972820282 (15.325730s)
INFO:niftynet:2019-02-12 13:29:28,102: training iter 74, loss=0.6753821969032288 (12.830887s)
INFO:niftynet:2019-02-12 13:29:47,376: iter 75 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 13:29:47,377: training iter 75, loss=0.7174094915390015 (18.471186s)
INFO:niftynet:2019-02-12 13:30:01,291: training iter 76, loss=0.7259665727615356 (13.914530s)
INFO:niftynet:2019-02-12 13:30:19,712: training iter 77, loss=0.702722430229187 (18.420421s)
INFO:niftynet:2019-02-12 13:30:34,733: training iter 78, loss=0.6698279976844788 (15.021011s)
INFO:niftynet:2019-02-12 13:30:50,146: training iter 79, loss=0.6535874605178833 (15.412285s)
INFO:niftynet:2019-02-12 13:31:11,499: training iter 80, loss=0.7383700013160706 (21.351305s)
INFO:niftynet:2019-02-12 13:31:11,941:     validation iter 80, loss=0.7159298658370972 (0.440046s)
INFO:niftynet:2019-02-12 13:31:26,783: training iter 81, loss=0.6744394898414612 (14.840822s)
INFO:niftynet:2019-02-12 13:31:46,118: training iter 82, loss=0.7380281686782837 (19.334630s)
INFO:niftynet:2019-02-12 13:32:00,307: training iter 83, loss=0.7187138795852661 (14.188857s)
INFO:niftynet:2019-02-12 13:32:23,528: training iter 84, loss=0.680310070514679 (23.220610s)
INFO:niftynet:2019-02-12 13:32:37,442: training iter 85, loss=0.7159696221351624 (13.913383s)
INFO:niftynet:2019-02-12 13:32:56,957: training iter 86, loss=0.7063361406326294 (19.514074s)
WARNING:niftynet:2019-02-12 13:33:09,917: User cancelled application
INFO:niftynet:2019-02-12 13:33:09,917: cleaning up...
INFO:niftynet:2019-02-12 13:33:09,918: stopping sampling threads
INFO:niftynet:2019-02-12 13:33:10,783: iter 87 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
WARNING:niftynet:2019-02-12 13:33:22,209: stopped early, incomplete iterations.
INFO:niftynet:2019-02-12 13:33:22,210: SegmentationApplication stopped (time in second 1566.03).
INFO:niftynet:2019-02-12 13:34:47,280: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 13:34:47,280: starting segmentation application
INFO:niftynet:2019-02-12 13:34:47,280: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:34:47,288: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:34:47,293: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:34:47,296: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 13:34:47,300: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 13:34:47,303: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 13:35:06,330: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:35:06,330: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:35:08,828: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:35:08,828: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:35:08,835: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:35:08,836: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:35:08,836: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:35:08,836: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:35:10,205: initialised uniform sampler {'image': (1, 96, 96, 96, 1, 3), 'image_location': (1, 7), 'label': (1, 96, 96, 96, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 13:35:10,261: initialised uniform sampler {'image': (1, 96, 96, 96, 1, 3), 'image_location': (1, 7), 'label': (1, 96, 96, 96, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 13:35:10,397: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 13:35:10,405: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 13:35:10,460: Initialising Dataset from 29 subjects...
WARNING:niftynet:2019-02-12 13:35:13,535: trying to use gpu id 1, but only has 1 GPU(s), please set num_gpus to 1 at most
INFO:niftynet:2019-02-12 13:35:20,147: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 13:36:42,238: cleaning up...
INFO:niftynet:2019-02-12 13:36:42,238: stopping sampling threads
INFO:niftynet:2019-02-12 13:39:54,534: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 13:39:54,534: starting segmentation application
INFO:niftynet:2019-02-12 13:39:54,534: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:39:54,541: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:39:54,545: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:39:54,549: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 13:39:54,553: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 13:39:54,555: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 13:40:14,047: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:40:14,048: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:40:17,026: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:40:17,026: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:40:17,029: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:40:17,029: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:40:17,029: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:40:17,029: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:40:18,040: initialised uniform sampler {'image': (1, 96, 96, 96, 1, 3), 'image_location': (1, 7), 'label': (1, 96, 96, 96, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 13:40:18,101: initialised uniform sampler {'image': (1, 96, 96, 96, 1, 3), 'image_location': (1, 7), 'label': (1, 96, 96, 96, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 13:40:18,224: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 13:40:18,232: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 13:40:18,283: Initialising Dataset from 29 subjects...
WARNING:niftynet:2019-02-12 13:40:21,334: trying to use gpu id 1, but only has 1 GPU(s), please set num_gpus to 1 at most
INFO:niftynet:2019-02-12 13:51:08,049: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 13:51:08,049: starting segmentation application
INFO:niftynet:2019-02-12 13:51:08,049: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:51:08,055: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:51:08,059: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:51:08,063: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 13:51:08,066: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 13:51:08,069: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 13:51:27,002: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:51:27,002: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:51:29,472: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:51:29,473: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:51:29,476: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:51:29,476: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:51:29,476: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:51:29,476: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:51:31,531: initialised uniform sampler {'image': (1, 96, 96, 96, 1, 3), 'image_location': (1, 7), 'label': (1, 96, 96, 96, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 13:51:31,621: initialised uniform sampler {'image': (1, 96, 96, 96, 1, 3), 'image_location': (1, 7), 'label': (1, 96, 96, 96, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 13:51:31,717: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 13:51:31,724: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 13:51:31,770: Initialising Dataset from 29 subjects...
WARNING:niftynet:2019-02-12 13:51:34,868: trying to use gpu id 1, but only has 1 GPU(s), please set num_gpus to 1 at most
INFO:niftynet:2019-02-12 13:51:41,630: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 13:53:03,967: cleaning up...
INFO:niftynet:2019-02-12 13:53:05,550: iter 1 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 13:53:05,550: stopping sampling threads
INFO:niftynet:2019-02-12 13:53:43,951: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 13:53:43,951: starting segmentation application
INFO:niftynet:2019-02-12 13:53:43,951: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:53:43,957: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:53:43,961: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 13:53:43,965: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 13:53:43,968: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 13:53:43,971: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 13:53:59,931: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:53:59,931: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:54:02,128: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 13:54:02,129: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 13:54:02,132: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:54:02,132: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:54:02,132: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 13:54:02,132: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 13:54:02,926: initialised uniform sampler {'image': (1, 96, 96, 96, 1, 3), 'image_location': (1, 7), 'label': (1, 96, 96, 96, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 13:54:03,000: initialised uniform sampler {'image': (1, 96, 96, 96, 1, 3), 'image_location': (1, 7), 'label': (1, 96, 96, 96, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 13:54:03,123: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 13:54:03,131: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 13:54:03,184: Initialising Dataset from 29 subjects...
INFO:niftynet:2019-02-12 13:54:09,388: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 13:55:25,310: training iter 1, loss=0.8598549365997314 (75.740531s)
INFO:niftynet:2019-02-12 13:55:26,066: training iter 2, loss=0.8247248530387878 (0.755843s)
INFO:niftynet:2019-02-12 13:55:26,845: training iter 3, loss=0.8693727254867554 (0.778396s)
INFO:niftynet:2019-02-12 13:55:27,601: training iter 4, loss=0.8877744674682617 (0.754489s)
INFO:niftynet:2019-02-12 13:55:31,924: training iter 5, loss=0.7989850640296936 (4.321479s)
INFO:niftynet:2019-02-12 13:55:32,680: training iter 6, loss=0.838905394077301 (0.754722s)
INFO:niftynet:2019-02-12 13:55:33,466: training iter 7, loss=0.6606459021568298 (0.786486s)
INFO:niftynet:2019-02-12 13:55:34,225: training iter 8, loss=0.8546999096870422 (0.757791s)
INFO:niftynet:2019-02-12 13:55:34,983: training iter 9, loss=0.7160682678222656 (0.756230s)
INFO:niftynet:2019-02-12 13:55:35,746: training iter 10, loss=0.8306346535682678 (0.763298s)
INFO:niftynet:2019-02-12 13:56:22,294:     validation iter 10, loss=0.7709409594535828 (46.547413s)
INFO:niftynet:2019-02-12 13:56:23,085: training iter 11, loss=0.8089224696159363 (0.764245s)
INFO:niftynet:2019-02-12 13:56:26,718: training iter 12, loss=0.7994893193244934 (3.631277s)
INFO:niftynet:2019-02-12 13:56:34,270: training iter 13, loss=0.7763266563415527 (7.550150s)
INFO:niftynet:2019-02-12 13:56:35,032: training iter 14, loss=0.8686975240707397 (0.761403s)
INFO:niftynet:2019-02-12 13:56:35,799: training iter 15, loss=0.8109084963798523 (0.766386s)
INFO:niftynet:2019-02-12 13:56:36,575: training iter 16, loss=0.660896897315979 (0.774095s)
INFO:niftynet:2019-02-12 13:56:37,350: training iter 17, loss=0.7803120613098145 (0.774654s)
INFO:niftynet:2019-02-12 13:56:38,150: training iter 18, loss=0.717727541923523 (0.788017s)
INFO:niftynet:2019-02-12 13:56:38,913: training iter 19, loss=0.8177887797355652 (0.761973s)
INFO:niftynet:2019-02-12 13:56:39,730: training iter 20, loss=0.7605018019676208 (0.765332s)
INFO:niftynet:2019-02-12 13:56:39,947:     validation iter 20, loss=0.8785493969917297 (0.214562s)
INFO:niftynet:2019-02-12 13:56:40,703: training iter 21, loss=0.844599723815918 (0.755725s)
INFO:niftynet:2019-02-12 13:56:48,390: training iter 22, loss=0.7762661576271057 (7.685079s)
INFO:niftynet:2019-02-12 13:56:49,147: training iter 23, loss=0.8379790782928467 (0.756227s)
INFO:niftynet:2019-02-12 13:56:56,363: training iter 24, loss=0.7672972679138184 (7.214669s)
INFO:niftynet:2019-02-12 13:57:01,770: iter 25 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 13:57:01,815: training iter 25, loss=0.8018926382064819 (0.965025s)
INFO:niftynet:2019-02-12 13:57:02,594: training iter 26, loss=0.7863348722457886 (0.778872s)
INFO:niftynet:2019-02-12 13:57:03,395: training iter 27, loss=0.7827857732772827 (0.799016s)
INFO:niftynet:2019-02-12 13:57:04,163: training iter 28, loss=0.8163825869560242 (0.758400s)
INFO:niftynet:2019-02-12 13:57:04,922: training iter 29, loss=0.7994138598442078 (0.758705s)
INFO:niftynet:2019-02-12 13:57:05,687: training iter 30, loss=0.6514971852302551 (0.763939s)
INFO:niftynet:2019-02-12 13:57:05,894:     validation iter 30, loss=0.7317593097686768 (0.207230s)
INFO:niftynet:2019-02-12 13:57:06,666: training iter 31, loss=0.7903258800506592 (0.770356s)
INFO:niftynet:2019-02-12 13:57:07,423: training iter 32, loss=0.8778808116912842 (0.755817s)
INFO:niftynet:2019-02-12 13:57:08,187: training iter 33, loss=0.7670031785964966 (0.762614s)
INFO:niftynet:2019-02-12 13:57:08,956: training iter 34, loss=0.7647300958633423 (0.767766s)
INFO:niftynet:2019-02-12 13:57:09,712: training iter 35, loss=0.7782384753227234 (0.755073s)
INFO:niftynet:2019-02-12 13:57:10,467: training iter 36, loss=0.8411819934844971 (0.755326s)
INFO:niftynet:2019-02-12 13:57:11,222: training iter 37, loss=0.7596590518951416 (0.754797s)
INFO:niftynet:2019-02-12 13:57:11,978: training iter 38, loss=0.7894749045372009 (0.755274s)
INFO:niftynet:2019-02-12 13:57:12,732: training iter 39, loss=0.8386586904525757 (0.753465s)
INFO:niftynet:2019-02-12 13:57:13,489: training iter 40, loss=0.7683721780776978 (0.757196s)
INFO:niftynet:2019-02-12 13:57:13,699:     validation iter 40, loss=0.7560715079307556 (0.209487s)
INFO:niftynet:2019-02-12 13:57:14,451: training iter 41, loss=0.8301796913146973 (0.751789s)
INFO:niftynet:2019-02-12 13:57:15,207: training iter 42, loss=0.7994623780250549 (0.755643s)
INFO:niftynet:2019-02-12 13:57:15,972: training iter 43, loss=0.7981462478637695 (0.764226s)
INFO:niftynet:2019-02-12 13:57:16,734: training iter 44, loss=0.7023309469223022 (0.760850s)
INFO:niftynet:2019-02-12 13:57:17,489: training iter 45, loss=0.897994875907898 (0.753964s)
INFO:niftynet:2019-02-12 13:57:18,246: training iter 46, loss=0.718771755695343 (0.756609s)
INFO:niftynet:2019-02-12 13:57:19,001: training iter 47, loss=0.7998670339584351 (0.753721s)
INFO:niftynet:2019-02-12 13:57:19,757: training iter 48, loss=0.986902117729187 (0.755433s)
INFO:niftynet:2019-02-12 13:57:20,542: training iter 49, loss=0.8062574863433838 (0.784338s)
INFO:niftynet:2019-02-12 13:57:22,868: iter 50 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 13:57:22,870: training iter 50, loss=0.723232626914978 (0.756052s)
INFO:niftynet:2019-02-12 13:57:23,079:     validation iter 50, loss=0.7842559218406677 (0.207599s)
INFO:niftynet:2019-02-12 13:57:23,834: training iter 51, loss=0.8086952567100525 (0.755326s)
INFO:niftynet:2019-02-12 13:57:24,590: training iter 52, loss=0.7627127766609192 (0.755278s)
INFO:niftynet:2019-02-12 13:57:25,380: training iter 53, loss=0.6871916055679321 (0.788146s)
INFO:niftynet:2019-02-12 13:57:26,137: training iter 54, loss=0.7784134745597839 (0.756068s)
INFO:niftynet:2019-02-12 13:57:26,892: training iter 55, loss=0.77235347032547 (0.754788s)
INFO:niftynet:2019-02-12 13:57:27,648: training iter 56, loss=0.7844290137290955 (0.755579s)
INFO:niftynet:2019-02-12 13:57:28,406: training iter 57, loss=0.7830091118812561 (0.756188s)
INFO:niftynet:2019-02-12 13:57:29,164: training iter 58, loss=0.8171970844268799 (0.757793s)
INFO:niftynet:2019-02-12 13:57:29,921: training iter 59, loss=0.7742760181427002 (0.756584s)
INFO:niftynet:2019-02-12 13:57:30,674: training iter 60, loss=0.760738730430603 (0.752451s)
INFO:niftynet:2019-02-12 13:57:30,879:     validation iter 60, loss=0.7615739107131958 (0.205269s)
INFO:niftynet:2019-02-12 13:57:31,636: training iter 61, loss=0.7654764652252197 (0.756601s)
INFO:niftynet:2019-02-12 13:57:32,390: training iter 62, loss=0.7563034892082214 (0.753550s)
INFO:niftynet:2019-02-12 13:57:33,146: training iter 63, loss=0.7574565410614014 (0.755418s)
INFO:niftynet:2019-02-12 13:57:33,902: training iter 64, loss=0.7553588151931763 (0.755314s)
INFO:niftynet:2019-02-12 13:57:34,661: training iter 65, loss=0.7587712407112122 (0.757448s)
INFO:niftynet:2019-02-12 13:57:35,415: training iter 66, loss=0.7600316405296326 (0.754082s)
INFO:niftynet:2019-02-12 13:57:36,171: training iter 67, loss=0.7530323266983032 (0.755124s)
INFO:niftynet:2019-02-12 13:57:36,927: training iter 68, loss=0.7534017562866211 (0.755120s)
INFO:niftynet:2019-02-12 13:57:37,685: training iter 69, loss=0.7490607500076294 (0.755982s)
INFO:niftynet:2019-02-12 13:57:38,444: training iter 70, loss=0.8713392019271851 (0.757897s)
INFO:niftynet:2019-02-12 13:57:38,656:     validation iter 70, loss=0.6973479390144348 (0.210990s)
INFO:niftynet:2019-02-12 13:57:39,410: training iter 71, loss=0.8529547452926636 (0.753765s)
INFO:niftynet:2019-02-12 13:57:40,188: training iter 72, loss=0.7539477348327637 (0.776941s)
INFO:niftynet:2019-02-12 13:57:40,947: training iter 73, loss=0.7553758025169373 (0.758559s)
INFO:niftynet:2019-02-12 13:57:41,710: training iter 74, loss=0.7557035088539124 (0.755682s)
INFO:niftynet:2019-02-12 13:57:45,012: iter 75 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 13:57:45,012: training iter 75, loss=0.7644076347351074 (1.123456s)
INFO:niftynet:2019-02-12 13:57:45,780: training iter 76, loss=0.7710558176040649 (0.767025s)
INFO:niftynet:2019-02-12 13:57:46,540: training iter 77, loss=0.7608586549758911 (0.760210s)
INFO:niftynet:2019-02-12 13:57:47,302: training iter 78, loss=0.7595318555831909 (0.761147s)
INFO:niftynet:2019-02-12 13:57:48,059: training iter 79, loss=0.7546947598457336 (0.756416s)
INFO:niftynet:2019-02-12 13:57:48,815: training iter 80, loss=0.7522426843643188 (0.756195s)
INFO:niftynet:2019-02-12 13:57:49,023:     validation iter 80, loss=0.7292876243591309 (0.207057s)
INFO:niftynet:2019-02-12 13:57:52,163: training iter 81, loss=0.7299991250038147 (3.139753s)
INFO:niftynet:2019-02-12 13:57:53,166: training iter 82, loss=0.7550502419471741 (1.001955s)
INFO:niftynet:2019-02-12 13:57:56,562: training iter 83, loss=0.7344880104064941 (3.395490s)
INFO:niftynet:2019-02-12 13:57:57,319: training iter 84, loss=0.7528567314147949 (0.756562s)
INFO:niftynet:2019-02-12 13:57:58,097: training iter 85, loss=0.7565146684646606 (0.775772s)
INFO:niftynet:2019-02-12 13:58:00,038: training iter 86, loss=0.753318190574646 (1.941334s)
INFO:niftynet:2019-02-12 13:58:00,795: training iter 87, loss=0.7329844236373901 (0.755926s)
INFO:niftynet:2019-02-12 13:58:01,562: training iter 88, loss=0.7517601251602173 (0.767351s)
INFO:niftynet:2019-02-12 13:58:04,217: training iter 89, loss=0.7547622919082642 (2.654142s)
INFO:niftynet:2019-02-12 13:58:04,977: training iter 90, loss=0.7325785160064697 (0.758696s)
INFO:niftynet:2019-02-12 13:58:05,192:     validation iter 90, loss=0.7190371751785278 (0.214158s)
INFO:niftynet:2019-02-12 13:58:07,867: training iter 91, loss=0.7542816996574402 (2.674277s)
INFO:niftynet:2019-02-12 13:58:11,298: training iter 92, loss=0.7550662755966187 (3.430368s)
INFO:niftynet:2019-02-12 13:58:12,078: training iter 93, loss=0.7374922037124634 (0.779935s)
INFO:niftynet:2019-02-12 13:58:12,834: training iter 94, loss=0.754210889339447 (0.755163s)
INFO:niftynet:2019-02-12 13:58:13,591: training iter 95, loss=0.7381263971328735 (0.755520s)
INFO:niftynet:2019-02-12 13:58:14,348: training iter 96, loss=0.7518813014030457 (0.756551s)
INFO:niftynet:2019-02-12 13:58:15,105: training iter 97, loss=0.7517021298408508 (0.756363s)
INFO:niftynet:2019-02-12 13:58:16,530: training iter 98, loss=0.7507262229919434 (1.422565s)
INFO:niftynet:2019-02-12 13:58:17,287: training iter 99, loss=0.7314279675483704 (0.756754s)
INFO:niftynet:2019-02-12 13:58:23,636: iter 100 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 13:58:23,636: training iter 100, loss=0.7512256503105164 (5.437027s)
INFO:niftynet:2019-02-12 13:58:23,844:     validation iter 100, loss=0.7518675923347473 (0.207749s)
INFO:niftynet:2019-02-12 13:58:24,630: training iter 101, loss=0.7528471946716309 (0.785357s)
INFO:niftynet:2019-02-12 13:58:25,387: training iter 102, loss=0.751006007194519 (0.756303s)
INFO:niftynet:2019-02-12 13:58:26,143: training iter 103, loss=0.7522006034851074 (0.755927s)
INFO:niftynet:2019-02-12 13:58:26,898: training iter 104, loss=0.7542507648468018 (0.755190s)
INFO:niftynet:2019-02-12 13:58:27,656: training iter 105, loss=0.7519320845603943 (0.756836s)
INFO:niftynet:2019-02-12 13:58:28,921: training iter 106, loss=0.7512955665588379 (1.264912s)
INFO:niftynet:2019-02-12 13:58:29,680: training iter 107, loss=0.7198479175567627 (0.757684s)
INFO:niftynet:2019-02-12 13:58:30,450: training iter 108, loss=0.7512698769569397 (0.769535s)
INFO:niftynet:2019-02-12 13:58:34,423: training iter 109, loss=0.7526993751525879 (3.970757s)
INFO:niftynet:2019-02-12 13:58:35,181: training iter 110, loss=0.751848578453064 (0.757265s)
INFO:niftynet:2019-02-12 13:58:35,388:     validation iter 110, loss=0.7103739976882935 (0.205898s)
INFO:niftynet:2019-02-12 13:58:38,160: training iter 111, loss=0.7508537173271179 (2.771063s)
INFO:niftynet:2019-02-12 13:58:38,920: training iter 112, loss=0.7531747817993164 (0.759914s)
INFO:niftynet:2019-02-12 13:58:40,610: training iter 113, loss=0.7528802156448364 (1.687920s)
INFO:niftynet:2019-02-12 13:58:41,364: training iter 114, loss=0.7535340785980225 (0.753606s)
INFO:niftynet:2019-02-12 13:58:42,130: training iter 115, loss=0.7528873682022095 (0.765969s)
INFO:niftynet:2019-02-12 13:58:42,888: training iter 116, loss=0.752995491027832 (0.757583s)
INFO:niftynet:2019-02-12 13:58:47,476: training iter 117, loss=0.754732608795166 (4.587529s)
INFO:niftynet:2019-02-12 13:58:48,231: training iter 118, loss=0.7527564764022827 (0.753846s)
INFO:niftynet:2019-02-12 13:58:50,111: training iter 119, loss=0.7537344694137573 (1.879254s)
INFO:niftynet:2019-02-12 13:58:50,920: training iter 120, loss=0.7567595839500427 (0.808486s)
INFO:niftynet:2019-02-12 13:58:51,135:     validation iter 120, loss=0.7526911497116089 (0.215274s)
INFO:niftynet:2019-02-12 13:58:51,895: training iter 121, loss=0.7501920461654663 (0.757889s)
INFO:niftynet:2019-02-12 13:58:53,053: training iter 122, loss=0.7521765232086182 (1.157168s)
INFO:niftynet:2019-02-12 13:58:53,813: training iter 123, loss=0.7114664316177368 (0.757807s)
INFO:niftynet:2019-02-12 13:58:54,567: training iter 124, loss=0.7520332336425781 (0.753902s)
INFO:niftynet:2019-02-12 13:58:59,642: iter 125 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 13:58:59,644: training iter 125, loss=0.7535265684127808 (4.203464s)
INFO:niftynet:2019-02-12 13:59:00,408: training iter 126, loss=0.7540023326873779 (0.753673s)
INFO:niftynet:2019-02-12 13:59:01,162: training iter 127, loss=0.7521976232528687 (0.753342s)
INFO:niftynet:2019-02-12 13:59:03,558: training iter 128, loss=0.7513771653175354 (2.394939s)
INFO:niftynet:2019-02-12 13:59:04,335: training iter 129, loss=0.7510582804679871 (0.776326s)
INFO:niftynet:2019-02-12 13:59:05,095: training iter 130, loss=0.7522671818733215 (0.760212s)
INFO:niftynet:2019-02-12 13:59:05,307:     validation iter 130, loss=0.7407010793685913 (0.210196s)
INFO:niftynet:2019-02-12 13:59:06,118: training iter 131, loss=0.7516690492630005 (0.810857s)
INFO:niftynet:2019-02-12 13:59:06,900: training iter 132, loss=0.7305216193199158 (0.762090s)
INFO:niftynet:2019-02-12 13:59:10,250: training iter 133, loss=0.7211405038833618 (3.348490s)
INFO:niftynet:2019-02-12 13:59:12,237: training iter 134, loss=0.7511008381843567 (1.985987s)
INFO:niftynet:2019-02-12 13:59:13,193: training iter 135, loss=0.7510249614715576 (0.954771s)
INFO:niftynet:2019-02-12 13:59:14,195: training iter 136, loss=0.7513750791549683 (1.001470s)
INFO:niftynet:2019-02-12 13:59:18,427: training iter 137, loss=0.752718448638916 (4.229247s)
INFO:niftynet:2019-02-12 13:59:19,182: training iter 138, loss=0.7516161203384399 (0.754716s)
INFO:niftynet:2019-02-12 13:59:19,943: training iter 139, loss=0.7521447539329529 (0.759924s)
INFO:niftynet:2019-02-12 13:59:20,698: training iter 140, loss=0.7505776286125183 (0.753875s)
INFO:niftynet:2019-02-12 13:59:20,905:     validation iter 140, loss=0.7511356472969055 (0.207072s)
INFO:niftynet:2019-02-12 13:59:23,521: training iter 141, loss=0.7512794137001038 (2.613695s)
INFO:niftynet:2019-02-12 13:59:24,285: training iter 142, loss=0.7403916120529175 (0.764144s)
INFO:niftynet:2019-02-12 13:59:25,042: training iter 143, loss=0.7525572776794434 (0.756228s)
INFO:niftynet:2019-02-12 13:59:25,798: training iter 144, loss=0.7537121772766113 (0.755303s)
INFO:niftynet:2019-02-12 13:59:27,721: training iter 145, loss=0.7520524263381958 (1.921952s)
INFO:niftynet:2019-02-12 13:59:31,558: training iter 146, loss=0.7511578798294067 (3.837366s)
INFO:niftynet:2019-02-12 13:59:32,337: training iter 147, loss=0.751701295375824 (0.778097s)
INFO:niftynet:2019-02-12 13:59:33,100: training iter 148, loss=0.7523728609085083 (0.763381s)
INFO:niftynet:2019-02-12 13:59:33,857: training iter 149, loss=0.7513174414634705 (0.756663s)
INFO:niftynet:2019-02-12 13:59:35,389: iter 150 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 13:59:35,391: training iter 150, loss=0.7502928376197815 (0.760597s)
INFO:niftynet:2019-02-12 13:59:35,599:     validation iter 150, loss=0.721691370010376 (0.208061s)
INFO:niftynet:2019-02-12 13:59:36,356: training iter 151, loss=0.7511427402496338 (0.755319s)
INFO:niftynet:2019-02-12 13:59:37,115: training iter 152, loss=0.7283657789230347 (0.757959s)
INFO:niftynet:2019-02-12 13:59:40,561: training iter 153, loss=0.7507707476615906 (3.443703s)
INFO:niftynet:2019-02-12 13:59:41,316: training iter 154, loss=0.7510068416595459 (0.754538s)
INFO:niftynet:2019-02-12 13:59:43,439: training iter 155, loss=0.7514620423316956 (2.122520s)
INFO:niftynet:2019-02-12 13:59:45,099: training iter 156, loss=0.7361742854118347 (1.658727s)
INFO:niftynet:2019-02-12 13:59:45,855: training iter 157, loss=0.750864565372467 (0.756322s)
INFO:niftynet:2019-02-12 13:59:46,610: training iter 158, loss=0.7519145011901855 (0.754834s)
INFO:niftynet:2019-02-12 13:59:47,365: training iter 159, loss=0.7345044612884521 (0.754319s)
INFO:niftynet:2019-02-12 13:59:48,123: training iter 160, loss=0.752068281173706 (0.757844s)
INFO:niftynet:2019-02-12 13:59:48,337:     validation iter 160, loss=0.7525909543037415 (0.212082s)
INFO:niftynet:2019-02-12 13:59:49,095: training iter 161, loss=0.7501971125602722 (0.755887s)
INFO:niftynet:2019-02-12 13:59:51,989: training iter 162, loss=0.7548609972000122 (2.894308s)
INFO:niftynet:2019-02-12 13:59:54,500: training iter 163, loss=0.7514244318008423 (2.510605s)
INFO:niftynet:2019-02-12 13:59:55,876: training iter 164, loss=0.7545896768569946 (1.374913s)
INFO:niftynet:2019-02-12 13:59:56,646: training iter 165, loss=0.7524582147598267 (0.769633s)
INFO:niftynet:2019-02-12 13:59:57,403: training iter 166, loss=0.7547174692153931 (0.756065s)
INFO:niftynet:2019-02-12 13:59:58,158: training iter 167, loss=0.7503495216369629 (0.754612s)
INFO:niftynet:2019-02-12 13:59:58,930: training iter 168, loss=0.7490152716636658 (0.771394s)
INFO:niftynet:2019-02-12 14:00:01,723: training iter 169, loss=0.7128227949142456 (2.793199s)
INFO:niftynet:2019-02-12 14:00:03,210: training iter 170, loss=0.737403392791748 (1.485114s)
INFO:niftynet:2019-02-12 14:00:09,377:     validation iter 170, loss=0.7512809634208679 (6.166517s)
INFO:niftynet:2019-02-12 14:00:12,256: training iter 171, loss=0.7350602149963379 (2.876766s)
INFO:niftynet:2019-02-12 14:00:13,014: training iter 172, loss=0.7565526366233826 (0.756858s)
INFO:niftynet:2019-02-12 14:00:13,773: training iter 173, loss=0.7557992935180664 (0.757335s)
INFO:niftynet:2019-02-12 14:00:14,529: training iter 174, loss=0.7326632738113403 (0.756294s)
INFO:niftynet:2019-02-12 14:00:16,650: iter 175 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:00:16,679: training iter 175, loss=0.7540606260299683 (0.755565s)
INFO:niftynet:2019-02-12 14:00:17,439: training iter 176, loss=0.7588213682174683 (0.760022s)
INFO:niftynet:2019-02-12 14:00:18,195: training iter 177, loss=0.7331302165985107 (0.754811s)
INFO:niftynet:2019-02-12 14:00:18,955: training iter 178, loss=0.7535592317581177 (0.758432s)
INFO:niftynet:2019-02-12 14:00:23,138: training iter 179, loss=0.7450308799743652 (4.182882s)
INFO:niftynet:2019-02-12 14:00:23,900: training iter 180, loss=0.6883361339569092 (0.761702s)
INFO:niftynet:2019-02-12 14:00:24,107:     validation iter 180, loss=0.7561241388320923 (0.206488s)
INFO:niftynet:2019-02-12 14:00:24,860: training iter 181, loss=0.756545901298523 (0.753223s)
INFO:niftynet:2019-02-12 14:00:25,617: training iter 182, loss=0.7584421038627625 (0.756730s)
INFO:niftynet:2019-02-12 14:00:26,374: training iter 183, loss=0.7638753056526184 (0.755907s)
INFO:niftynet:2019-02-12 14:00:27,131: training iter 184, loss=0.6983044743537903 (0.756706s)
INFO:niftynet:2019-02-12 14:00:29,603: training iter 185, loss=0.7570195198059082 (2.470474s)
INFO:niftynet:2019-02-12 14:00:30,360: training iter 186, loss=0.7620181441307068 (0.756004s)
INFO:niftynet:2019-02-12 14:00:34,058: training iter 187, loss=0.7609371542930603 (3.698523s)
INFO:niftynet:2019-02-12 14:00:34,966: training iter 188, loss=0.7546818852424622 (0.907637s)
INFO:niftynet:2019-02-12 14:00:35,718: training iter 189, loss=0.762978196144104 (0.751688s)
INFO:niftynet:2019-02-12 14:00:36,485: training iter 190, loss=0.7581290006637573 (0.766398s)
INFO:niftynet:2019-02-12 14:00:36,714:     validation iter 190, loss=0.7594887018203735 (0.227535s)
INFO:niftynet:2019-02-12 14:00:37,501: training iter 191, loss=0.7578541040420532 (0.787052s)
INFO:niftynet:2019-02-12 14:00:38,290: training iter 192, loss=0.7592092156410217 (0.778799s)
INFO:niftynet:2019-02-12 14:00:48,406: training iter 193, loss=0.7548943758010864 (10.115236s)
INFO:niftynet:2019-02-12 14:00:49,168: training iter 194, loss=0.7540922164916992 (0.761514s)
INFO:niftynet:2019-02-12 14:00:49,932: training iter 195, loss=0.7570378184318542 (0.764242s)
INFO:niftynet:2019-02-12 14:00:50,696: training iter 196, loss=0.7557433843612671 (0.760602s)
INFO:niftynet:2019-02-12 14:00:51,454: training iter 197, loss=0.7544922232627869 (0.756732s)
INFO:niftynet:2019-02-12 14:00:52,218: training iter 198, loss=0.7556453943252563 (0.763048s)
INFO:niftynet:2019-02-12 14:00:52,994: training iter 199, loss=0.7514546513557434 (0.773321s)
INFO:niftynet:2019-02-12 14:00:54,722: iter 200 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:00:54,722: training iter 200, loss=0.7506163120269775 (0.757239s)
INFO:niftynet:2019-02-12 14:00:54,930:     validation iter 200, loss=0.7511790990829468 (0.207633s)
INFO:niftynet:2019-02-12 14:00:58,998: training iter 201, loss=0.7228158712387085 (4.066835s)
INFO:niftynet:2019-02-12 14:01:01,158: training iter 202, loss=0.7515043020248413 (2.158801s)
INFO:niftynet:2019-02-12 14:01:02,711: training iter 203, loss=0.7294867038726807 (1.553297s)
INFO:niftynet:2019-02-12 14:01:03,473: training iter 204, loss=0.7522464394569397 (0.761673s)
INFO:niftynet:2019-02-12 14:01:04,229: training iter 205, loss=0.7521494030952454 (0.754954s)
INFO:niftynet:2019-02-12 14:01:04,984: training iter 206, loss=0.7520121335983276 (0.755114s)
INFO:niftynet:2019-02-12 14:01:05,762: training iter 207, loss=0.7517246603965759 (0.777656s)
INFO:niftynet:2019-02-12 14:01:06,517: training iter 208, loss=0.7515808343887329 (0.754246s)
INFO:niftynet:2019-02-12 14:01:07,278: training iter 209, loss=0.7148678302764893 (0.760027s)
INFO:niftynet:2019-02-12 14:01:11,369: training iter 210, loss=0.7530945539474487 (4.090488s)
INFO:niftynet:2019-02-12 14:01:11,579:     validation iter 210, loss=0.7053999900817871 (0.208593s)
INFO:niftynet:2019-02-12 14:01:12,334: training iter 211, loss=0.7527557611465454 (0.754147s)
INFO:niftynet:2019-02-12 14:01:13,089: training iter 212, loss=0.7532631754875183 (0.754571s)
INFO:niftynet:2019-02-12 14:01:15,019: training iter 213, loss=0.7549930214881897 (1.930422s)
INFO:niftynet:2019-02-12 14:01:15,786: training iter 214, loss=0.7344496250152588 (0.765875s)
INFO:niftynet:2019-02-12 14:01:16,548: training iter 215, loss=0.7562681436538696 (0.761280s)
INFO:niftynet:2019-02-12 14:01:17,317: training iter 216, loss=0.7538089752197266 (0.768840s)
INFO:niftynet:2019-02-12 14:01:20,387: training iter 217, loss=0.7378197908401489 (3.057108s)
INFO:niftynet:2019-02-12 14:01:23,457: training iter 218, loss=0.7548575401306152 (3.068865s)
INFO:niftynet:2019-02-12 14:01:24,228: training iter 219, loss=0.7545741200447083 (0.770940s)
INFO:niftynet:2019-02-12 14:01:25,532: training iter 220, loss=0.7543456554412842 (1.303509s)
INFO:niftynet:2019-02-12 14:01:25,765:     validation iter 220, loss=0.7532289028167725 (0.233082s)
INFO:niftynet:2019-02-12 14:01:27,138: training iter 221, loss=0.7542449235916138 (1.371824s)
INFO:niftynet:2019-02-12 14:01:27,948: training iter 222, loss=0.7533032894134521 (0.763849s)
INFO:niftynet:2019-02-12 14:01:28,718: training iter 223, loss=0.7530893087387085 (0.769436s)
INFO:niftynet:2019-02-12 14:01:29,992: training iter 224, loss=0.7516865730285645 (1.269888s)
INFO:niftynet:2019-02-12 14:01:34,460: iter 225 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:01:34,460: training iter 225, loss=0.7531960010528564 (3.471350s)
INFO:niftynet:2019-02-12 14:01:35,915: training iter 226, loss=0.7520473599433899 (1.454913s)
INFO:niftynet:2019-02-12 14:01:36,670: training iter 227, loss=0.7527537941932678 (0.753728s)
INFO:niftynet:2019-02-12 14:01:38,384: training iter 228, loss=0.7527740001678467 (1.713413s)
INFO:niftynet:2019-02-12 14:01:40,575: training iter 229, loss=0.7512110471725464 (2.189732s)
INFO:niftynet:2019-02-12 14:01:41,345: training iter 230, loss=0.7511566281318665 (0.769303s)
INFO:niftynet:2019-02-12 14:01:41,555:     validation iter 230, loss=0.7515673041343689 (0.206510s)
INFO:niftynet:2019-02-12 14:01:42,314: training iter 231, loss=0.7522578239440918 (0.757395s)
INFO:niftynet:2019-02-12 14:01:43,071: training iter 232, loss=0.7512191534042358 (0.755352s)
INFO:niftynet:2019-02-12 14:01:46,069: training iter 233, loss=0.7504724264144897 (2.996758s)
INFO:niftynet:2019-02-12 14:01:46,825: training iter 234, loss=0.7509523630142212 (0.755772s)
INFO:niftynet:2019-02-12 14:01:47,594: training iter 235, loss=0.7504203915596008 (0.768685s)
INFO:niftynet:2019-02-12 14:01:50,060: training iter 236, loss=0.7507369518280029 (2.465911s)
INFO:niftynet:2019-02-12 14:01:51,994: training iter 237, loss=0.7507216334342957 (1.933472s)
INFO:niftynet:2019-02-12 14:01:53,659: training iter 238, loss=0.7503345608711243 (1.664109s)
INFO:niftynet:2019-02-12 14:01:54,413: training iter 239, loss=0.7503506541252136 (0.754384s)
INFO:niftynet:2019-02-12 14:01:55,168: training iter 240, loss=0.7502861618995667 (0.753991s)
INFO:niftynet:2019-02-12 14:01:55,378:     validation iter 240, loss=0.7508928179740906 (0.210171s)
INFO:niftynet:2019-02-12 14:01:58,477: training iter 241, loss=0.7472869157791138 (3.098633s)
INFO:niftynet:2019-02-12 14:01:59,232: training iter 242, loss=0.6832455396652222 (0.754201s)
INFO:niftynet:2019-02-12 14:01:59,989: training iter 243, loss=0.7335572242736816 (0.756449s)
INFO:niftynet:2019-02-12 14:02:01,013: training iter 244, loss=0.7508138418197632 (1.023626s)
INFO:niftynet:2019-02-12 14:02:01,771: training iter 245, loss=0.723158597946167 (0.756711s)
INFO:niftynet:2019-02-12 14:02:04,297: training iter 246, loss=0.7664536237716675 (2.525405s)
INFO:niftynet:2019-02-12 14:02:06,263: training iter 247, loss=0.751474916934967 (1.966051s)
INFO:niftynet:2019-02-12 14:02:07,465: training iter 248, loss=0.7517085671424866 (1.201514s)
INFO:niftynet:2019-02-12 14:02:10,469: training iter 249, loss=0.7664274573326111 (3.003068s)
INFO:niftynet:2019-02-12 14:02:12,224: iter 250 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:02:12,238: training iter 250, loss=0.7711056470870972 (0.756492s)
INFO:niftynet:2019-02-12 14:02:12,452:     validation iter 250, loss=0.7520445585250854 (0.213468s)
INFO:niftynet:2019-02-12 14:02:13,213: training iter 251, loss=0.7528611421585083 (0.760805s)
INFO:niftynet:2019-02-12 14:02:14,990: training iter 252, loss=0.756241500377655 (1.776823s)
INFO:niftynet:2019-02-12 14:02:15,748: training iter 253, loss=0.7530561685562134 (0.757518s)
INFO:niftynet:2019-02-12 14:02:16,543: training iter 254, loss=0.7515562176704407 (0.793612s)
INFO:niftynet:2019-02-12 14:02:17,317: training iter 255, loss=0.7682159543037415 (0.773105s)
INFO:niftynet:2019-02-12 14:02:19,794: training iter 256, loss=0.761888325214386 (2.477238s)
INFO:niftynet:2019-02-12 14:02:20,925: training iter 257, loss=0.7509987354278564 (1.128743s)
INFO:niftynet:2019-02-12 14:02:24,010: training iter 258, loss=0.7554082274436951 (3.083616s)
INFO:niftynet:2019-02-12 14:02:24,773: training iter 259, loss=0.7356049418449402 (0.762638s)
INFO:niftynet:2019-02-12 14:02:25,559: training iter 260, loss=0.753990650177002 (0.785680s)
INFO:niftynet:2019-02-12 14:02:25,788:     validation iter 260, loss=0.7566736340522766 (0.204939s)
INFO:niftynet:2019-02-12 14:02:28,611: training iter 261, loss=0.7617518305778503 (2.821920s)
INFO:niftynet:2019-02-12 14:02:29,383: training iter 262, loss=0.7619748711585999 (0.770891s)
INFO:niftynet:2019-02-12 14:02:30,143: training iter 263, loss=0.7648715972900391 (0.759281s)
INFO:niftynet:2019-02-12 14:02:30,922: training iter 264, loss=0.7575255632400513 (0.776822s)
INFO:niftynet:2019-02-12 14:02:33,052: training iter 265, loss=0.7552476525306702 (2.128919s)
INFO:niftynet:2019-02-12 14:02:35,879: training iter 266, loss=0.7575393319129944 (2.825441s)
INFO:niftynet:2019-02-12 14:02:37,022: training iter 267, loss=0.7553037405014038 (0.947154s)
INFO:niftynet:2019-02-12 14:02:37,778: training iter 268, loss=0.7540159225463867 (0.755690s)
INFO:niftynet:2019-02-12 14:02:39,657: training iter 269, loss=0.7532130479812622 (1.876795s)
INFO:niftynet:2019-02-12 14:02:40,413: training iter 270, loss=0.7544463872909546 (0.755936s)
INFO:niftynet:2019-02-12 14:02:40,625:     validation iter 270, loss=0.7056835889816284 (0.212193s)
INFO:niftynet:2019-02-12 14:02:44,042: training iter 271, loss=0.735889732837677 (3.415846s)
INFO:niftynet:2019-02-12 14:02:44,799: training iter 272, loss=0.7507892847061157 (0.755432s)
INFO:niftynet:2019-02-12 14:02:45,556: training iter 273, loss=0.7532746195793152 (0.755676s)
INFO:niftynet:2019-02-12 14:02:46,822: training iter 274, loss=0.7517412900924683 (1.266395s)
INFO:niftynet:2019-02-12 14:02:48,463: iter 275 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:02:48,463: training iter 275, loss=0.7537409067153931 (0.756737s)
INFO:niftynet:2019-02-12 14:02:49,230: training iter 276, loss=0.7535314559936523 (0.766029s)
INFO:niftynet:2019-02-12 14:02:50,674: training iter 277, loss=0.752768337726593 (1.443689s)
INFO:niftynet:2019-02-12 14:02:52,819: training iter 278, loss=0.751455545425415 (1.980710s)
INFO:niftynet:2019-02-12 14:02:53,575: training iter 279, loss=0.7517155408859253 (0.754207s)
INFO:niftynet:2019-02-12 14:02:57,094: training iter 280, loss=0.7264281511306763 (3.519201s)
INFO:niftynet:2019-02-12 14:02:57,302:     validation iter 280, loss=0.751832902431488 (0.206856s)
INFO:niftynet:2019-02-12 14:02:58,083: training iter 281, loss=0.7471591234207153 (0.779585s)
INFO:niftynet:2019-02-12 14:03:00,484: training iter 282, loss=0.7530075311660767 (2.400096s)
INFO:niftynet:2019-02-12 14:03:01,269: training iter 283, loss=0.8785450458526611 (0.784581s)
INFO:niftynet:2019-02-12 14:03:02,030: training iter 284, loss=0.7537369728088379 (0.760421s)
INFO:niftynet:2019-02-12 14:03:02,787: training iter 285, loss=0.6878738403320312 (0.756860s)
INFO:niftynet:2019-02-12 14:03:04,912: training iter 286, loss=0.7533907294273376 (2.123866s)
INFO:niftynet:2019-02-12 14:03:06,878: training iter 287, loss=0.7522984147071838 (1.964697s)
INFO:niftynet:2019-02-12 14:03:10,233: training iter 288, loss=0.7129127979278564 (3.354117s)
INFO:niftynet:2019-02-12 14:03:10,988: training iter 289, loss=0.7539808750152588 (0.755360s)
INFO:niftynet:2019-02-12 14:03:12,921: training iter 290, loss=0.7297687530517578 (1.931827s)
INFO:niftynet:2019-02-12 14:03:13,128:     validation iter 290, loss=0.7607188820838928 (0.207225s)
INFO:niftynet:2019-02-12 14:03:13,884: training iter 291, loss=0.7037869691848755 (0.755027s)
INFO:niftynet:2019-02-12 14:03:14,641: training iter 292, loss=0.7541196346282959 (0.757464s)
INFO:niftynet:2019-02-12 14:03:15,400: training iter 293, loss=0.7610763907432556 (0.757221s)
INFO:niftynet:2019-02-12 14:03:17,065: training iter 294, loss=0.7583125233650208 (1.665077s)
INFO:niftynet:2019-02-12 14:03:18,744: training iter 295, loss=0.726211667060852 (1.678183s)
INFO:niftynet:2019-02-12 14:03:19,514: training iter 296, loss=0.7018250226974487 (0.768417s)
INFO:niftynet:2019-02-12 14:03:23,862: training iter 297, loss=0.76567542552948 (4.346193s)
INFO:niftynet:2019-02-12 14:03:24,616: training iter 298, loss=0.7044106721878052 (0.754220s)
INFO:niftynet:2019-02-12 14:03:26,322: training iter 299, loss=0.7410334348678589 (1.705541s)
INFO:niftynet:2019-02-12 14:03:28,270: iter 300 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:03:28,272: training iter 300, loss=0.7371103763580322 (0.771097s)
INFO:niftynet:2019-02-12 14:03:28,531:     validation iter 300, loss=0.7638514041900635 (0.207080s)
INFO:niftynet:2019-02-12 14:03:29,291: training iter 301, loss=0.7271215915679932 (0.759948s)
INFO:niftynet:2019-02-12 14:03:30,048: training iter 302, loss=0.7012519836425781 (0.755366s)
INFO:niftynet:2019-02-12 14:03:30,817: training iter 303, loss=0.7618962526321411 (0.769159s)
INFO:niftynet:2019-02-12 14:03:31,578: training iter 304, loss=0.7592586278915405 (0.760755s)
INFO:niftynet:2019-02-12 14:03:35,616: training iter 305, loss=0.7606756687164307 (4.037631s)
INFO:niftynet:2019-02-12 14:03:37,364: training iter 306, loss=0.7572659254074097 (1.746571s)
INFO:niftynet:2019-02-12 14:03:39,515: training iter 307, loss=0.7149642705917358 (2.150776s)
INFO:niftynet:2019-02-12 14:03:40,281: training iter 308, loss=0.7417275905609131 (0.765373s)
INFO:niftynet:2019-02-12 14:03:41,042: training iter 309, loss=0.7577841281890869 (0.759804s)
INFO:niftynet:2019-02-12 14:03:41,810: training iter 310, loss=0.7625258564949036 (0.767597s)
INFO:niftynet:2019-02-12 14:03:42,019:     validation iter 310, loss=0.7545430660247803 (0.208023s)
INFO:niftynet:2019-02-12 14:03:42,794: training iter 311, loss=0.754114031791687 (0.774163s)
INFO:niftynet:2019-02-12 14:03:46,020: training iter 312, loss=0.7559968829154968 (3.225077s)
INFO:niftynet:2019-02-12 14:03:47,467: training iter 313, loss=0.75770103931427 (1.447035s)
INFO:niftynet:2019-02-12 14:03:49,955: training iter 314, loss=0.7296394109725952 (2.487243s)
INFO:niftynet:2019-02-12 14:03:51,768: training iter 315, loss=0.7573882341384888 (1.812433s)
INFO:niftynet:2019-02-12 14:03:52,555: training iter 316, loss=0.7539175152778625 (0.787181s)
INFO:niftynet:2019-02-12 14:03:54,606: training iter 317, loss=0.7558203935623169 (2.050635s)
INFO:niftynet:2019-02-12 14:03:55,366: training iter 318, loss=0.753867506980896 (0.757508s)
INFO:niftynet:2019-02-12 14:03:56,121: training iter 319, loss=0.7531916499137878 (0.755158s)
INFO:niftynet:2019-02-12 14:03:56,883: training iter 320, loss=0.7575218677520752 (0.760953s)
INFO:niftynet:2019-02-12 14:03:57,096:     validation iter 320, loss=0.7536335587501526 (0.213091s)
INFO:niftynet:2019-02-12 14:04:00,227: training iter 321, loss=0.7535830140113831 (3.129110s)
INFO:niftynet:2019-02-12 14:04:00,983: training iter 322, loss=0.7523671984672546 (0.755120s)
INFO:niftynet:2019-02-12 14:04:03,570: training iter 323, loss=0.7516469955444336 (2.585658s)
INFO:niftynet:2019-02-12 14:04:05,160: training iter 324, loss=0.7510280013084412 (1.589220s)
INFO:niftynet:2019-02-12 14:04:06,931: iter 325 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:04:06,932: training iter 325, loss=0.7526904940605164 (0.752271s)
INFO:niftynet:2019-02-12 14:04:07,694: training iter 326, loss=0.727025032043457 (0.761790s)
INFO:niftynet:2019-02-12 14:04:08,652: training iter 327, loss=0.7508357763290405 (0.957799s)
INFO:niftynet:2019-02-12 14:04:09,410: training iter 328, loss=0.752269983291626 (0.756498s)
INFO:niftynet:2019-02-12 14:04:10,167: training iter 329, loss=0.7509506940841675 (0.755988s)
INFO:niftynet:2019-02-12 14:04:11,554: training iter 330, loss=0.7419062256813049 (1.387222s)
INFO:niftynet:2019-02-12 14:04:11,767:     validation iter 330, loss=0.7398784160614014 (0.212396s)
INFO:niftynet:2019-02-12 14:04:16,425: training iter 331, loss=0.7515779733657837 (4.657004s)
INFO:niftynet:2019-02-12 14:04:17,186: training iter 332, loss=0.7511258125305176 (0.760743s)
INFO:niftynet:2019-02-12 14:04:17,971: training iter 333, loss=0.7511285543441772 (0.779553s)
INFO:niftynet:2019-02-12 14:04:20,965: training iter 334, loss=0.7509335279464722 (2.992976s)
INFO:niftynet:2019-02-12 14:04:21,723: training iter 335, loss=0.7513623833656311 (0.757809s)
INFO:niftynet:2019-02-12 14:04:22,501: training iter 336, loss=0.7284653186798096 (0.762730s)
INFO:niftynet:2019-02-12 14:04:23,256: training iter 337, loss=0.7505772113800049 (0.754812s)
INFO:niftynet:2019-02-12 14:04:24,012: training iter 338, loss=0.7511794567108154 (0.756067s)
INFO:niftynet:2019-02-12 14:04:25,566: training iter 339, loss=0.751183271408081 (1.553584s)
INFO:niftynet:2019-02-12 14:04:26,631: training iter 340, loss=0.7509058713912964 (1.064702s)
INFO:niftynet:2019-02-12 14:04:26,847:     validation iter 340, loss=0.7508981227874756 (0.214770s)
INFO:niftynet:2019-02-12 14:04:30,971: training iter 341, loss=0.7517271041870117 (4.124034s)
INFO:niftynet:2019-02-12 14:04:31,728: training iter 342, loss=0.726417064666748 (0.756212s)
INFO:niftynet:2019-02-12 14:04:32,495: training iter 343, loss=0.7512931823730469 (0.765979s)
INFO:niftynet:2019-02-12 14:04:33,259: training iter 344, loss=0.7521548271179199 (0.762938s)
INFO:niftynet:2019-02-12 14:04:34,019: training iter 345, loss=0.7522673606872559 (0.759007s)
INFO:niftynet:2019-02-12 14:04:37,538: training iter 346, loss=0.7519490718841553 (3.518686s)
INFO:niftynet:2019-02-12 14:04:39,634: training iter 347, loss=0.7521976828575134 (2.095218s)
INFO:niftynet:2019-02-12 14:04:40,390: training iter 348, loss=0.7519722580909729 (0.754291s)
INFO:niftynet:2019-02-12 14:04:41,562: training iter 349, loss=0.7519724369049072 (1.172240s)
INFO:niftynet:2019-02-12 14:04:44,289: iter 350 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:04:44,289: training iter 350, loss=0.7524619698524475 (1.722664s)
INFO:niftynet:2019-02-12 14:04:44,500:     validation iter 350, loss=0.7513288259506226 (0.210874s)
INFO:niftynet:2019-02-12 14:04:45,254: training iter 351, loss=0.7160497307777405 (0.753576s)
INFO:niftynet:2019-02-12 14:04:46,010: training iter 352, loss=0.7377452850341797 (0.755120s)
INFO:niftynet:2019-02-12 14:04:46,780: training iter 353, loss=0.7526019811630249 (0.768351s)
INFO:niftynet:2019-02-12 14:04:49,586: training iter 354, loss=0.753193199634552 (2.806120s)
INFO:niftynet:2019-02-12 14:04:52,326: training iter 355, loss=0.7528355717658997 (2.738848s)
INFO:niftynet:2019-02-12 14:04:53,082: training iter 356, loss=0.753847599029541 (0.755591s)
INFO:niftynet:2019-02-12 14:04:53,894: training iter 357, loss=0.7324033975601196 (0.811699s)
INFO:niftynet:2019-02-12 14:04:54,649: training iter 358, loss=0.6939511299133301 (0.754553s)
INFO:niftynet:2019-02-12 14:04:55,492: training iter 359, loss=0.7550566792488098 (0.842042s)
INFO:niftynet:2019-02-12 14:04:56,249: training iter 360, loss=0.7526508569717407 (0.757032s)
INFO:niftynet:2019-02-12 14:04:56,459:     validation iter 360, loss=0.7124421000480652 (0.209251s)
INFO:niftynet:2019-02-12 14:04:59,185: training iter 361, loss=0.7539440393447876 (2.726147s)
INFO:niftynet:2019-02-12 14:05:01,565: training iter 362, loss=0.7542979121208191 (2.377888s)
INFO:niftynet:2019-02-12 14:05:04,520: training iter 363, loss=0.7552264928817749 (2.955374s)
INFO:niftynet:2019-02-12 14:05:05,278: training iter 364, loss=0.7539244890213013 (0.756058s)
INFO:niftynet:2019-02-12 14:05:06,038: training iter 365, loss=0.7108043432235718 (0.759701s)
INFO:niftynet:2019-02-12 14:05:06,798: training iter 366, loss=0.7551689743995667 (0.759448s)
INFO:niftynet:2019-02-12 14:05:07,559: training iter 367, loss=0.7055509686470032 (0.756997s)
INFO:niftynet:2019-02-12 14:05:08,317: training iter 368, loss=0.7546155452728271 (0.757302s)
INFO:niftynet:2019-02-12 14:05:12,131: training iter 369, loss=0.7217408418655396 (3.813612s)
INFO:niftynet:2019-02-12 14:05:12,886: training iter 370, loss=0.756374180316925 (0.753913s)
INFO:niftynet:2019-02-12 14:05:13,094:     validation iter 370, loss=0.7556600570678711 (0.206509s)
INFO:niftynet:2019-02-12 14:05:13,850: training iter 371, loss=0.7560713291168213 (0.755427s)
INFO:niftynet:2019-02-12 14:05:18,207: training iter 372, loss=0.7534810304641724 (4.357445s)
INFO:niftynet:2019-02-12 14:05:18,978: training iter 373, loss=0.7559124827384949 (0.770776s)
INFO:niftynet:2019-02-12 14:05:19,735: training iter 374, loss=0.7565235495567322 (0.756175s)
INFO:niftynet:2019-02-12 14:05:21,428: iter 375 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:05:21,429: training iter 375, loss=0.7563313841819763 (0.757696s)
INFO:niftynet:2019-02-12 14:05:22,185: training iter 376, loss=0.6850048303604126 (0.756136s)
INFO:niftynet:2019-02-12 14:05:22,942: training iter 377, loss=0.7555897235870361 (0.756887s)
INFO:niftynet:2019-02-12 14:05:23,701: training iter 378, loss=0.7552752494812012 (0.757266s)
INFO:niftynet:2019-02-12 14:05:25,699: training iter 379, loss=0.7524122595787048 (1.996824s)
INFO:niftynet:2019-02-12 14:05:26,766: training iter 380, loss=0.7519289255142212 (1.066954s)
INFO:niftynet:2019-02-12 14:05:26,973:     validation iter 380, loss=0.7190551161766052 (0.205516s)
INFO:niftynet:2019-02-12 14:05:30,759: training iter 381, loss=0.7544461488723755 (3.784127s)
INFO:niftynet:2019-02-12 14:05:31,526: training iter 382, loss=0.7524343729019165 (0.766570s)
INFO:niftynet:2019-02-12 14:05:32,289: training iter 383, loss=0.7556503415107727 (0.761187s)
INFO:niftynet:2019-02-12 14:05:33,044: training iter 384, loss=0.7494677305221558 (0.754337s)
INFO:niftynet:2019-02-12 14:05:34,465: training iter 385, loss=0.7548534870147705 (1.420796s)
INFO:niftynet:2019-02-12 14:05:35,730: training iter 386, loss=0.6940336227416992 (1.263846s)
INFO:niftynet:2019-02-12 14:05:36,596: training iter 387, loss=0.7522147297859192 (0.866086s)
INFO:niftynet:2019-02-12 14:05:38,743: training iter 388, loss=0.7538486123085022 (2.146493s)
INFO:niftynet:2019-02-12 14:05:40,320: training iter 389, loss=0.7468135356903076 (1.576319s)
INFO:niftynet:2019-02-12 14:05:43,329: training iter 390, loss=0.7534283399581909 (3.009323s)
INFO:niftynet:2019-02-12 14:05:43,538:     validation iter 390, loss=0.7560392618179321 (0.208317s)
INFO:niftynet:2019-02-12 14:05:44,292: training iter 391, loss=0.7540806531906128 (0.753477s)
INFO:niftynet:2019-02-12 14:05:45,048: training iter 392, loss=0.752176821231842 (0.755247s)
INFO:niftynet:2019-02-12 14:05:46,895: training iter 393, loss=0.7534331679344177 (1.846490s)
INFO:niftynet:2019-02-12 14:05:49,065: training iter 394, loss=0.7535604238510132 (2.169311s)
INFO:niftynet:2019-02-12 14:05:49,818: training iter 395, loss=0.7529593110084534 (0.752894s)
INFO:niftynet:2019-02-12 14:05:51,726: training iter 396, loss=0.7191588282585144 (1.906370s)
INFO:niftynet:2019-02-12 14:05:52,493: training iter 397, loss=0.7528527975082397 (0.756219s)
INFO:niftynet:2019-02-12 14:05:53,251: training iter 398, loss=0.752468466758728 (0.757885s)
INFO:niftynet:2019-02-12 14:05:54,023: training iter 399, loss=0.7522470951080322 (0.771269s)
INFO:niftynet:2019-02-12 14:05:57,729: iter 400 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:05:57,736: training iter 400, loss=0.7550830841064453 (2.552855s)
INFO:niftynet:2019-02-12 14:05:57,949:     validation iter 400, loss=0.760345458984375 (0.212375s)
INFO:niftynet:2019-02-12 14:05:58,702: training iter 401, loss=0.7560505270957947 (0.752923s)
INFO:niftynet:2019-02-12 14:05:59,752: training iter 402, loss=0.7512348294258118 (1.048708s)
INFO:niftynet:2019-02-12 14:06:00,522: training iter 403, loss=0.7520895004272461 (0.768772s)
INFO:niftynet:2019-02-12 14:06:04,123: training iter 404, loss=0.7561625242233276 (3.598950s)
INFO:niftynet:2019-02-12 14:06:04,879: training iter 405, loss=0.7510583400726318 (0.754627s)
INFO:niftynet:2019-02-12 14:06:05,635: training iter 406, loss=0.751132607460022 (0.756089s)
INFO:niftynet:2019-02-12 14:06:06,391: training iter 407, loss=0.7511294484138489 (0.755704s)
INFO:niftynet:2019-02-12 14:06:08,065: training iter 408, loss=0.7132946252822876 (1.673852s)
INFO:niftynet:2019-02-12 14:06:12,054: training iter 409, loss=0.7296080589294434 (3.987449s)
INFO:niftynet:2019-02-12 14:06:12,824: training iter 410, loss=0.7513153553009033 (0.768775s)
INFO:niftynet:2019-02-12 14:06:13,061:     validation iter 410, loss=0.7511739730834961 (0.205672s)
INFO:niftynet:2019-02-12 14:06:13,816: training iter 411, loss=0.7513867020606995 (0.754379s)
INFO:niftynet:2019-02-12 14:06:14,574: training iter 412, loss=0.7270548343658447 (0.758239s)
INFO:niftynet:2019-02-12 14:06:15,448: training iter 413, loss=0.75178062915802 (0.873211s)
INFO:niftynet:2019-02-12 14:06:16,621: training iter 414, loss=0.75128173828125 (1.173306s)
INFO:niftynet:2019-02-12 14:06:17,377: training iter 415, loss=0.7525109648704529 (0.754677s)
INFO:niftynet:2019-02-12 14:06:18,133: training iter 416, loss=0.7168316841125488 (0.755426s)
INFO:niftynet:2019-02-12 14:06:20,974: training iter 417, loss=0.7517778873443604 (2.840544s)
INFO:niftynet:2019-02-12 14:06:21,742: training iter 418, loss=0.7328306436538696 (0.767737s)
INFO:niftynet:2019-02-12 14:06:22,584: training iter 419, loss=0.7519457340240479 (0.839808s)
INFO:niftynet:2019-02-12 14:06:33,131: training iter 420, loss=0.7554742097854614 (10.545900s)
INFO:niftynet:2019-02-12 14:06:33,345:     validation iter 420, loss=0.7517842054367065 (0.213182s)
INFO:niftynet:2019-02-12 14:06:34,099: training iter 421, loss=0.7521578073501587 (0.753490s)
INFO:niftynet:2019-02-12 14:06:34,860: training iter 422, loss=0.7523930072784424 (0.756261s)
INFO:niftynet:2019-02-12 14:06:35,625: training iter 423, loss=0.7529192566871643 (0.764177s)
INFO:niftynet:2019-02-12 14:06:36,440: training iter 424, loss=0.7530480623245239 (0.814117s)
INFO:niftynet:2019-02-12 14:06:38,120: iter 425 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:06:38,120: training iter 425, loss=0.753504753112793 (0.758291s)
INFO:niftynet:2019-02-12 14:06:38,877: training iter 426, loss=0.7016527652740479 (0.756287s)
INFO:niftynet:2019-02-12 14:06:39,632: training iter 427, loss=0.7535433173179626 (0.754490s)
INFO:niftynet:2019-02-12 14:06:40,389: training iter 428, loss=0.7534142732620239 (0.756385s)
INFO:niftynet:2019-02-12 14:06:44,531: training iter 429, loss=0.7130807638168335 (4.140483s)
INFO:niftynet:2019-02-12 14:06:45,764: training iter 430, loss=0.730377733707428 (1.232254s)
INFO:niftynet:2019-02-12 14:06:45,971:     validation iter 430, loss=0.7535445690155029 (0.206247s)
INFO:niftynet:2019-02-12 14:06:46,729: training iter 431, loss=0.7340298891067505 (0.756760s)
INFO:niftynet:2019-02-12 14:06:47,512: training iter 432, loss=0.7558164596557617 (0.781958s)
INFO:niftynet:2019-02-12 14:06:48,289: training iter 433, loss=0.7561365365982056 (0.768552s)
INFO:niftynet:2019-02-12 14:06:49,058: training iter 434, loss=0.7220475673675537 (0.769451s)
INFO:niftynet:2019-02-12 14:06:49,816: training iter 435, loss=0.7568093538284302 (0.755884s)
INFO:niftynet:2019-02-12 14:06:50,580: training iter 436, loss=0.7556747198104858 (0.763785s)
INFO:niftynet:2019-02-12 14:06:56,172: training iter 437, loss=0.7539613842964172 (5.592010s)
INFO:niftynet:2019-02-12 14:06:56,927: training iter 438, loss=0.7530558109283447 (0.754400s)
INFO:niftynet:2019-02-12 14:06:57,683: training iter 439, loss=0.736359715461731 (0.755298s)
INFO:niftynet:2019-02-12 14:06:58,554: training iter 440, loss=0.7557331323623657 (0.870212s)
INFO:niftynet:2019-02-12 14:06:58,761:     validation iter 440, loss=0.7552075982093811 (0.206542s)
INFO:niftynet:2019-02-12 14:06:59,530: training iter 441, loss=0.7533934116363525 (0.767790s)
INFO:niftynet:2019-02-12 14:07:00,295: training iter 442, loss=0.7524597644805908 (0.764938s)
INFO:niftynet:2019-02-12 14:07:01,068: training iter 443, loss=0.754095196723938 (0.772750s)
INFO:niftynet:2019-02-12 14:07:01,828: training iter 444, loss=0.75201815366745 (0.758269s)
INFO:niftynet:2019-02-12 14:07:08,343: training iter 445, loss=0.7528882622718811 (6.510869s)
INFO:niftynet:2019-02-12 14:07:09,100: training iter 446, loss=0.7519493699073792 (0.755773s)
INFO:niftynet:2019-02-12 14:07:09,868: training iter 447, loss=0.7530156373977661 (0.766889s)
INFO:niftynet:2019-02-12 14:07:10,627: training iter 448, loss=0.7519466280937195 (0.758769s)
INFO:niftynet:2019-02-12 14:07:11,397: training iter 449, loss=0.7524055242538452 (0.762271s)
INFO:niftynet:2019-02-12 14:07:13,314: iter 450 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:07:13,315: training iter 450, loss=0.7521916627883911 (0.759728s)
INFO:niftynet:2019-02-12 14:07:13,530:     validation iter 450, loss=0.7513400316238403 (0.214666s)
INFO:niftynet:2019-02-12 14:07:14,290: training iter 451, loss=0.7511987686157227 (0.758297s)
INFO:niftynet:2019-02-12 14:07:15,061: training iter 452, loss=0.7513987421989441 (0.769477s)
INFO:niftynet:2019-02-12 14:07:18,538: training iter 453, loss=0.7514547109603882 (3.467424s)
INFO:niftynet:2019-02-12 14:07:21,386: training iter 454, loss=0.7514168620109558 (2.848139s)
INFO:niftynet:2019-02-12 14:07:22,150: training iter 455, loss=0.7509409189224243 (0.762830s)
INFO:niftynet:2019-02-12 14:07:22,910: training iter 456, loss=0.750769853591919 (0.759435s)
INFO:niftynet:2019-02-12 14:07:23,671: training iter 457, loss=0.734927773475647 (0.760776s)
INFO:niftynet:2019-02-12 14:07:25,299: training iter 458, loss=0.7511067390441895 (1.626755s)
INFO:niftynet:2019-02-12 14:07:26,074: training iter 459, loss=0.7525717616081238 (0.774341s)
INFO:niftynet:2019-02-12 14:07:26,830: training iter 460, loss=0.7399125099182129 (0.755736s)
INFO:niftynet:2019-02-12 14:07:31,668:     validation iter 460, loss=0.7403159141540527 (4.836013s)
INFO:niftynet:2019-02-12 14:07:32,627: training iter 461, loss=0.7502974271774292 (0.958683s)
INFO:niftynet:2019-02-12 14:07:36,566: training iter 462, loss=0.7504013776779175 (3.933819s)
INFO:niftynet:2019-02-12 14:07:37,328: training iter 463, loss=0.7504674792289734 (0.759732s)
INFO:niftynet:2019-02-12 14:07:38,098: training iter 464, loss=0.7505096197128296 (0.768817s)
INFO:niftynet:2019-02-12 14:07:38,882: training iter 465, loss=0.7503424882888794 (0.775197s)
INFO:niftynet:2019-02-12 14:07:39,642: training iter 466, loss=0.7502963542938232 (0.757783s)
INFO:niftynet:2019-02-12 14:07:41,843: training iter 467, loss=0.7506240010261536 (2.201330s)
INFO:niftynet:2019-02-12 14:07:42,598: training iter 468, loss=0.7511017322540283 (0.754256s)
INFO:niftynet:2019-02-12 14:07:46,184: training iter 469, loss=0.7348947525024414 (3.586022s)
INFO:niftynet:2019-02-12 14:07:49,834: training iter 470, loss=0.7540913820266724 (3.648403s)
INFO:niftynet:2019-02-12 14:07:50,042:     validation iter 470, loss=0.7515019178390503 (0.207427s)
INFO:niftynet:2019-02-12 14:07:50,801: training iter 471, loss=0.7510896325111389 (0.757118s)
INFO:niftynet:2019-02-12 14:07:51,579: training iter 472, loss=0.7506136894226074 (0.760044s)
INFO:niftynet:2019-02-12 14:07:52,335: training iter 473, loss=0.7508342862129211 (0.755605s)
INFO:niftynet:2019-02-12 14:07:53,091: training iter 474, loss=0.7509416341781616 (0.755946s)
INFO:niftynet:2019-02-12 14:07:54,808: iter 475 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:07:54,810: training iter 475, loss=0.7523001432418823 (0.769757s)
INFO:niftynet:2019-02-12 14:07:55,566: training iter 476, loss=0.7507511377334595 (0.755311s)
INFO:niftynet:2019-02-12 14:07:56,320: training iter 477, loss=0.7508732080459595 (0.753069s)
INFO:niftynet:2019-02-12 14:07:57,083: training iter 478, loss=0.7390835285186768 (0.762444s)
INFO:niftynet:2019-02-12 14:08:02,293: training iter 479, loss=0.75074303150177 (5.209679s)
INFO:niftynet:2019-02-12 14:08:03,108: training iter 480, loss=0.7508048415184021 (0.814334s)
INFO:niftynet:2019-02-12 14:08:03,314:     validation iter 480, loss=0.7509946227073669 (0.205622s)
INFO:niftynet:2019-02-12 14:08:04,070: training iter 481, loss=0.7400784492492676 (0.755337s)
INFO:niftynet:2019-02-12 14:08:04,828: training iter 482, loss=0.7508578300476074 (0.757907s)
INFO:niftynet:2019-02-12 14:08:05,611: training iter 483, loss=0.7509282827377319 (0.782624s)
INFO:niftynet:2019-02-12 14:08:06,368: training iter 484, loss=0.7268245816230774 (0.756558s)
INFO:niftynet:2019-02-12 14:08:07,443: training iter 485, loss=0.7510186433792114 (1.074200s)
INFO:niftynet:2019-02-12 14:08:09,143: training iter 486, loss=0.7190459370613098 (1.700289s)
INFO:niftynet:2019-02-12 14:08:11,173: training iter 487, loss=0.7510145306587219 (2.029825s)
INFO:niftynet:2019-02-12 14:08:15,856: training iter 488, loss=0.7511898279190063 (4.676890s)
INFO:niftynet:2019-02-12 14:08:16,616: training iter 489, loss=0.7109317779541016 (0.759623s)
INFO:niftynet:2019-02-12 14:08:17,394: training iter 490, loss=0.7519423961639404 (0.778226s)
INFO:niftynet:2019-02-12 14:08:17,609:     validation iter 490, loss=0.752288818359375 (0.214048s)
INFO:niftynet:2019-02-12 14:08:18,369: training iter 491, loss=0.7522554397583008 (0.758375s)
INFO:niftynet:2019-02-12 14:08:19,140: training iter 492, loss=0.7375314235687256 (0.769727s)
INFO:niftynet:2019-02-12 14:08:19,941: training iter 493, loss=0.7330213785171509 (0.800628s)
INFO:niftynet:2019-02-12 14:08:20,702: training iter 494, loss=0.7541765570640564 (0.759508s)
INFO:niftynet:2019-02-12 14:08:21,461: training iter 495, loss=0.7117003202438354 (0.756839s)
INFO:niftynet:2019-02-12 14:08:27,472: training iter 496, loss=0.7545861601829529 (6.010841s)
INFO:niftynet:2019-02-12 14:08:28,524: training iter 497, loss=0.7574400305747986 (0.756561s)
INFO:niftynet:2019-02-12 14:08:29,281: training iter 498, loss=0.7557772994041443 (0.755931s)
INFO:niftynet:2019-02-12 14:08:31,814: training iter 499, loss=0.7568774819374084 (2.532225s)
INFO:niftynet:2019-02-12 14:08:33,727: iter 500 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:08:33,728: training iter 500, loss=0.7543036937713623 (0.765910s)
INFO:niftynet:2019-02-12 14:08:33,956:     validation iter 500, loss=0.7559618949890137 (0.227566s)
INFO:niftynet:2019-02-12 14:08:34,722: training iter 501, loss=0.755171537399292 (0.765878s)
INFO:niftynet:2019-02-12 14:08:35,479: training iter 502, loss=0.7558853030204773 (0.755558s)
INFO:niftynet:2019-02-12 14:08:36,236: training iter 503, loss=0.7648809552192688 (0.756104s)
INFO:niftynet:2019-02-12 14:08:37,914: training iter 504, loss=0.7557204365730286 (1.677522s)
INFO:niftynet:2019-02-12 14:08:38,867: training iter 505, loss=0.7317012548446655 (0.952367s)
INFO:niftynet:2019-02-12 14:08:40,890: training iter 506, loss=0.7537835836410522 (2.022601s)
INFO:niftynet:2019-02-12 14:08:41,646: training iter 507, loss=0.7539464235305786 (0.755841s)
INFO:niftynet:2019-02-12 14:08:42,578: training iter 508, loss=0.7135141491889954 (0.931286s)
INFO:niftynet:2019-02-12 14:08:43,343: training iter 509, loss=0.7547360062599182 (0.756573s)
INFO:niftynet:2019-02-12 14:08:44,099: training iter 510, loss=0.7127220630645752 (0.755351s)
INFO:niftynet:2019-02-12 14:08:44,306:     validation iter 510, loss=0.7532382011413574 (0.206982s)
INFO:niftynet:2019-02-12 14:08:47,979: training iter 511, loss=0.7545119524002075 (3.672779s)
INFO:niftynet:2019-02-12 14:08:49,690: training iter 512, loss=0.7543421387672424 (1.703735s)
INFO:niftynet:2019-02-12 14:08:50,447: training iter 513, loss=0.7524046897888184 (0.755258s)
INFO:niftynet:2019-02-12 14:08:51,205: training iter 514, loss=0.7531766891479492 (0.757489s)
INFO:niftynet:2019-02-12 14:08:52,751: training iter 515, loss=0.7262159585952759 (1.545448s)
INFO:niftynet:2019-02-12 14:08:57,191: training iter 516, loss=0.7540838122367859 (4.440045s)
INFO:niftynet:2019-02-12 14:08:57,947: training iter 517, loss=0.7529633045196533 (0.754205s)
INFO:niftynet:2019-02-12 14:08:58,703: training iter 518, loss=0.7527681589126587 (0.755693s)
INFO:niftynet:2019-02-12 14:09:01,266: training iter 519, loss=0.7519756555557251 (2.563098s)
INFO:niftynet:2019-02-12 14:09:02,023: training iter 520, loss=0.753166913986206 (0.756048s)
INFO:niftynet:2019-02-12 14:09:02,230:     validation iter 520, loss=0.7393612861633301 (0.206858s)
INFO:niftynet:2019-02-12 14:09:03,002: training iter 521, loss=0.7535887360572815 (0.758548s)
INFO:niftynet:2019-02-12 14:09:03,768: training iter 522, loss=0.7527105808258057 (0.764020s)
INFO:niftynet:2019-02-12 14:09:04,531: training iter 523, loss=0.6917065978050232 (0.760090s)
INFO:niftynet:2019-02-12 14:09:05,287: training iter 524, loss=0.7532311677932739 (0.755570s)
INFO:niftynet:2019-02-12 14:09:10,442: iter 525 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:09:10,442: training iter 525, loss=0.7533060908317566 (4.321585s)
INFO:niftynet:2019-02-12 14:09:11,202: training iter 526, loss=0.7533352971076965 (0.759333s)
INFO:niftynet:2019-02-12 14:09:11,957: training iter 527, loss=0.7528265118598938 (0.754859s)
INFO:niftynet:2019-02-12 14:09:14,894: training iter 528, loss=0.7527904510498047 (2.935386s)
INFO:niftynet:2019-02-12 14:09:15,652: training iter 529, loss=0.7525538802146912 (0.757432s)
INFO:niftynet:2019-02-12 14:09:16,408: training iter 530, loss=0.7518451809883118 (0.755682s)
INFO:niftynet:2019-02-12 14:09:16,617:     validation iter 530, loss=0.7525876760482788 (0.206999s)
INFO:niftynet:2019-02-12 14:09:17,380: training iter 531, loss=0.7522356510162354 (0.761727s)
INFO:niftynet:2019-02-12 14:09:18,142: training iter 532, loss=0.7000110149383545 (0.762155s)
INFO:niftynet:2019-02-12 14:09:19,144: training iter 533, loss=0.7338360548019409 (1.001752s)
INFO:niftynet:2019-02-12 14:09:22,710: training iter 534, loss=0.7534772157669067 (3.565650s)
INFO:niftynet:2019-02-12 14:09:23,466: training iter 535, loss=0.7355773448944092 (0.754836s)
INFO:niftynet:2019-02-12 14:09:24,225: training iter 536, loss=0.7540815472602844 (0.758424s)
INFO:niftynet:2019-02-12 14:09:25,252: training iter 537, loss=0.7374658584594727 (1.026563s)
INFO:niftynet:2019-02-12 14:09:26,113: training iter 538, loss=0.733130693435669 (0.859948s)
INFO:niftynet:2019-02-12 14:09:26,870: training iter 539, loss=0.7568197250366211 (0.755943s)
INFO:niftynet:2019-02-12 14:09:29,157: training iter 540, loss=0.7523336410522461 (2.287013s)
INFO:niftynet:2019-02-12 14:09:29,376:     validation iter 540, loss=0.7538753747940063 (0.216182s)
INFO:niftynet:2019-02-12 14:09:30,190: training iter 541, loss=0.7558028101921082 (0.814229s)
INFO:niftynet:2019-02-12 14:09:32,185: training iter 542, loss=0.757846474647522 (1.993960s)
INFO:niftynet:2019-02-12 14:09:35,606: training iter 543, loss=0.7456713914871216 (3.420081s)
INFO:niftynet:2019-02-12 14:09:36,543: training iter 544, loss=0.7565012574195862 (0.936151s)
INFO:niftynet:2019-02-12 14:09:37,305: training iter 545, loss=0.7541933059692383 (0.757319s)
INFO:niftynet:2019-02-12 14:09:38,059: training iter 546, loss=0.7534183263778687 (0.753752s)
INFO:niftynet:2019-02-12 14:09:38,824: training iter 547, loss=0.7605571746826172 (0.763477s)
INFO:niftynet:2019-02-12 14:09:41,762: training iter 548, loss=0.7525956630706787 (2.937368s)
INFO:niftynet:2019-02-12 14:09:42,531: training iter 549, loss=0.7542738318443298 (0.767642s)
INFO:niftynet:2019-02-12 14:09:45,134: iter 550 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:09:45,136: training iter 550, loss=0.7535690069198608 (1.808891s)
INFO:niftynet:2019-02-12 14:09:45,358:     validation iter 550, loss=0.7522858381271362 (0.219065s)
INFO:niftynet:2019-02-12 14:09:47,831: training iter 551, loss=0.7534412145614624 (2.471292s)
INFO:niftynet:2019-02-12 14:09:48,586: training iter 552, loss=0.7525063157081604 (0.754717s)
INFO:niftynet:2019-02-12 14:09:49,350: training iter 553, loss=0.7391977906227112 (0.763314s)
INFO:niftynet:2019-02-12 14:09:50,107: training iter 554, loss=0.7358015179634094 (0.756812s)
INFO:niftynet:2019-02-12 14:09:50,876: training iter 555, loss=0.7516368627548218 (0.767984s)
INFO:niftynet:2019-02-12 14:09:56,097: training iter 556, loss=0.7508260607719421 (5.221026s)
INFO:niftynet:2019-02-12 14:09:56,853: training iter 557, loss=0.7566596865653992 (0.755859s)
INFO:niftynet:2019-02-12 14:09:57,612: training iter 558, loss=0.7509230375289917 (0.757327s)
INFO:niftynet:2019-02-12 14:09:59,514: training iter 559, loss=0.7514767646789551 (1.901707s)
INFO:niftynet:2019-02-12 14:10:00,733: training iter 560, loss=0.7516500949859619 (1.218627s)
INFO:niftynet:2019-02-12 14:10:00,939:     validation iter 560, loss=0.7513017058372498 (0.206109s)
INFO:niftynet:2019-02-12 14:10:01,695: training iter 561, loss=0.751508891582489 (0.754668s)
INFO:niftynet:2019-02-12 14:10:02,454: training iter 562, loss=0.7518349885940552 (0.758865s)
INFO:niftynet:2019-02-12 14:10:03,240: training iter 563, loss=0.7512679100036621 (0.785464s)
INFO:niftynet:2019-02-12 14:10:04,006: training iter 564, loss=0.7514106631278992 (0.759654s)
INFO:niftynet:2019-02-12 14:10:07,882: training iter 565, loss=0.7510269284248352 (3.875512s)
INFO:niftynet:2019-02-12 14:10:08,642: training iter 566, loss=0.7518669366836548 (0.756551s)
INFO:niftynet:2019-02-12 14:10:09,398: training iter 567, loss=0.7512999773025513 (0.755886s)
INFO:niftynet:2019-02-12 14:10:11,663: training iter 568, loss=0.7510493993759155 (2.264428s)
INFO:niftynet:2019-02-12 14:10:14,093: training iter 569, loss=0.7511523962020874 (2.429520s)
INFO:niftynet:2019-02-12 14:10:15,450: training iter 570, loss=0.7503796815872192 (0.995181s)
INFO:niftynet:2019-02-12 14:10:15,656:     validation iter 570, loss=0.7401808500289917 (0.206172s)
INFO:niftynet:2019-02-12 14:10:16,414: training iter 571, loss=0.7508326768875122 (0.756444s)
INFO:niftynet:2019-02-12 14:10:17,168: training iter 572, loss=0.7508687973022461 (0.753437s)
INFO:niftynet:2019-02-12 14:10:20,308: training iter 573, loss=0.7514259815216064 (3.139575s)
INFO:niftynet:2019-02-12 14:10:21,246: training iter 574, loss=0.741950511932373 (0.937315s)
INFO:niftynet:2019-02-12 14:10:22,892: iter 575 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:10:22,894: training iter 575, loss=0.7506840825080872 (0.757858s)
INFO:niftynet:2019-02-12 14:10:24,150: training iter 576, loss=0.7509252429008484 (1.253515s)
INFO:niftynet:2019-02-12 14:10:24,906: training iter 577, loss=0.7046130895614624 (0.756121s)
INFO:niftynet:2019-02-12 14:10:26,902: training iter 578, loss=0.7507656812667847 (1.994801s)
INFO:niftynet:2019-02-12 14:10:27,694: training iter 579, loss=0.7508289217948914 (0.791790s)
INFO:niftynet:2019-02-12 14:10:30,478: training iter 580, loss=0.7506729960441589 (2.783102s)
INFO:niftynet:2019-02-12 14:10:30,686:     validation iter 580, loss=0.7507636547088623 (0.206439s)
INFO:niftynet:2019-02-12 14:10:31,577: training iter 581, loss=0.7445837259292603 (0.889659s)
INFO:niftynet:2019-02-12 14:10:32,332: training iter 582, loss=0.739392876625061 (0.755148s)
INFO:niftynet:2019-02-12 14:10:33,090: training iter 583, loss=0.7518109679222107 (0.756433s)
INFO:niftynet:2019-02-12 14:10:33,854: training iter 584, loss=0.7275989055633545 (0.758605s)
INFO:niftynet:2019-02-12 14:10:35,738: training iter 585, loss=0.7509689331054688 (1.882212s)
INFO:niftynet:2019-02-12 14:10:39,748: training iter 586, loss=0.7522281408309937 (4.009651s)
INFO:niftynet:2019-02-12 14:10:40,504: training iter 587, loss=0.7512768507003784 (0.756497s)
INFO:niftynet:2019-02-12 14:10:43,336: training iter 588, loss=0.7515877485275269 (2.830574s)
INFO:niftynet:2019-02-12 14:10:44,093: training iter 589, loss=0.752452552318573 (0.756175s)
INFO:niftynet:2019-02-12 14:10:44,849: training iter 590, loss=0.7520716190338135 (0.755261s)
INFO:niftynet:2019-02-12 14:10:45,058:     validation iter 590, loss=0.7543318271636963 (0.209443s)
INFO:niftynet:2019-02-12 14:10:46,449: training iter 591, loss=0.751896321773529 (1.389792s)
INFO:niftynet:2019-02-12 14:10:47,218: training iter 592, loss=0.752119779586792 (0.769163s)
INFO:niftynet:2019-02-12 14:10:47,974: training iter 593, loss=0.7529496550559998 (0.755439s)
INFO:niftynet:2019-02-12 14:10:53,360: training iter 594, loss=0.7390313148498535 (5.385458s)
INFO:niftynet:2019-02-12 14:10:56,982: training iter 595, loss=0.7523934245109558 (0.758300s)
INFO:niftynet:2019-02-12 14:10:57,738: training iter 596, loss=0.7514992356300354 (0.755364s)
INFO:niftynet:2019-02-12 14:10:58,526: training iter 597, loss=0.752374529838562 (0.787537s)
INFO:niftynet:2019-02-12 14:10:59,284: training iter 598, loss=0.7522194385528564 (0.757739s)
INFO:niftynet:2019-02-12 14:11:00,038: training iter 599, loss=0.7517857551574707 (0.753170s)
INFO:niftynet:2019-02-12 14:11:01,765: iter 600 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:11:01,765: training iter 600, loss=0.7517051100730896 (0.757466s)
INFO:niftynet:2019-02-12 14:11:01,980:     validation iter 600, loss=0.7522586584091187 (0.214688s)
INFO:niftynet:2019-02-12 14:11:02,738: training iter 601, loss=0.7393172383308411 (0.757450s)
INFO:niftynet:2019-02-12 14:11:05,604: training iter 602, loss=0.720221996307373 (2.865791s)
INFO:niftynet:2019-02-12 14:11:06,465: training iter 603, loss=0.752591609954834 (0.859254s)
INFO:niftynet:2019-02-12 14:11:07,238: training iter 604, loss=0.7164519429206848 (0.772563s)
INFO:niftynet:2019-02-12 14:11:08,093: training iter 605, loss=0.753722608089447 (0.854465s)
INFO:niftynet:2019-02-12 14:11:08,853: training iter 606, loss=0.755339503288269 (0.760224s)
INFO:niftynet:2019-02-12 14:11:09,625: training iter 607, loss=0.7565732002258301 (0.771415s)
INFO:niftynet:2019-02-12 14:11:10,956: training iter 608, loss=0.7552499175071716 (1.330966s)
INFO:niftynet:2019-02-12 14:11:13,748: training iter 609, loss=0.7560075521469116 (2.790171s)
INFO:niftynet:2019-02-12 14:11:18,144: training iter 610, loss=0.7536529898643494 (4.394591s)
INFO:niftynet:2019-02-12 14:11:18,366:     validation iter 610, loss=0.7541327476501465 (0.221698s)
INFO:niftynet:2019-02-12 14:11:19,130: training iter 611, loss=0.7542475461959839 (0.762099s)
INFO:niftynet:2019-02-12 14:11:19,902: training iter 612, loss=0.753036618232727 (0.772355s)
INFO:niftynet:2019-02-12 14:11:20,664: training iter 613, loss=0.7188372611999512 (0.760779s)
INFO:niftynet:2019-02-12 14:11:21,422: training iter 614, loss=0.7563787698745728 (0.757865s)
WARNING:niftynet:2019-02-12 14:11:22,182: User cancelled application
INFO:niftynet:2019-02-12 14:11:22,182: cleaning up...
INFO:niftynet:2019-02-12 14:11:22,183: stopping sampling threads
INFO:niftynet:2019-02-12 14:11:23,462: iter 615 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
WARNING:niftynet:2019-02-12 14:11:32,821: stopped early, incomplete iterations.
INFO:niftynet:2019-02-12 14:11:32,821: SegmentationApplication stopped (time in second 1044.91).
INFO:niftynet:2019-02-12 14:14:25,582: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 14:14:25,582: starting segmentation application
INFO:niftynet:2019-02-12 14:14:25,583: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 14:14:25,612: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 14:14:25,616: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 14:14:25,620: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 14:14:25,624: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 14:14:25,626: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 14:14:39,157: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 14:14:39,158: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 14:14:40,685: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 14:14:40,685: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 14:14:40,696: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 14:14:40,696: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 14:14:40,696: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 14:14:40,696: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 14:14:42,776: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 14:14:42,822: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 14:14:42,933: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 14:14:42,934: using DenseVNet
INFO:niftynet:2019-02-12 14:14:42,939: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 14:14:42,980: Initialising Dataset from 29 subjects...
INFO:niftynet:2019-02-12 14:14:56,067: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 14:16:19,800: training iter 1, loss=0.8967020511627197 (83.411477s)
INFO:niftynet:2019-02-12 14:16:22,583: training iter 2, loss=0.8289585113525391 (2.782687s)
INFO:niftynet:2019-02-12 14:16:25,034: training iter 3, loss=0.7875880002975464 (2.451323s)
INFO:niftynet:2019-02-12 14:16:27,786: training iter 4, loss=0.6848485469818115 (2.751255s)
INFO:niftynet:2019-02-12 14:16:41,997: training iter 5, loss=0.5937510132789612 (14.210618s)
INFO:niftynet:2019-02-12 14:16:44,637: training iter 6, loss=0.6408222913742065 (2.638184s)
INFO:niftynet:2019-02-12 14:16:47,016: training iter 7, loss=0.6194819211959839 (2.378477s)
INFO:niftynet:2019-02-12 14:16:49,711: training iter 8, loss=0.6492644548416138 (2.694193s)
INFO:niftynet:2019-02-12 14:16:52,804: training iter 9, loss=0.6790016293525696 (3.093473s)
INFO:niftynet:2019-02-12 14:16:55,374: training iter 10, loss=0.6790746450424194 (2.566930s)
INFO:niftynet:2019-02-12 14:17:51,683:     validation iter 10, loss=0.6719561815261841 (56.306134s)
INFO:niftynet:2019-02-12 14:18:03,637: training iter 11, loss=0.6612659692764282 (11.942045s)
INFO:niftynet:2019-02-12 14:18:07,053: training iter 12, loss=0.6942881345748901 (3.414947s)
INFO:niftynet:2019-02-12 14:18:09,706: training iter 13, loss=0.6818256378173828 (2.653499s)
INFO:niftynet:2019-02-12 14:18:14,582: training iter 14, loss=0.6792469620704651 (4.874811s)
INFO:niftynet:2019-02-12 14:18:30,714: training iter 15, loss=0.6586587429046631 (16.109548s)
INFO:niftynet:2019-02-12 14:18:36,093: training iter 16, loss=0.7002078890800476 (5.370150s)
INFO:niftynet:2019-02-12 14:18:39,628: training iter 17, loss=0.6893067955970764 (3.501666s)
INFO:niftynet:2019-02-12 14:18:42,348: training iter 18, loss=0.6821560859680176 (2.718091s)
INFO:niftynet:2019-02-12 14:18:45,536: training iter 19, loss=0.7151437401771545 (3.187266s)
INFO:niftynet:2019-02-12 14:18:48,171: training iter 20, loss=0.7046006917953491 (2.634711s)
INFO:niftynet:2019-02-12 14:18:50,785:     validation iter 20, loss=0.7010188102722168 (2.611772s)
INFO:niftynet:2019-02-12 14:18:53,252: training iter 21, loss=0.7063971757888794 (2.464242s)
INFO:niftynet:2019-02-12 14:18:55,914: training iter 22, loss=0.7108173370361328 (2.658159s)
INFO:niftynet:2019-02-12 14:18:58,706: training iter 23, loss=0.6908554434776306 (2.791776s)
INFO:niftynet:2019-02-12 14:19:01,446: training iter 24, loss=0.6564990282058716 (2.739524s)
INFO:niftynet:2019-02-12 14:19:03,755: training iter 25, loss=0.6801292896270752 (2.308783s)
INFO:niftynet:2019-02-12 14:19:05,592: iter 25 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:19:08,832: training iter 26, loss=0.6873463988304138 (3.238558s)
INFO:niftynet:2019-02-12 14:19:11,076: training iter 27, loss=0.6513762474060059 (2.243794s)
INFO:niftynet:2019-02-12 14:19:13,768: training iter 28, loss=0.7594724893569946 (2.691148s)
INFO:niftynet:2019-02-12 14:19:16,128: training iter 29, loss=0.6036150455474854 (2.360246s)
INFO:niftynet:2019-02-12 14:19:18,819: training iter 30, loss=0.6767486333847046 (2.690153s)
INFO:niftynet:2019-02-12 14:19:21,109:     validation iter 30, loss=0.762089729309082 (2.288978s)
INFO:niftynet:2019-02-12 14:19:24,305: training iter 31, loss=0.7490248680114746 (3.191872s)
INFO:niftynet:2019-02-12 14:19:27,269: training iter 32, loss=0.5953999757766724 (2.962583s)
INFO:niftynet:2019-02-12 14:19:29,547: training iter 33, loss=0.576245129108429 (2.277662s)
INFO:niftynet:2019-02-12 14:19:32,549: training iter 34, loss=0.7340428829193115 (3.000233s)
INFO:niftynet:2019-02-12 14:19:34,938: training iter 35, loss=0.7258280515670776 (2.388891s)
INFO:niftynet:2019-02-12 14:19:37,443: training iter 36, loss=0.5447984933853149 (2.502167s)
INFO:niftynet:2019-02-12 14:19:39,475: training iter 37, loss=0.6386072635650635 (2.030639s)
INFO:niftynet:2019-02-12 14:19:42,679: training iter 38, loss=0.7386565208435059 (3.202754s)
INFO:niftynet:2019-02-12 14:19:44,741: training iter 39, loss=0.7194751501083374 (2.061511s)
INFO:niftynet:2019-02-12 14:19:47,766: training iter 40, loss=0.6209671497344971 (3.024056s)
INFO:niftynet:2019-02-12 14:19:50,046:     validation iter 40, loss=0.6286935210227966 (2.277302s)
INFO:niftynet:2019-02-12 14:19:52,762: training iter 41, loss=0.649186909198761 (2.713971s)
INFO:niftynet:2019-02-12 14:19:55,716: training iter 42, loss=0.5159429311752319 (2.953329s)
INFO:niftynet:2019-02-12 14:19:58,222: training iter 43, loss=0.46931192278862 (2.505085s)
INFO:niftynet:2019-02-12 14:20:00,842: training iter 44, loss=0.687041163444519 (2.618001s)
INFO:niftynet:2019-02-12 14:20:03,703: training iter 45, loss=0.616938054561615 (2.860448s)
INFO:niftynet:2019-02-12 14:20:06,532: training iter 46, loss=0.6305440068244934 (2.827131s)
INFO:niftynet:2019-02-12 14:20:09,011: training iter 47, loss=0.41065678000450134 (2.478631s)
INFO:niftynet:2019-02-12 14:20:12,445: training iter 48, loss=0.6603043675422668 (3.431903s)
INFO:niftynet:2019-02-12 14:20:14,739: training iter 49, loss=0.5946625471115112 (2.292394s)
INFO:niftynet:2019-02-12 14:20:17,580: training iter 50, loss=0.6746958494186401 (2.841329s)
INFO:niftynet:2019-02-12 14:20:19,450: iter 50 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:20:21,329:     validation iter 50, loss=0.6086181402206421 (1.873229s)
INFO:niftynet:2019-02-12 14:20:23,701: training iter 51, loss=0.6418516635894775 (2.370333s)
INFO:niftynet:2019-02-12 14:20:26,627: training iter 52, loss=0.6868352890014648 (2.923637s)
INFO:niftynet:2019-02-12 14:20:29,520: training iter 53, loss=0.6597739458084106 (2.848180s)
INFO:niftynet:2019-02-12 14:20:31,991: training iter 54, loss=0.6353013515472412 (2.470609s)
INFO:niftynet:2019-02-12 14:20:34,816: training iter 55, loss=0.5373530983924866 (2.824796s)
INFO:niftynet:2019-02-12 14:20:37,232: training iter 56, loss=0.7609249353408813 (2.412333s)
INFO:niftynet:2019-02-12 14:20:39,876: training iter 57, loss=0.7085137963294983 (2.642613s)
INFO:niftynet:2019-02-12 14:20:42,299: training iter 58, loss=0.645868182182312 (2.423222s)
INFO:niftynet:2019-02-12 14:20:44,409: training iter 59, loss=0.6614257097244263 (2.108918s)
INFO:niftynet:2019-02-12 14:20:46,872: training iter 60, loss=0.6993781328201294 (2.462542s)
INFO:niftynet:2019-02-12 14:20:48,679:     validation iter 60, loss=0.6256082057952881 (1.805358s)
INFO:niftynet:2019-02-12 14:20:51,106: training iter 61, loss=0.6247982978820801 (2.424844s)
INFO:niftynet:2019-02-12 14:20:53,356: training iter 62, loss=0.7082650065422058 (2.249156s)
INFO:niftynet:2019-02-12 14:20:55,672: training iter 63, loss=0.7477109432220459 (2.316472s)
INFO:niftynet:2019-02-12 14:20:57,362: training iter 64, loss=0.7696361541748047 (1.689543s)
INFO:niftynet:2019-02-12 14:21:00,150: training iter 65, loss=0.5772779583930969 (2.787744s)
INFO:niftynet:2019-02-12 14:21:02,962: training iter 66, loss=0.7157261371612549 (2.808763s)
INFO:niftynet:2019-02-12 14:21:05,398: training iter 67, loss=0.6544108390808105 (2.435756s)
INFO:niftynet:2019-02-12 14:21:08,251: training iter 68, loss=0.7110946774482727 (2.851668s)
INFO:niftynet:2019-02-12 14:21:11,019: training iter 69, loss=0.7106094360351562 (2.767473s)
INFO:niftynet:2019-02-12 14:21:14,097: training iter 70, loss=0.7285125255584717 (3.077358s)
INFO:niftynet:2019-02-12 14:21:16,142:     validation iter 70, loss=0.5817933082580566 (2.043408s)
INFO:niftynet:2019-02-12 14:21:18,731: training iter 71, loss=0.559755802154541 (2.587771s)
INFO:niftynet:2019-02-12 14:21:21,091: training iter 72, loss=0.7068483829498291 (2.359520s)
INFO:niftynet:2019-02-12 14:21:24,646: training iter 73, loss=0.7557699680328369 (3.552977s)
INFO:niftynet:2019-02-12 14:21:27,059: training iter 74, loss=0.5611783266067505 (2.412747s)
INFO:niftynet:2019-02-12 14:21:29,837: training iter 75, loss=0.49685949087142944 (2.777675s)
INFO:niftynet:2019-02-12 14:21:31,735: iter 75 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:21:34,512: training iter 76, loss=0.6670231819152832 (2.758714s)
INFO:niftynet:2019-02-12 14:21:37,322: training iter 77, loss=0.5840293765068054 (2.808555s)
INFO:niftynet:2019-02-12 14:21:40,414: training iter 78, loss=0.6626977920532227 (3.091366s)
INFO:niftynet:2019-02-12 14:21:42,990: training iter 79, loss=0.5165215134620667 (2.575578s)
INFO:niftynet:2019-02-12 14:21:45,507: training iter 80, loss=0.5119419097900391 (2.516006s)
INFO:niftynet:2019-02-12 14:21:47,723:     validation iter 80, loss=0.6261599063873291 (2.212397s)
INFO:niftynet:2019-02-12 14:21:49,880: training iter 81, loss=0.661865234375 (2.155035s)
INFO:niftynet:2019-02-12 14:21:52,570: training iter 82, loss=0.5028190016746521 (2.687747s)
INFO:niftynet:2019-02-12 14:21:55,276: training iter 83, loss=0.5201655030250549 (2.705263s)
INFO:niftynet:2019-02-12 14:21:57,890: training iter 84, loss=0.7312583327293396 (2.613373s)
INFO:niftynet:2019-02-12 14:22:00,335: training iter 85, loss=0.748755693435669 (2.443048s)
INFO:niftynet:2019-02-12 14:22:03,235: training iter 86, loss=0.48180851340293884 (2.897575s)
INFO:niftynet:2019-02-12 14:22:05,717: training iter 87, loss=0.7258882522583008 (2.480030s)
INFO:niftynet:2019-02-12 14:22:07,768: training iter 88, loss=0.6079310774803162 (2.050650s)
INFO:niftynet:2019-02-12 14:22:10,684: training iter 89, loss=0.699292004108429 (2.916308s)
INFO:niftynet:2019-02-12 14:22:13,784: training iter 90, loss=0.5196384191513062 (3.099360s)
INFO:niftynet:2019-02-12 14:22:23,733:     validation iter 90, loss=0.711843729019165 (9.945604s)
INFO:niftynet:2019-02-12 14:22:25,837: training iter 91, loss=0.5457509160041809 (2.098993s)
INFO:niftynet:2019-02-12 14:22:28,264: training iter 92, loss=0.6869899034500122 (2.426485s)
INFO:niftynet:2019-02-12 14:22:30,556: training iter 93, loss=0.7323079109191895 (2.292700s)
INFO:niftynet:2019-02-12 14:22:34,011: training iter 94, loss=0.6657688617706299 (3.453800s)
INFO:niftynet:2019-02-12 14:22:36,535: training iter 95, loss=0.679122805595398 (2.524549s)
INFO:niftynet:2019-02-12 14:22:40,234: training iter 96, loss=0.7125056982040405 (3.696979s)
INFO:niftynet:2019-02-12 14:22:47,870: training iter 97, loss=0.6982909440994263 (7.635677s)
INFO:niftynet:2019-02-12 14:22:50,611: training iter 98, loss=0.5092399716377258 (2.740549s)
INFO:niftynet:2019-02-12 14:22:52,797: training iter 99, loss=0.6741085052490234 (2.185111s)
INFO:niftynet:2019-02-12 14:22:55,949: training iter 100, loss=0.5379453897476196 (3.151949s)
INFO:niftynet:2019-02-12 14:22:57,553: iter 100 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:22:59,470:     validation iter 100, loss=0.5480780601501465 (1.915321s)
INFO:niftynet:2019-02-12 14:23:04,474: training iter 101, loss=0.6168327331542969 (5.000750s)
INFO:niftynet:2019-02-12 14:23:07,279: training iter 102, loss=0.6529139876365662 (2.803794s)
INFO:niftynet:2019-02-12 14:23:10,643: training iter 103, loss=0.7523658275604248 (3.363275s)
INFO:niftynet:2019-02-12 14:23:13,201: training iter 104, loss=0.56868577003479 (2.557835s)
INFO:niftynet:2019-02-12 14:23:15,552: training iter 105, loss=0.6648690700531006 (2.350230s)
INFO:niftynet:2019-02-12 14:23:21,087: training iter 106, loss=0.5950751900672913 (5.533968s)
INFO:niftynet:2019-02-12 14:23:23,525: training iter 107, loss=0.605806827545166 (2.435588s)
INFO:niftynet:2019-02-12 14:23:26,046: training iter 108, loss=0.5537191033363342 (2.518876s)
INFO:niftynet:2019-02-12 14:23:28,926: training iter 109, loss=0.6232285499572754 (2.879954s)
INFO:niftynet:2019-02-12 14:23:35,651: training iter 110, loss=0.710739254951477 (6.723312s)
INFO:niftynet:2019-02-12 14:23:37,381:     validation iter 110, loss=0.7105467319488525 (1.728762s)
INFO:niftynet:2019-02-12 14:23:40,184: training iter 111, loss=0.5140384435653687 (2.800960s)
INFO:niftynet:2019-02-12 14:23:43,206: training iter 112, loss=0.5470105409622192 (3.020132s)
INFO:niftynet:2019-02-12 14:23:45,934: training iter 113, loss=0.5881296396255493 (2.723614s)
INFO:niftynet:2019-02-12 14:23:49,475: training iter 114, loss=0.6321329474449158 (3.540869s)
INFO:niftynet:2019-02-12 14:23:54,057: training iter 115, loss=0.6485582590103149 (4.580093s)
INFO:niftynet:2019-02-12 14:23:56,461: training iter 116, loss=0.6960256099700928 (2.399777s)
INFO:niftynet:2019-02-12 14:23:59,478: training iter 117, loss=0.6955550909042358 (3.011958s)
INFO:niftynet:2019-02-12 14:24:02,160: training iter 118, loss=0.6309928894042969 (2.682300s)
INFO:niftynet:2019-02-12 14:24:05,921: training iter 119, loss=0.5593404769897461 (3.758992s)
INFO:niftynet:2019-02-12 14:24:09,769: training iter 120, loss=0.7628751993179321 (3.847631s)
INFO:niftynet:2019-02-12 14:24:12,268:     validation iter 120, loss=0.7561774849891663 (2.496341s)
INFO:niftynet:2019-02-12 14:24:15,157: training iter 121, loss=0.6122565269470215 (2.865169s)
INFO:niftynet:2019-02-12 14:24:17,821: training iter 122, loss=0.48000144958496094 (2.663562s)
INFO:niftynet:2019-02-12 14:24:21,323: training iter 123, loss=0.7075189352035522 (3.502001s)
INFO:niftynet:2019-02-12 14:24:26,016: training iter 124, loss=0.5462773442268372 (4.692618s)
INFO:niftynet:2019-02-12 14:24:28,216: training iter 125, loss=0.6417024731636047 (2.199741s)
INFO:niftynet:2019-02-12 14:24:30,795: iter 125 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:24:33,476: training iter 126, loss=0.6933410167694092 (2.678385s)
INFO:niftynet:2019-02-12 14:24:35,978: training iter 127, loss=0.572013258934021 (2.501017s)
INFO:niftynet:2019-02-12 14:24:38,285: training iter 128, loss=0.7308012843132019 (2.305809s)
INFO:niftynet:2019-02-12 14:24:42,388: training iter 129, loss=0.7045564651489258 (4.103010s)
INFO:niftynet:2019-02-12 14:24:46,140: training iter 130, loss=0.571439266204834 (3.751443s)
INFO:niftynet:2019-02-12 14:24:48,319:     validation iter 130, loss=0.649211049079895 (2.174625s)
INFO:niftynet:2019-02-12 14:24:50,945: training iter 131, loss=0.5679856538772583 (2.624762s)
INFO:niftynet:2019-02-12 14:24:55,428: training iter 132, loss=0.5465143322944641 (4.482672s)
INFO:niftynet:2019-02-12 14:25:01,098: training iter 133, loss=0.5494041442871094 (5.668397s)
INFO:niftynet:2019-02-12 14:25:04,087: training iter 134, loss=0.6530516147613525 (2.988310s)
INFO:niftynet:2019-02-12 14:25:06,836: training iter 135, loss=0.5410438776016235 (2.747965s)
INFO:niftynet:2019-02-12 14:25:09,672: training iter 136, loss=0.4423270523548126 (2.833077s)
INFO:niftynet:2019-02-12 14:25:14,912: training iter 137, loss=0.6977746486663818 (5.239115s)
INFO:niftynet:2019-02-12 14:25:19,101: training iter 138, loss=0.675786018371582 (4.187923s)
INFO:niftynet:2019-02-12 14:25:21,654: training iter 139, loss=0.6373443007469177 (2.552744s)
INFO:niftynet:2019-02-12 14:25:24,741: training iter 140, loss=0.44487619400024414 (3.085873s)
INFO:niftynet:2019-02-12 14:25:27,360:     validation iter 140, loss=0.5814850330352783 (2.615609s)
INFO:niftynet:2019-02-12 14:25:29,565: training iter 141, loss=0.5373625755310059 (2.202542s)
INFO:niftynet:2019-02-12 14:25:34,062: training iter 142, loss=0.5519432425498962 (4.497251s)
INFO:niftynet:2019-02-12 14:25:37,245: training iter 143, loss=0.7235158681869507 (3.182628s)
INFO:niftynet:2019-02-12 14:25:39,577: training iter 144, loss=0.7274406552314758 (2.330328s)
INFO:niftynet:2019-02-12 14:25:42,849: training iter 145, loss=0.7103838920593262 (3.271427s)
INFO:niftynet:2019-02-12 14:25:46,988: training iter 146, loss=0.5223811864852905 (4.137949s)
INFO:niftynet:2019-02-12 14:25:49,770: training iter 147, loss=0.5155254006385803 (2.760412s)
INFO:niftynet:2019-02-12 14:25:53,336: training iter 148, loss=0.6061885952949524 (3.564734s)
INFO:niftynet:2019-02-12 14:25:57,128: training iter 149, loss=0.6092169284820557 (3.790443s)
INFO:niftynet:2019-02-12 14:26:01,732: training iter 150, loss=0.5317542552947998 (4.593482s)
INFO:niftynet:2019-02-12 14:26:04,114: iter 150 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:26:06,260:     validation iter 150, loss=0.7524133324623108 (1.824145s)
INFO:niftynet:2019-02-12 14:26:09,089: training iter 151, loss=0.5648540258407593 (2.827575s)
INFO:niftynet:2019-02-12 14:26:11,449: training iter 152, loss=0.666853666305542 (2.359336s)
INFO:niftynet:2019-02-12 14:26:14,209: training iter 153, loss=0.6537309885025024 (2.759511s)
INFO:niftynet:2019-02-12 14:26:18,959: training iter 154, loss=0.6230369210243225 (4.749238s)
INFO:niftynet:2019-02-12 14:26:21,953: training iter 155, loss=0.6537220478057861 (2.993136s)
INFO:niftynet:2019-02-12 14:26:24,707: training iter 156, loss=0.5706515312194824 (2.751721s)
INFO:niftynet:2019-02-12 14:26:28,211: training iter 157, loss=0.46676743030548096 (3.503447s)
INFO:niftynet:2019-02-12 14:26:31,016: training iter 158, loss=0.6487860083580017 (2.803981s)
INFO:niftynet:2019-02-12 14:26:35,037: training iter 159, loss=0.7185937166213989 (4.021334s)
INFO:niftynet:2019-02-12 14:26:37,789: training iter 160, loss=0.5728851556777954 (2.750223s)
INFO:niftynet:2019-02-12 14:26:39,780:     validation iter 160, loss=0.6971741914749146 (1.984523s)
INFO:niftynet:2019-02-12 14:26:42,628: training iter 161, loss=0.5337386131286621 (2.845630s)
INFO:niftynet:2019-02-12 14:26:45,152: training iter 162, loss=0.3947931230068207 (2.524172s)
INFO:niftynet:2019-02-12 14:26:50,122: training iter 163, loss=0.4703906178474426 (4.968325s)
INFO:niftynet:2019-02-12 14:26:53,314: training iter 164, loss=0.6991987824440002 (3.192027s)
INFO:niftynet:2019-02-12 14:26:56,167: training iter 165, loss=0.5426371693611145 (2.851861s)
INFO:niftynet:2019-02-12 14:27:00,015: training iter 166, loss=0.4617776870727539 (3.846269s)
INFO:niftynet:2019-02-12 14:27:03,510: training iter 167, loss=0.514885663986206 (3.495390s)
INFO:niftynet:2019-02-12 14:27:06,402: training iter 168, loss=0.702126145362854 (2.867608s)
INFO:niftynet:2019-02-12 14:27:09,262: training iter 169, loss=0.5379576683044434 (2.860151s)
INFO:niftynet:2019-02-12 14:27:14,737: training iter 170, loss=0.5872707366943359 (5.468087s)
INFO:niftynet:2019-02-12 14:27:17,355:     validation iter 170, loss=0.6228074431419373 (2.615364s)
INFO:niftynet:2019-02-12 14:27:20,021: training iter 171, loss=0.47862592339515686 (2.664061s)
INFO:niftynet:2019-02-12 14:27:22,336: training iter 172, loss=0.6583003401756287 (2.311050s)
INFO:niftynet:2019-02-12 14:27:25,039: training iter 173, loss=0.6040747165679932 (2.701780s)
INFO:niftynet:2019-02-12 14:27:27,670: training iter 174, loss=0.581145167350769 (2.630940s)
INFO:niftynet:2019-02-12 14:27:33,445: training iter 175, loss=0.5041656494140625 (5.775266s)
INFO:niftynet:2019-02-12 14:27:34,815: iter 175 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:27:38,460: training iter 176, loss=0.6650488376617432 (3.642325s)
INFO:niftynet:2019-02-12 14:27:41,063: training iter 177, loss=0.7572664022445679 (2.602864s)
INFO:niftynet:2019-02-12 14:27:43,834: training iter 178, loss=0.5928851366043091 (2.769696s)
INFO:niftynet:2019-02-12 14:27:47,555: training iter 179, loss=0.546616792678833 (3.720737s)
INFO:niftynet:2019-02-12 14:27:53,507: training iter 180, loss=0.532203197479248 (5.951263s)
INFO:niftynet:2019-02-12 14:27:55,739:     validation iter 180, loss=0.7132449150085449 (2.229030s)
INFO:niftynet:2019-02-12 14:27:58,519: training iter 181, loss=0.5580954551696777 (2.776522s)
INFO:niftynet:2019-02-12 14:28:01,022: training iter 182, loss=0.550201416015625 (2.502848s)
INFO:niftynet:2019-02-12 14:28:04,350: training iter 183, loss=0.5756714940071106 (3.327258s)
INFO:niftynet:2019-02-12 14:28:06,738: training iter 184, loss=0.6798279285430908 (2.388141s)
INFO:niftynet:2019-02-12 14:28:10,171: training iter 185, loss=0.7474925518035889 (3.432579s)
INFO:niftynet:2019-02-12 14:28:13,090: training iter 186, loss=0.7103719711303711 (2.913412s)
INFO:niftynet:2019-02-12 14:28:17,275: training iter 187, loss=0.7277116775512695 (4.183662s)
INFO:niftynet:2019-02-12 14:28:19,865: training iter 188, loss=0.760757327079773 (2.588860s)
INFO:niftynet:2019-02-12 14:28:22,340: training iter 189, loss=0.6727108359336853 (2.475193s)
INFO:niftynet:2019-02-12 14:28:30,485: training iter 190, loss=0.5622745752334595 (8.144468s)
INFO:niftynet:2019-02-12 14:28:32,998:     validation iter 190, loss=0.6043635010719299 (2.511318s)
INFO:niftynet:2019-02-12 14:28:35,648: training iter 191, loss=0.5537039041519165 (2.642055s)
INFO:niftynet:2019-02-12 14:28:38,788: training iter 192, loss=0.6699811220169067 (3.139372s)
INFO:niftynet:2019-02-12 14:28:41,886: training iter 193, loss=0.7525613903999329 (3.098093s)
INFO:niftynet:2019-02-12 14:28:46,262: training iter 194, loss=0.6358492374420166 (4.375117s)
INFO:niftynet:2019-02-12 14:28:49,191: training iter 195, loss=0.5596354007720947 (2.928453s)
INFO:niftynet:2019-02-12 14:28:52,005: training iter 196, loss=0.6122403144836426 (2.811949s)
INFO:niftynet:2019-02-12 14:28:54,607: training iter 197, loss=0.552211582660675 (2.600972s)
INFO:niftynet:2019-02-12 14:28:59,089: training iter 198, loss=0.6297401189804077 (4.476729s)
INFO:niftynet:2019-02-12 14:29:02,099: training iter 199, loss=0.5683605074882507 (3.010510s)
INFO:niftynet:2019-02-12 14:29:04,587: training iter 200, loss=0.625020444393158 (2.485908s)
INFO:niftynet:2019-02-12 14:29:06,723: iter 200 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:29:08,978:     validation iter 200, loss=0.6299239993095398 (2.251578s)
INFO:niftynet:2019-02-12 14:29:11,430: training iter 201, loss=0.5855884552001953 (2.450454s)
INFO:niftynet:2019-02-12 14:29:14,797: training iter 202, loss=0.6640591025352478 (3.367084s)
INFO:niftynet:2019-02-12 14:29:17,075: training iter 203, loss=0.6046761870384216 (2.277148s)
INFO:niftynet:2019-02-12 14:29:20,062: training iter 204, loss=0.6854014992713928 (2.987514s)
INFO:niftynet:2019-02-12 14:29:22,368: training iter 205, loss=0.7630200386047363 (2.304442s)
INFO:niftynet:2019-02-12 14:29:24,923: training iter 206, loss=0.5225220918655396 (2.550487s)
INFO:niftynet:2019-02-12 14:29:27,123: training iter 207, loss=0.661725640296936 (2.199881s)
INFO:niftynet:2019-02-12 14:29:29,140: training iter 208, loss=0.6160030364990234 (2.017262s)
INFO:niftynet:2019-02-12 14:29:30,979: training iter 209, loss=0.6564350128173828 (1.838985s)
INFO:niftynet:2019-02-12 14:29:46,875: training iter 210, loss=0.5669909715652466 (15.895480s)
INFO:niftynet:2019-02-12 14:29:49,347:     validation iter 210, loss=0.46243202686309814 (2.468781s)
INFO:niftynet:2019-02-12 14:29:51,313: training iter 211, loss=0.5245881080627441 (1.964416s)
INFO:niftynet:2019-02-12 14:29:53,843: training iter 212, loss=0.7662296295166016 (2.529561s)
INFO:niftynet:2019-02-12 14:29:56,197: training iter 213, loss=0.5024591684341431 (2.353520s)
INFO:niftynet:2019-02-12 14:29:58,439: training iter 214, loss=0.6254101395606995 (2.242079s)
INFO:niftynet:2019-02-12 14:30:02,282: training iter 215, loss=0.5174441337585449 (3.842379s)
INFO:niftynet:2019-02-12 14:30:05,162: training iter 216, loss=0.698292076587677 (2.877622s)
INFO:niftynet:2019-02-12 14:30:08,093: training iter 217, loss=0.680396318435669 (2.930398s)
INFO:niftynet:2019-02-12 14:30:10,668: training iter 218, loss=0.5043543577194214 (2.574496s)
INFO:niftynet:2019-02-12 14:30:13,728: training iter 219, loss=0.5645978450775146 (3.059806s)
INFO:niftynet:2019-02-12 14:30:18,387: training iter 220, loss=0.493069589138031 (4.658648s)
INFO:niftynet:2019-02-12 14:30:21,366:     validation iter 220, loss=0.5980615615844727 (2.976659s)
INFO:niftynet:2019-02-12 14:30:24,279: training iter 221, loss=0.6664289832115173 (2.910879s)
INFO:niftynet:2019-02-12 14:30:27,369: training iter 222, loss=0.5362905263900757 (3.088815s)
INFO:niftynet:2019-02-12 14:30:29,919: training iter 223, loss=0.5607409477233887 (2.548424s)
INFO:niftynet:2019-02-12 14:30:34,264: training iter 224, loss=0.5297427177429199 (4.343444s)
INFO:niftynet:2019-02-12 14:30:36,846: training iter 225, loss=0.6389303207397461 (2.581481s)
INFO:niftynet:2019-02-12 14:30:38,530: iter 225 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:30:41,041: training iter 226, loss=0.4347572326660156 (2.508924s)
INFO:niftynet:2019-02-12 14:30:44,229: training iter 227, loss=0.6310242414474487 (3.187839s)
INFO:niftynet:2019-02-12 14:30:46,945: training iter 228, loss=0.6271897554397583 (2.694502s)
INFO:niftynet:2019-02-12 14:30:49,881: training iter 229, loss=0.62867671251297 (2.936048s)
INFO:niftynet:2019-02-12 14:30:52,658: training iter 230, loss=0.46972790360450745 (2.725806s)
INFO:niftynet:2019-02-12 14:31:04,473:     validation iter 230, loss=0.7475223541259766 (11.812422s)
INFO:niftynet:2019-02-12 14:31:07,361: training iter 231, loss=0.6186596155166626 (2.886453s)
INFO:niftynet:2019-02-12 14:31:09,584: training iter 232, loss=0.6650668978691101 (2.222131s)
INFO:niftynet:2019-02-12 14:31:12,621: training iter 233, loss=0.6664500832557678 (3.035785s)
INFO:niftynet:2019-02-12 14:31:15,175: training iter 234, loss=0.6619488000869751 (2.553333s)
INFO:niftynet:2019-02-12 14:31:18,023: training iter 235, loss=0.5970103144645691 (2.848208s)
INFO:niftynet:2019-02-12 14:31:20,720: training iter 236, loss=0.4316413700580597 (2.695262s)
INFO:niftynet:2019-02-12 14:31:22,974: training iter 237, loss=0.6473288536071777 (2.253174s)
INFO:niftynet:2019-02-12 14:31:25,317: training iter 238, loss=0.5096414685249329 (2.342273s)
INFO:niftynet:2019-02-12 14:31:28,159: training iter 239, loss=0.6193927526473999 (2.842324s)
INFO:niftynet:2019-02-12 14:31:31,079: training iter 240, loss=0.7004492878913879 (2.917804s)
INFO:niftynet:2019-02-12 14:31:33,011:     validation iter 240, loss=0.46946460008621216 (1.928749s)
INFO:niftynet:2019-02-12 14:31:35,893: training iter 241, loss=0.7005412578582764 (2.879104s)
INFO:niftynet:2019-02-12 14:31:38,263: training iter 242, loss=0.5522454380989075 (2.358362s)
INFO:niftynet:2019-02-12 14:31:40,712: training iter 243, loss=0.5400729775428772 (2.448023s)
INFO:niftynet:2019-02-12 14:31:45,468: training iter 244, loss=0.5460022687911987 (4.755579s)
INFO:niftynet:2019-02-12 14:31:51,027: training iter 245, loss=0.5389262437820435 (5.558817s)
INFO:niftynet:2019-02-12 14:31:53,772: training iter 246, loss=0.6012152433395386 (2.743619s)
INFO:niftynet:2019-02-12 14:31:56,939: training iter 247, loss=0.7615781426429749 (3.165134s)
INFO:niftynet:2019-02-12 14:31:59,438: training iter 248, loss=0.4147741198539734 (2.498986s)
INFO:niftynet:2019-02-12 14:32:05,358: training iter 249, loss=0.4257810711860657 (5.919318s)
INFO:niftynet:2019-02-12 14:32:08,443: training iter 250, loss=0.6848797798156738 (3.084890s)
INFO:niftynet:2019-02-12 14:32:10,520: iter 250 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:32:12,787:     validation iter 250, loss=0.5837993025779724 (2.265827s)
INFO:niftynet:2019-02-12 14:32:15,362: training iter 251, loss=0.4369905889034271 (2.573494s)
INFO:niftynet:2019-02-12 14:32:17,980: training iter 252, loss=0.6016435623168945 (2.617275s)
INFO:niftynet:2019-02-12 14:32:21,114: training iter 253, loss=0.64039146900177 (3.132138s)
INFO:niftynet:2019-02-12 14:32:23,575: training iter 254, loss=0.5951738357543945 (2.460810s)
INFO:niftynet:2019-02-12 14:32:26,482: training iter 255, loss=0.6150562763214111 (2.906896s)
INFO:niftynet:2019-02-12 14:32:29,125: training iter 256, loss=0.6219743490219116 (2.623425s)
INFO:niftynet:2019-02-12 14:32:35,959: training iter 257, loss=0.6014240384101868 (6.832350s)
INFO:niftynet:2019-02-12 14:32:38,407: training iter 258, loss=0.5371564626693726 (2.447386s)
INFO:niftynet:2019-02-12 14:32:40,871: training iter 259, loss=0.6374050378799438 (2.463698s)
INFO:niftynet:2019-02-12 14:32:43,836: training iter 260, loss=0.6679227948188782 (2.964246s)
INFO:niftynet:2019-02-12 14:32:45,732:     validation iter 260, loss=0.5189709663391113 (1.893256s)
INFO:niftynet:2019-02-12 14:32:50,630: training iter 261, loss=0.7629704475402832 (4.896307s)
INFO:niftynet:2019-02-12 14:32:52,854: training iter 262, loss=0.5518771409988403 (2.221849s)
INFO:niftynet:2019-02-12 14:32:55,553: training iter 263, loss=0.6678582429885864 (2.685230s)
INFO:niftynet:2019-02-12 14:32:57,963: training iter 264, loss=0.6787576675415039 (2.409771s)
INFO:niftynet:2019-02-12 14:33:05,366: training iter 265, loss=0.6130944490432739 (7.403267s)
INFO:niftynet:2019-02-12 14:33:07,608: training iter 266, loss=0.5511505603790283 (2.239782s)
INFO:niftynet:2019-02-12 14:33:10,373: training iter 267, loss=0.6420196890830994 (2.765105s)
INFO:niftynet:2019-02-12 14:33:12,696: training iter 268, loss=0.5887256860733032 (2.321820s)
INFO:niftynet:2019-02-12 14:33:18,463: training iter 269, loss=0.6041558384895325 (5.765777s)
INFO:niftynet:2019-02-12 14:33:20,961: training iter 270, loss=0.6841830015182495 (2.496960s)
INFO:niftynet:2019-02-12 14:33:23,236:     validation iter 270, loss=0.5315383672714233 (2.273770s)
INFO:niftynet:2019-02-12 14:33:26,019: training iter 271, loss=0.5031460523605347 (2.779164s)
INFO:niftynet:2019-02-12 14:33:28,571: training iter 272, loss=0.46752384305000305 (2.550803s)
INFO:niftynet:2019-02-12 14:33:31,529: training iter 273, loss=0.536393404006958 (2.957767s)
INFO:niftynet:2019-02-12 14:33:37,046: training iter 274, loss=0.5172387361526489 (5.516397s)
INFO:niftynet:2019-02-12 14:33:39,562: training iter 275, loss=0.5037986040115356 (2.514696s)
INFO:niftynet:2019-02-12 14:33:41,832: iter 275 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:33:44,350: training iter 276, loss=0.5833547115325928 (2.515019s)
INFO:niftynet:2019-02-12 14:33:47,070: training iter 277, loss=0.539472222328186 (2.719289s)
INFO:niftynet:2019-02-12 14:33:50,532: training iter 278, loss=0.6538406610488892 (3.461471s)
INFO:niftynet:2019-02-12 14:33:55,348: training iter 279, loss=0.7035140991210938 (4.815700s)
INFO:niftynet:2019-02-12 14:33:57,802: training iter 280, loss=0.5961645245552063 (2.453557s)
INFO:niftynet:2019-02-12 14:34:00,033:     validation iter 280, loss=0.4445417821407318 (2.229116s)
INFO:niftynet:2019-02-12 14:34:03,296: training iter 281, loss=0.6840052008628845 (3.261465s)
INFO:niftynet:2019-02-12 14:34:06,567: training iter 282, loss=0.6420329809188843 (3.270830s)
INFO:niftynet:2019-02-12 14:34:10,152: training iter 283, loss=0.6594523191452026 (3.583977s)
INFO:niftynet:2019-02-12 14:34:13,415: training iter 284, loss=0.6381484270095825 (3.262165s)
INFO:niftynet:2019-02-12 14:34:16,211: training iter 285, loss=0.4700758755207062 (2.794118s)
INFO:niftynet:2019-02-12 14:34:21,563: training iter 286, loss=0.6828061938285828 (5.349591s)
INFO:niftynet:2019-02-12 14:34:24,468: training iter 287, loss=0.6327094435691833 (2.904348s)
INFO:niftynet:2019-02-12 14:34:26,872: training iter 288, loss=0.6054466962814331 (2.401907s)
INFO:niftynet:2019-02-12 14:34:29,286: training iter 289, loss=0.4911758005619049 (2.413239s)
INFO:niftynet:2019-02-12 14:34:35,339: training iter 290, loss=0.5968878865242004 (6.052895s)
INFO:niftynet:2019-02-12 14:34:37,549:     validation iter 290, loss=0.5145837068557739 (2.208218s)
INFO:niftynet:2019-02-12 14:34:40,543: training iter 291, loss=0.6267967224121094 (2.991829s)
INFO:niftynet:2019-02-12 14:34:43,338: training iter 292, loss=0.7320282459259033 (2.794763s)
INFO:niftynet:2019-02-12 14:34:45,640: training iter 293, loss=0.5195626020431519 (2.301043s)
INFO:niftynet:2019-02-12 14:34:48,516: training iter 294, loss=0.6915084719657898 (2.875726s)
INFO:niftynet:2019-02-12 14:34:52,806: training iter 295, loss=0.5713765025138855 (4.289635s)
INFO:niftynet:2019-02-12 14:34:56,182: training iter 296, loss=0.7014051675796509 (3.370583s)
INFO:niftynet:2019-02-12 14:34:59,178: training iter 297, loss=0.6997321844100952 (2.996049s)
INFO:niftynet:2019-02-12 14:35:02,136: training iter 298, loss=0.5599831342697144 (2.958009s)
INFO:niftynet:2019-02-12 14:35:04,698: training iter 299, loss=0.5030714869499207 (2.561572s)
INFO:niftynet:2019-02-12 14:35:10,823: training iter 300, loss=0.5378848910331726 (6.121565s)
INFO:niftynet:2019-02-12 14:35:12,680: iter 300 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:35:14,837:     validation iter 300, loss=0.4828283488750458 (2.137529s)
INFO:niftynet:2019-02-12 14:35:17,983: training iter 301, loss=0.42401260137557983 (3.143876s)
INFO:niftynet:2019-02-12 14:35:20,508: training iter 302, loss=0.7134838104248047 (2.523844s)
INFO:niftynet:2019-02-12 14:35:23,323: training iter 303, loss=0.41025397181510925 (2.814718s)
INFO:niftynet:2019-02-12 14:35:25,898: training iter 304, loss=0.6592504382133484 (2.573606s)
INFO:niftynet:2019-02-12 14:35:30,008: training iter 305, loss=0.6327183246612549 (4.109199s)
INFO:niftynet:2019-02-12 14:35:32,477: training iter 306, loss=0.6261296272277832 (2.464840s)
INFO:niftynet:2019-02-12 14:35:35,713: training iter 307, loss=0.5487179756164551 (3.235210s)
INFO:niftynet:2019-02-12 14:35:39,884: training iter 308, loss=0.44855058193206787 (4.169566s)
INFO:niftynet:2019-02-12 14:35:44,997: training iter 309, loss=0.5515669584274292 (5.111215s)
INFO:niftynet:2019-02-12 14:35:47,596: training iter 310, loss=0.506149411201477 (2.598945s)
INFO:niftynet:2019-02-12 14:35:50,025:     validation iter 310, loss=0.4154069423675537 (2.425542s)
INFO:niftynet:2019-02-12 14:35:53,043: training iter 311, loss=0.5535709857940674 (3.015805s)
INFO:niftynet:2019-02-12 14:35:55,691: training iter 312, loss=0.451588898897171 (2.646754s)
INFO:niftynet:2019-02-12 14:35:58,703: training iter 313, loss=0.6181615591049194 (3.011724s)
INFO:niftynet:2019-02-12 14:36:01,264: training iter 314, loss=0.680416464805603 (2.560601s)
INFO:niftynet:2019-02-12 14:36:04,741: training iter 315, loss=0.5716725587844849 (3.476792s)
INFO:niftynet:2019-02-12 14:36:08,736: training iter 316, loss=0.5527306199073792 (3.992405s)
INFO:niftynet:2019-02-12 14:36:11,521: training iter 317, loss=0.543587863445282 (2.784464s)
INFO:niftynet:2019-02-12 14:36:15,583: training iter 318, loss=0.6749607920646667 (4.061275s)
INFO:niftynet:2019-02-12 14:36:19,539: training iter 319, loss=0.6163066625595093 (3.954425s)
INFO:niftynet:2019-02-12 14:36:21,865: training iter 320, loss=0.6738166809082031 (2.324959s)
INFO:niftynet:2019-02-12 14:36:23,785:     validation iter 320, loss=0.6077357530593872 (1.918377s)
INFO:niftynet:2019-02-12 14:36:25,801: training iter 321, loss=0.6077903509140015 (2.013062s)
INFO:niftynet:2019-02-12 14:36:27,889: training iter 322, loss=0.5804585218429565 (2.088190s)
INFO:niftynet:2019-02-12 14:36:30,300: training iter 323, loss=0.6274533271789551 (2.410764s)
INFO:niftynet:2019-02-12 14:36:42,406: training iter 324, loss=0.4847950339317322 (12.104234s)
INFO:niftynet:2019-02-12 14:36:45,295: training iter 325, loss=0.6009763479232788 (2.888037s)
INFO:niftynet:2019-02-12 14:36:47,529: iter 325 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:36:50,100: training iter 326, loss=0.44930216670036316 (2.558614s)
INFO:niftynet:2019-02-12 14:36:52,486: training iter 327, loss=0.6038058400154114 (2.385150s)
INFO:niftynet:2019-02-12 14:36:56,293: training iter 328, loss=0.532538652420044 (3.806205s)
INFO:niftynet:2019-02-12 14:36:58,572: training iter 329, loss=0.5876531004905701 (2.279106s)
INFO:niftynet:2019-02-12 14:37:03,353: training iter 330, loss=0.7668790817260742 (4.777778s)
INFO:niftynet:2019-02-12 14:37:05,574:     validation iter 330, loss=0.7017834186553955 (2.218336s)
INFO:niftynet:2019-02-12 14:37:08,214: training iter 331, loss=0.5153359770774841 (2.638706s)
INFO:niftynet:2019-02-12 14:37:11,721: training iter 332, loss=0.556062638759613 (3.501332s)
INFO:niftynet:2019-02-12 14:37:14,706: training iter 333, loss=0.7084020972251892 (2.982814s)
INFO:niftynet:2019-02-12 14:37:17,805: training iter 334, loss=0.5835132598876953 (3.098837s)
INFO:niftynet:2019-02-12 14:37:20,573: training iter 335, loss=0.4561429023742676 (2.766237s)
INFO:niftynet:2019-02-12 14:37:27,270: training iter 336, loss=0.5458696484565735 (6.695737s)
INFO:niftynet:2019-02-12 14:37:30,164: training iter 337, loss=0.743344783782959 (2.891276s)
INFO:niftynet:2019-02-12 14:37:32,679: training iter 338, loss=0.3808138966560364 (2.514425s)
INFO:niftynet:2019-02-12 14:37:35,246: training iter 339, loss=0.49399179220199585 (2.567039s)
INFO:niftynet:2019-02-12 14:37:41,028: training iter 340, loss=0.7240530252456665 (5.780303s)
INFO:niftynet:2019-02-12 14:37:43,435:     validation iter 340, loss=0.7378904819488525 (2.405575s)
INFO:niftynet:2019-02-12 14:37:45,914: training iter 341, loss=0.5567313432693481 (2.476538s)
INFO:niftynet:2019-02-12 14:37:48,772: training iter 342, loss=0.5721085071563721 (2.858206s)
INFO:niftynet:2019-02-12 14:37:51,146: training iter 343, loss=0.5528069734573364 (2.373494s)
INFO:niftynet:2019-02-12 14:37:55,726: training iter 344, loss=0.6261855363845825 (4.579488s)
INFO:niftynet:2019-02-12 14:37:59,092: training iter 345, loss=0.5885454416275024 (3.365751s)
INFO:niftynet:2019-02-12 14:38:01,718: training iter 346, loss=0.6304478049278259 (2.623224s)
INFO:niftynet:2019-02-12 14:38:04,520: training iter 347, loss=0.6910076141357422 (2.800958s)
INFO:niftynet:2019-02-12 14:38:08,841: training iter 348, loss=0.5332082509994507 (4.319669s)
INFO:niftynet:2019-02-12 14:38:11,569: training iter 349, loss=0.4920104444026947 (2.727644s)
INFO:niftynet:2019-02-12 14:38:14,365: training iter 350, loss=0.3872866928577423 (2.796118s)
INFO:niftynet:2019-02-12 14:38:16,076: iter 350 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:38:18,741:     validation iter 350, loss=0.557016134262085 (2.662078s)
INFO:niftynet:2019-02-12 14:38:21,267: training iter 351, loss=0.5407322645187378 (2.524762s)
INFO:niftynet:2019-02-12 14:38:23,724: training iter 352, loss=0.5580345392227173 (2.455776s)
INFO:niftynet:2019-02-12 14:38:26,912: training iter 353, loss=0.5304313898086548 (3.188385s)
INFO:niftynet:2019-02-12 14:38:29,679: training iter 354, loss=0.5295514464378357 (2.613600s)
INFO:niftynet:2019-02-12 14:38:34,920: training iter 355, loss=0.6655089259147644 (5.239916s)
INFO:niftynet:2019-02-12 14:38:37,857: training iter 356, loss=0.441806823015213 (2.935441s)
INFO:niftynet:2019-02-12 14:38:40,526: training iter 357, loss=0.43658003211021423 (2.667689s)
INFO:niftynet:2019-02-12 14:38:43,376: training iter 358, loss=0.39951086044311523 (2.848701s)
INFO:niftynet:2019-02-12 14:38:49,501: training iter 359, loss=0.6202137470245361 (6.125552s)
INFO:niftynet:2019-02-12 14:38:52,519: training iter 360, loss=0.690819263458252 (3.017027s)
INFO:niftynet:2019-02-12 14:38:54,834:     validation iter 360, loss=0.5055899024009705 (2.312982s)
INFO:niftynet:2019-02-12 14:38:57,609: training iter 361, loss=0.5830410718917847 (2.771308s)
INFO:niftynet:2019-02-12 14:38:59,896: training iter 362, loss=0.562620997428894 (2.286671s)
INFO:niftynet:2019-02-12 14:39:02,890: training iter 363, loss=0.70965576171875 (2.993863s)
INFO:niftynet:2019-02-12 14:39:05,477: training iter 364, loss=0.49257874488830566 (2.585013s)
INFO:niftynet:2019-02-12 14:39:10,158: training iter 365, loss=0.5236330032348633 (4.680233s)
INFO:niftynet:2019-02-12 14:39:12,563: training iter 366, loss=0.6281155943870544 (2.403124s)
INFO:niftynet:2019-02-12 14:39:15,023: training iter 367, loss=0.4084765315055847 (2.459134s)
INFO:niftynet:2019-02-12 14:39:19,677: training iter 368, loss=0.6506639719009399 (4.653856s)
INFO:niftynet:2019-02-12 14:39:22,301: training iter 369, loss=0.4587659537792206 (2.623886s)
INFO:niftynet:2019-02-12 14:39:26,732: training iter 370, loss=0.6085365414619446 (4.430135s)
INFO:niftynet:2019-02-12 14:39:28,519:     validation iter 370, loss=0.6769227385520935 (1.785272s)
INFO:niftynet:2019-02-12 14:39:30,637: training iter 371, loss=0.748261570930481 (2.116685s)
INFO:niftynet:2019-02-12 14:39:33,631: training iter 372, loss=0.6169866323471069 (2.992727s)
INFO:niftynet:2019-02-12 14:39:37,311: training iter 373, loss=0.6786890029907227 (3.652399s)
INFO:niftynet:2019-02-12 14:39:40,687: training iter 374, loss=0.6072865724563599 (3.376163s)
INFO:niftynet:2019-02-12 14:39:43,136: training iter 375, loss=0.47157418727874756 (2.447520s)
INFO:niftynet:2019-02-12 14:39:44,574: iter 375 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:39:47,404: training iter 376, loss=0.5182538032531738 (2.828614s)
INFO:niftynet:2019-02-12 14:39:49,854: training iter 377, loss=0.5527317523956299 (2.449029s)
INFO:niftynet:2019-02-12 14:39:52,125: training iter 378, loss=0.5276595950126648 (2.269665s)
INFO:niftynet:2019-02-12 14:39:57,751: training iter 379, loss=0.6262610554695129 (5.625428s)
INFO:niftynet:2019-02-12 14:40:03,616: training iter 380, loss=0.5045506954193115 (5.863876s)
INFO:niftynet:2019-02-12 14:40:15,359:     validation iter 380, loss=0.7015490531921387 (11.741327s)
INFO:niftynet:2019-02-12 14:40:18,574: training iter 381, loss=0.6985583305358887 (3.213387s)
INFO:niftynet:2019-02-12 14:40:21,496: training iter 382, loss=0.5589922070503235 (2.920278s)
INFO:niftynet:2019-02-12 14:40:23,627: training iter 383, loss=0.5280698537826538 (2.130259s)
INFO:niftynet:2019-02-12 14:40:26,058: training iter 384, loss=0.5591137409210205 (2.431594s)
INFO:niftynet:2019-02-12 14:40:28,750: training iter 385, loss=0.6822777986526489 (2.689563s)
INFO:niftynet:2019-02-12 14:40:31,482: training iter 386, loss=0.6510379314422607 (2.729422s)
INFO:niftynet:2019-02-12 14:40:33,908: training iter 387, loss=0.5879579186439514 (2.423963s)
INFO:niftynet:2019-02-12 14:40:36,463: training iter 388, loss=0.5497439503669739 (2.554938s)
INFO:niftynet:2019-02-12 14:40:39,090: training iter 389, loss=0.6447438597679138 (2.625065s)
INFO:niftynet:2019-02-12 14:40:41,249: training iter 390, loss=0.6800341606140137 (2.158654s)
INFO:niftynet:2019-02-12 14:40:43,843:     validation iter 390, loss=0.6928935050964355 (2.592757s)
INFO:niftynet:2019-02-12 14:40:46,635: training iter 391, loss=0.6124202013015747 (2.789269s)
INFO:niftynet:2019-02-12 14:40:49,249: training iter 392, loss=0.6957406401634216 (2.613359s)
INFO:niftynet:2019-02-12 14:40:53,432: training iter 393, loss=0.5607150793075562 (4.181203s)
INFO:niftynet:2019-02-12 14:40:55,958: training iter 394, loss=0.5917343497276306 (2.525803s)
INFO:niftynet:2019-02-12 14:41:01,207: training iter 395, loss=0.6731418967247009 (5.244413s)
INFO:niftynet:2019-02-12 14:41:03,839: training iter 396, loss=0.6306114196777344 (2.630686s)
INFO:niftynet:2019-02-12 14:41:08,232: training iter 397, loss=0.696098804473877 (4.392978s)
INFO:niftynet:2019-02-12 14:41:10,479: training iter 398, loss=0.5174192190170288 (2.246461s)
INFO:niftynet:2019-02-12 14:41:13,257: training iter 399, loss=0.498175710439682 (2.777604s)
INFO:niftynet:2019-02-12 14:41:16,654: training iter 400, loss=0.5094069242477417 (3.396299s)
INFO:niftynet:2019-02-12 14:41:18,799: iter 400 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 14:41:21,288:     validation iter 400, loss=0.6553184986114502 (2.475543s)
INFO:niftynet:2019-02-12 14:41:24,124: training iter 401, loss=0.601676344871521 (2.833524s)
INFO:niftynet:2019-02-12 14:41:27,878: training iter 402, loss=0.5523267388343811 (3.753383s)
INFO:niftynet:2019-02-12 14:41:30,526: training iter 403, loss=0.6239161491394043 (2.646140s)
INFO:niftynet:2019-02-12 14:41:33,056: training iter 404, loss=0.49314451217651367 (2.529980s)
WARNING:niftynet:2019-02-12 14:41:35,801: User cancelled application
INFO:niftynet:2019-02-12 14:41:35,801: cleaning up...
INFO:niftynet:2019-02-12 14:41:35,802: stopping sampling threads
INFO:niftynet:2019-02-12 14:41:37,206: iter 405 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
WARNING:niftynet:2019-02-12 14:41:44,920: stopped early, incomplete iterations.
INFO:niftynet:2019-02-12 14:41:44,920: SegmentationApplication stopped (time in second 1610.25).
INFO:niftynet:2019-02-12 15:01:13,184: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 15:01:13,184: starting segmentation application
INFO:niftynet:2019-02-12 15:01:13,185: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 15:01:13,193: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 15:01:13,198: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 15:01:13,202: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 15:01:13,205: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 15:01:13,208: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 15:01:34,870: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 15:01:34,870: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 15:01:36,579: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 15:01:36,579: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 15:01:36,580: training normalisation histogram references for image:{'T2', 'T1', 'T1ce'}, using 227 subjects
INFO:niftynet:2019-02-12 15:06:15,670: Looking for the set of unique discrete labels from input label using 227 subjects
WARNING:niftynet:2019-02-12 15:07:17,770: moved existing histogram reference file
 from /home/sathiesh/niftynet_brain/models/brats/histogram_ref_file.txt to /home/sathiesh/niftynet_brain/models/brats/histogram_ref_file.txt.backup
INFO:niftynet:2019-02-12 15:07:17,770: normalisation histogram reference models ready for image:('T1', 'T1ce', 'T2')
INFO:niftynet:2019-02-12 15:07:17,770: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 15:07:18,634: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 15:07:18,708: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 15:07:18,796: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 15:07:18,797: using DenseVNet
INFO:niftynet:2019-02-12 15:07:18,808: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 15:07:18,892: Initialising Dataset from 29 subjects...
INFO:niftynet:2019-02-12 15:07:33,445: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 15:08:57,775: training iter 1, loss=0.8646674752235413 (84.329488s)
INFO:niftynet:2019-02-12 15:09:00,610: training iter 2, loss=0.7684112787246704 (2.834516s)
INFO:niftynet:2019-02-12 15:09:04,119: training iter 3, loss=0.6833475828170776 (3.509024s)
INFO:niftynet:2019-02-12 15:09:06,791: training iter 4, loss=0.7610229253768921 (2.671441s)
INFO:niftynet:2019-02-12 15:09:18,392: training iter 5, loss=0.6726719737052917 (11.599405s)
INFO:niftynet:2019-02-12 15:09:20,862: training iter 6, loss=0.726507306098938 (2.466678s)
INFO:niftynet:2019-02-12 15:09:23,411: training iter 7, loss=0.6934430599212646 (2.546523s)
INFO:niftynet:2019-02-12 15:09:25,654: training iter 8, loss=0.5916327238082886 (2.243428s)
INFO:niftynet:2019-02-12 15:09:28,345: training iter 9, loss=0.645902156829834 (2.689152s)
INFO:niftynet:2019-02-12 15:09:30,881: training iter 10, loss=0.6774143576622009 (2.535137s)
INFO:niftynet:2019-02-12 15:10:35,827:     validation iter 10, loss=0.6987795829772949 (64.944488s)
INFO:niftynet:2019-02-12 15:10:44,887: training iter 11, loss=0.696950376033783 (9.050203s)
INFO:niftynet:2019-02-12 15:10:49,151: training iter 12, loss=0.6666269302368164 (4.263938s)
INFO:niftynet:2019-02-12 15:10:52,243: training iter 13, loss=0.7575402855873108 (3.091636s)
INFO:niftynet:2019-02-12 15:11:05,055: training iter 14, loss=0.7386028170585632 (12.811541s)
INFO:niftynet:2019-02-12 15:11:11,741: training iter 15, loss=0.6970559358596802 (6.684295s)
INFO:niftynet:2019-02-12 15:11:15,624: training iter 16, loss=0.703292727470398 (3.879624s)
INFO:niftynet:2019-02-12 15:11:20,497: training iter 17, loss=0.6934647560119629 (4.872330s)
INFO:niftynet:2019-02-12 15:11:25,472: training iter 18, loss=0.7147157192230225 (4.974441s)
INFO:niftynet:2019-02-12 15:11:28,041: training iter 19, loss=0.5336712598800659 (2.550884s)
INFO:niftynet:2019-02-12 15:11:30,696: training iter 20, loss=0.6185254454612732 (2.652862s)
INFO:niftynet:2019-02-12 15:11:32,854:     validation iter 20, loss=0.66644287109375 (2.156816s)
INFO:niftynet:2019-02-12 15:11:35,973: training iter 21, loss=0.6586903929710388 (3.115914s)
INFO:niftynet:2019-02-12 15:11:38,764: training iter 22, loss=0.7276819944381714 (2.790593s)
INFO:niftynet:2019-02-12 15:11:41,403: training iter 23, loss=0.5935524702072144 (2.638713s)
INFO:niftynet:2019-02-12 15:11:44,170: training iter 24, loss=0.5748105645179749 (2.765453s)
INFO:niftynet:2019-02-12 15:11:46,623: training iter 25, loss=0.6466426849365234 (2.452944s)
INFO:niftynet:2019-02-12 15:11:48,567: iter 25 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:11:50,674: training iter 26, loss=0.7475142478942871 (2.105790s)
INFO:niftynet:2019-02-12 15:11:53,019: training iter 27, loss=0.6595696806907654 (2.344985s)
INFO:niftynet:2019-02-12 15:11:55,383: training iter 28, loss=0.654151201248169 (2.363465s)
INFO:niftynet:2019-02-12 15:11:58,276: training iter 29, loss=0.5986325740814209 (2.892043s)
INFO:niftynet:2019-02-12 15:12:00,631: training iter 30, loss=0.7517937421798706 (2.353235s)
INFO:niftynet:2019-02-12 15:12:03,222:     validation iter 30, loss=0.6610626578330994 (2.589507s)
INFO:niftynet:2019-02-12 15:12:06,047: training iter 31, loss=0.6357024908065796 (2.820990s)
INFO:niftynet:2019-02-12 15:12:08,638: training iter 32, loss=0.5755991339683533 (2.582121s)
INFO:niftynet:2019-02-12 15:12:11,157: training iter 33, loss=0.5937894582748413 (2.512998s)
INFO:niftynet:2019-02-12 15:12:13,958: training iter 34, loss=0.6034616827964783 (2.800961s)
INFO:niftynet:2019-02-12 15:12:16,888: training iter 35, loss=0.725459635257721 (2.928990s)
INFO:niftynet:2019-02-12 15:12:19,639: training iter 36, loss=0.6292994022369385 (2.750090s)
INFO:niftynet:2019-02-12 15:12:22,479: training iter 37, loss=0.7078882455825806 (2.838683s)
INFO:niftynet:2019-02-12 15:12:25,747: training iter 38, loss=0.7738140821456909 (3.265676s)
INFO:niftynet:2019-02-12 15:12:28,780: training iter 39, loss=0.7395601272583008 (3.032111s)
INFO:niftynet:2019-02-12 15:12:31,903: training iter 40, loss=0.7282819151878357 (3.121368s)
INFO:niftynet:2019-02-12 15:12:33,972:     validation iter 40, loss=0.5304839611053467 (2.068258s)
INFO:niftynet:2019-02-12 15:12:36,612: training iter 41, loss=0.625673770904541 (2.636755s)
INFO:niftynet:2019-02-12 15:12:39,376: training iter 42, loss=0.7084859609603882 (2.762422s)
INFO:niftynet:2019-02-12 15:12:42,782: training iter 43, loss=0.7188971042633057 (3.405930s)
INFO:niftynet:2019-02-12 15:12:45,164: training iter 44, loss=0.676827073097229 (2.382004s)
INFO:niftynet:2019-02-12 15:12:47,917: training iter 45, loss=0.6628375053405762 (2.752221s)
INFO:niftynet:2019-02-12 15:12:50,448: training iter 46, loss=0.6151247620582581 (2.529550s)
INFO:niftynet:2019-02-12 15:12:52,931: training iter 47, loss=0.588376522064209 (2.481263s)
INFO:niftynet:2019-02-12 15:12:55,425: training iter 48, loss=0.5990331768989563 (2.493375s)
INFO:niftynet:2019-02-12 15:12:57,679: training iter 49, loss=0.5156940221786499 (2.253494s)
INFO:niftynet:2019-02-12 15:13:00,726: training iter 50, loss=0.6641327142715454 (3.046751s)
INFO:niftynet:2019-02-12 15:13:02,888: iter 50 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:13:04,866:     validation iter 50, loss=0.6058012247085571 (1.973924s)
INFO:niftynet:2019-02-12 15:13:07,302: training iter 51, loss=0.638971209526062 (2.434984s)
INFO:niftynet:2019-02-12 15:13:09,925: training iter 52, loss=0.5086711049079895 (2.622033s)
INFO:niftynet:2019-02-12 15:13:12,552: training iter 53, loss=0.6690993309020996 (2.627196s)
INFO:niftynet:2019-02-12 15:13:15,135: training iter 54, loss=0.5377253293991089 (2.582036s)
INFO:niftynet:2019-02-12 15:13:17,870: training iter 55, loss=0.5796575546264648 (2.734319s)
INFO:niftynet:2019-02-12 15:13:20,449: training iter 56, loss=0.45815202593803406 (2.577845s)
INFO:niftynet:2019-02-12 15:13:23,305: training iter 57, loss=0.614167332649231 (2.855048s)
INFO:niftynet:2019-02-12 15:13:26,106: training iter 58, loss=0.7103005647659302 (2.801176s)
INFO:niftynet:2019-02-12 15:13:28,955: training iter 59, loss=0.38672447204589844 (2.846931s)
INFO:niftynet:2019-02-12 15:13:31,540: training iter 60, loss=0.7295016646385193 (2.584801s)
INFO:niftynet:2019-02-12 15:13:33,670:     validation iter 60, loss=0.6357787847518921 (2.128032s)
INFO:niftynet:2019-02-12 15:13:36,537: training iter 61, loss=0.7388997673988342 (2.865207s)
INFO:niftynet:2019-02-12 15:13:39,132: training iter 62, loss=0.7572859525680542 (2.594967s)
INFO:niftynet:2019-02-12 15:13:41,498: training iter 63, loss=0.504825234413147 (2.364615s)
INFO:niftynet:2019-02-12 15:13:43,935: training iter 64, loss=0.6040014028549194 (2.436282s)
INFO:niftynet:2019-02-12 15:13:46,749: training iter 65, loss=0.5644537210464478 (2.814428s)
INFO:niftynet:2019-02-12 15:13:49,598: training iter 66, loss=0.6664072275161743 (2.845189s)
INFO:niftynet:2019-02-12 15:13:52,356: training iter 67, loss=0.5215067863464355 (2.758000s)
INFO:niftynet:2019-02-12 15:13:54,841: training iter 68, loss=0.5334622263908386 (2.483436s)
INFO:niftynet:2019-02-12 15:13:56,994: training iter 69, loss=0.5599925518035889 (2.152507s)
INFO:niftynet:2019-02-12 15:14:00,371: training iter 70, loss=0.7550861835479736 (3.377097s)
INFO:niftynet:2019-02-12 15:14:02,607:     validation iter 70, loss=0.6413232684135437 (2.215591s)
INFO:niftynet:2019-02-12 15:14:04,830: training iter 71, loss=0.46921297907829285 (2.170585s)
INFO:niftynet:2019-02-12 15:14:07,698: training iter 72, loss=0.5089899301528931 (2.867345s)
INFO:niftynet:2019-02-12 15:14:10,492: training iter 73, loss=0.6399993896484375 (2.793103s)
INFO:niftynet:2019-02-12 15:14:12,545: training iter 74, loss=0.7209857702255249 (2.053108s)
INFO:niftynet:2019-02-12 15:14:14,944: training iter 75, loss=0.6070296168327332 (2.398912s)
INFO:niftynet:2019-02-12 15:14:16,616: iter 75 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:14:19,552: training iter 76, loss=0.5720924139022827 (2.934742s)
INFO:niftynet:2019-02-12 15:14:22,507: training iter 77, loss=0.6254591941833496 (2.954442s)
INFO:niftynet:2019-02-12 15:14:24,957: training iter 78, loss=0.5726374387741089 (2.449680s)
INFO:niftynet:2019-02-12 15:14:27,497: training iter 79, loss=0.5915400981903076 (2.540430s)
INFO:niftynet:2019-02-12 15:14:30,227: training iter 80, loss=0.6263955235481262 (2.728961s)
INFO:niftynet:2019-02-12 15:14:32,447:     validation iter 80, loss=0.5893742442131042 (2.213117s)
INFO:niftynet:2019-02-12 15:14:34,788: training iter 81, loss=0.6281269788742065 (2.339946s)
INFO:niftynet:2019-02-12 15:14:37,357: training iter 82, loss=0.6465134024620056 (2.568152s)
INFO:niftynet:2019-02-12 15:14:39,837: training iter 83, loss=0.6065424680709839 (2.478415s)
INFO:niftynet:2019-02-12 15:14:42,520: training iter 84, loss=0.524293065071106 (2.683562s)
INFO:niftynet:2019-02-12 15:14:45,273: training iter 85, loss=0.5143290758132935 (2.744211s)
INFO:niftynet:2019-02-12 15:14:48,139: training iter 86, loss=0.7665489315986633 (2.862186s)
INFO:niftynet:2019-02-12 15:14:50,672: training iter 87, loss=0.5071784853935242 (2.533112s)
INFO:niftynet:2019-02-12 15:14:53,612: training iter 88, loss=0.5965224504470825 (2.938271s)
INFO:niftynet:2019-02-12 15:14:56,209: training iter 89, loss=0.5834455490112305 (2.596020s)
INFO:niftynet:2019-02-12 15:14:58,647: training iter 90, loss=0.5880545377731323 (2.437639s)
INFO:niftynet:2019-02-12 15:15:03,455:     validation iter 90, loss=0.6536352038383484 (4.802231s)
INFO:niftynet:2019-02-12 15:15:06,151: training iter 91, loss=0.6501158475875854 (2.693994s)
INFO:niftynet:2019-02-12 15:15:09,290: training iter 92, loss=0.5680072903633118 (3.139533s)
INFO:niftynet:2019-02-12 15:15:11,805: training iter 93, loss=0.6278149485588074 (2.513937s)
INFO:niftynet:2019-02-12 15:15:13,764: training iter 94, loss=0.650025486946106 (1.959270s)
INFO:niftynet:2019-02-12 15:15:16,235: training iter 95, loss=0.5909613370895386 (2.468925s)
INFO:niftynet:2019-02-12 15:15:19,980: training iter 96, loss=0.6318185329437256 (3.743943s)
INFO:niftynet:2019-02-12 15:15:32,216: training iter 97, loss=0.6067087650299072 (12.234426s)
INFO:niftynet:2019-02-12 15:15:35,655: training iter 98, loss=0.5476179122924805 (3.437687s)
INFO:niftynet:2019-02-12 15:15:38,560: training iter 99, loss=0.685732364654541 (2.903469s)
INFO:niftynet:2019-02-12 15:15:41,197: training iter 100, loss=0.6808004379272461 (2.635580s)
INFO:niftynet:2019-02-12 15:15:43,379: iter 100 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:15:45,667:     validation iter 100, loss=0.4932275712490082 (2.269083s)
INFO:niftynet:2019-02-12 15:15:48,736: training iter 101, loss=0.5947630405426025 (3.066225s)
INFO:niftynet:2019-02-12 15:15:51,464: training iter 102, loss=0.5311727523803711 (2.727759s)
INFO:niftynet:2019-02-12 15:15:53,676: training iter 103, loss=0.7044011354446411 (2.211780s)
INFO:niftynet:2019-02-12 15:15:56,217: training iter 104, loss=0.5211504101753235 (2.540079s)
INFO:niftynet:2019-02-12 15:16:02,709: training iter 105, loss=0.6878520250320435 (6.491513s)
INFO:niftynet:2019-02-12 15:16:05,514: training iter 106, loss=0.5058767795562744 (2.800971s)
INFO:niftynet:2019-02-12 15:16:07,755: training iter 107, loss=0.647801399230957 (2.240932s)
INFO:niftynet:2019-02-12 15:16:10,683: training iter 108, loss=0.5266069173812866 (2.926511s)
INFO:niftynet:2019-02-12 15:16:17,925: training iter 109, loss=0.5414778590202332 (7.240756s)
INFO:niftynet:2019-02-12 15:16:21,000: training iter 110, loss=0.6675341725349426 (3.075249s)
INFO:niftynet:2019-02-12 15:16:22,978:     validation iter 110, loss=0.4213322103023529 (1.975469s)
INFO:niftynet:2019-02-12 15:16:26,007: training iter 111, loss=0.6152591705322266 (3.026139s)
INFO:niftynet:2019-02-12 15:16:28,872: training iter 112, loss=0.512637197971344 (2.863990s)
INFO:niftynet:2019-02-12 15:16:32,043: training iter 113, loss=0.465094655752182 (3.170230s)
INFO:niftynet:2019-02-12 15:16:35,379: training iter 114, loss=0.6074828505516052 (3.334508s)
INFO:niftynet:2019-02-12 15:16:38,184: training iter 115, loss=0.4590607285499573 (2.804795s)
INFO:niftynet:2019-02-12 15:16:41,610: training iter 116, loss=0.6475151181221008 (3.422613s)
INFO:niftynet:2019-02-12 15:16:44,184: training iter 117, loss=0.5616312026977539 (2.573023s)
INFO:niftynet:2019-02-12 15:16:50,411: training iter 118, loss=0.6796161532402039 (6.226289s)
INFO:niftynet:2019-02-12 15:16:53,623: training iter 119, loss=0.6499338150024414 (3.211853s)
INFO:niftynet:2019-02-12 15:16:56,610: training iter 120, loss=0.4914669096469879 (2.986519s)
INFO:niftynet:2019-02-12 15:16:59,047:     validation iter 120, loss=0.709186851978302 (2.433941s)
INFO:niftynet:2019-02-12 15:17:02,074: training iter 121, loss=0.6859644651412964 (3.023066s)
INFO:niftynet:2019-02-12 15:17:07,237: training iter 122, loss=0.5789180397987366 (5.162829s)
INFO:niftynet:2019-02-12 15:17:10,173: training iter 123, loss=0.6395124793052673 (2.936221s)
INFO:niftynet:2019-02-12 15:17:12,922: training iter 124, loss=0.5058758854866028 (2.748572s)
INFO:niftynet:2019-02-12 15:17:15,906: training iter 125, loss=0.6516008377075195 (2.982819s)
INFO:niftynet:2019-02-12 15:17:18,107: iter 125 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:17:20,829: training iter 126, loss=0.655443549156189 (2.695075s)
INFO:niftynet:2019-02-12 15:17:23,084: training iter 127, loss=0.6288312673568726 (2.254324s)
INFO:niftynet:2019-02-12 15:17:25,853: training iter 128, loss=0.5601310729980469 (2.768561s)
INFO:niftynet:2019-02-12 15:17:28,751: training iter 129, loss=0.681130051612854 (2.896766s)
INFO:niftynet:2019-02-12 15:17:36,860: training iter 130, loss=0.6325297355651855 (8.097466s)
INFO:niftynet:2019-02-12 15:17:39,044:     validation iter 130, loss=0.5428047180175781 (2.181012s)
INFO:niftynet:2019-02-12 15:17:41,616: training iter 131, loss=0.6050759553909302 (2.568607s)
INFO:niftynet:2019-02-12 15:17:44,450: training iter 132, loss=0.5864031314849854 (2.833805s)
INFO:niftynet:2019-02-12 15:17:47,358: training iter 133, loss=0.6974024772644043 (2.908490s)
INFO:niftynet:2019-02-12 15:17:49,920: training iter 134, loss=0.5923389196395874 (2.561178s)
INFO:niftynet:2019-02-12 15:17:53,291: training iter 135, loss=0.7491664886474609 (3.369334s)
INFO:niftynet:2019-02-12 15:17:55,983: training iter 136, loss=0.4165172874927521 (2.688043s)
INFO:niftynet:2019-02-12 15:17:59,120: training iter 137, loss=0.44915348291397095 (3.137403s)
INFO:niftynet:2019-02-12 15:18:02,418: training iter 138, loss=0.45957595109939575 (3.295682s)
INFO:niftynet:2019-02-12 15:18:07,684: training iter 139, loss=0.6178320646286011 (5.265775s)
INFO:niftynet:2019-02-12 15:18:10,818: training iter 140, loss=0.583061158657074 (3.134281s)
INFO:niftynet:2019-02-12 15:18:13,146:     validation iter 140, loss=0.6755298972129822 (2.325504s)
INFO:niftynet:2019-02-12 15:18:16,262: training iter 141, loss=0.7073911428451538 (3.110084s)
INFO:niftynet:2019-02-12 15:18:18,537: training iter 142, loss=0.6218297481536865 (2.270691s)
INFO:niftynet:2019-02-12 15:18:21,866: training iter 143, loss=0.5329149961471558 (3.328023s)
INFO:niftynet:2019-02-12 15:18:24,806: training iter 144, loss=0.6522621512413025 (2.939408s)
INFO:niftynet:2019-02-12 15:18:29,265: training iter 145, loss=0.6318625211715698 (4.457575s)
INFO:niftynet:2019-02-12 15:18:32,359: training iter 146, loss=0.5263025164604187 (3.091646s)
INFO:niftynet:2019-02-12 15:18:34,809: training iter 147, loss=0.6727932095527649 (2.450550s)
INFO:niftynet:2019-02-12 15:18:40,891: training iter 148, loss=0.6748712062835693 (6.080559s)
INFO:niftynet:2019-02-12 15:18:43,620: training iter 149, loss=0.5828491449356079 (2.729331s)
INFO:niftynet:2019-02-12 15:18:46,449: training iter 150, loss=0.6691127419471741 (2.828811s)
INFO:niftynet:2019-02-12 15:18:48,872: iter 150 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:18:50,644:     validation iter 150, loss=0.42012685537338257 (1.770934s)
INFO:niftynet:2019-02-12 15:18:53,678: training iter 151, loss=0.5685346722602844 (3.031448s)
INFO:niftynet:2019-02-12 15:18:56,724: training iter 152, loss=0.6784127950668335 (3.040163s)
INFO:niftynet:2019-02-12 15:18:59,395: training iter 153, loss=0.6683257818222046 (2.671420s)
INFO:niftynet:2019-02-12 15:19:03,937: training iter 154, loss=0.5597070455551147 (4.541490s)
INFO:niftynet:2019-02-12 15:19:06,395: training iter 155, loss=0.5448812246322632 (2.457582s)
INFO:niftynet:2019-02-12 15:19:09,422: training iter 156, loss=0.6541067361831665 (3.024519s)
INFO:niftynet:2019-02-12 15:19:12,459: training iter 157, loss=0.5790976285934448 (3.036209s)
INFO:niftynet:2019-02-12 15:19:18,804: training iter 158, loss=0.5165519118309021 (6.344557s)
INFO:niftynet:2019-02-12 15:19:21,343: training iter 159, loss=0.5941681861877441 (2.536769s)
INFO:niftynet:2019-02-12 15:19:23,953: training iter 160, loss=0.5628887414932251 (2.610007s)
INFO:niftynet:2019-02-12 15:19:26,409:     validation iter 160, loss=0.638581395149231 (2.445276s)
INFO:niftynet:2019-02-12 15:19:29,047: training iter 161, loss=0.5375849008560181 (2.636038s)
INFO:niftynet:2019-02-12 15:19:31,561: training iter 162, loss=0.5738054513931274 (2.513948s)
INFO:niftynet:2019-02-12 15:19:35,803: training iter 163, loss=0.5956043004989624 (4.241668s)
INFO:niftynet:2019-02-12 15:19:38,843: training iter 164, loss=0.5994941592216492 (3.039526s)
INFO:niftynet:2019-02-12 15:19:42,248: training iter 165, loss=0.6453211307525635 (3.404144s)
INFO:niftynet:2019-02-12 15:19:45,170: training iter 166, loss=0.6799801588058472 (2.919394s)
INFO:niftynet:2019-02-12 15:19:50,839: training iter 167, loss=0.5699359774589539 (5.668055s)
INFO:niftynet:2019-02-12 15:19:53,341: training iter 168, loss=0.5654632449150085 (2.501471s)
INFO:niftynet:2019-02-12 15:19:56,017: training iter 169, loss=0.6063531637191772 (2.675802s)
INFO:niftynet:2019-02-12 15:19:59,295: training iter 170, loss=0.717731237411499 (3.277459s)
INFO:niftynet:2019-02-12 15:20:01,961:     validation iter 170, loss=0.6412011384963989 (2.664531s)
INFO:niftynet:2019-02-12 15:20:05,408: training iter 171, loss=0.5973383784294128 (3.442551s)
INFO:niftynet:2019-02-12 15:20:08,412: training iter 172, loss=0.4631302058696747 (3.003689s)
INFO:niftynet:2019-02-12 15:20:11,056: training iter 173, loss=0.6650667786598206 (2.642979s)
INFO:niftynet:2019-02-12 15:20:14,570: training iter 174, loss=0.7038300037384033 (3.514314s)
INFO:niftynet:2019-02-12 15:20:17,314: training iter 175, loss=0.6317349672317505 (2.743299s)
INFO:niftynet:2019-02-12 15:20:18,810: iter 175 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:20:23,610: training iter 176, loss=0.7202376127243042 (4.799372s)
INFO:niftynet:2019-02-12 15:20:26,340: training iter 177, loss=0.7286636829376221 (2.729611s)
INFO:niftynet:2019-02-12 15:20:29,759: training iter 178, loss=0.6263828277587891 (3.417986s)
INFO:niftynet:2019-02-12 15:20:32,408: training iter 179, loss=0.4864458739757538 (2.648489s)
INFO:niftynet:2019-02-12 15:20:35,301: training iter 180, loss=0.6350063681602478 (2.892427s)
INFO:niftynet:2019-02-12 15:20:37,373:     validation iter 180, loss=0.6018548011779785 (2.069693s)
INFO:niftynet:2019-02-12 15:20:40,384: training iter 181, loss=0.4657701551914215 (3.009452s)
INFO:niftynet:2019-02-12 15:20:43,728: training iter 182, loss=0.5284817218780518 (3.325928s)
INFO:niftynet:2019-02-12 15:20:46,559: training iter 183, loss=0.6938650608062744 (2.830884s)
INFO:niftynet:2019-02-12 15:20:49,600: training iter 184, loss=0.634530782699585 (3.039973s)
INFO:niftynet:2019-02-12 15:20:54,662: training iter 185, loss=0.5103811025619507 (5.060834s)
INFO:niftynet:2019-02-12 15:20:58,157: training iter 186, loss=0.7455269694328308 (3.491493s)
INFO:niftynet:2019-02-12 15:21:00,974: training iter 187, loss=0.5419341325759888 (2.817056s)
INFO:niftynet:2019-02-12 15:21:03,489: training iter 188, loss=0.5780854225158691 (2.514706s)
INFO:niftynet:2019-02-12 15:21:06,827: training iter 189, loss=0.7155483961105347 (3.337134s)
INFO:niftynet:2019-02-12 15:21:09,594: training iter 190, loss=0.6189684271812439 (2.767090s)
INFO:niftynet:2019-02-12 15:21:11,628:     validation iter 190, loss=0.6233763694763184 (2.032542s)
INFO:niftynet:2019-02-12 15:21:16,025: training iter 191, loss=0.5410656929016113 (4.394557s)
INFO:niftynet:2019-02-12 15:21:18,476: training iter 192, loss=0.6680575609207153 (2.449075s)
INFO:niftynet:2019-02-12 15:21:22,337: training iter 193, loss=0.6163221597671509 (3.859456s)
INFO:niftynet:2019-02-12 15:21:25,331: training iter 194, loss=0.6972866058349609 (2.994015s)
INFO:niftynet:2019-02-12 15:21:28,456: training iter 195, loss=0.7260711193084717 (3.123318s)
INFO:niftynet:2019-02-12 15:21:31,107: training iter 196, loss=0.6090558767318726 (2.648431s)
INFO:niftynet:2019-02-12 15:21:36,763: training iter 197, loss=0.6058555245399475 (5.654991s)
INFO:niftynet:2019-02-12 15:21:39,735: training iter 198, loss=0.6076295375823975 (2.972497s)
INFO:niftynet:2019-02-12 15:21:42,505: training iter 199, loss=0.47311931848526 (2.768461s)
INFO:niftynet:2019-02-12 15:21:45,289: training iter 200, loss=0.7036060690879822 (2.783696s)
INFO:niftynet:2019-02-12 15:21:47,379: iter 200 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:21:49,978:     validation iter 200, loss=0.6334735155105591 (2.575752s)
INFO:niftynet:2019-02-12 15:21:52,719: training iter 201, loss=0.64480060338974 (2.737646s)
INFO:niftynet:2019-02-12 15:21:55,780: training iter 202, loss=0.667575478553772 (3.061234s)
INFO:niftynet:2019-02-12 15:21:58,496: training iter 203, loss=0.47432196140289307 (2.715265s)
INFO:niftynet:2019-02-12 15:22:00,824: training iter 204, loss=0.6000027656555176 (2.318451s)
INFO:niftynet:2019-02-12 15:22:05,278: training iter 205, loss=0.6429888010025024 (4.452028s)
INFO:niftynet:2019-02-12 15:22:07,615: training iter 206, loss=0.4574708342552185 (2.333834s)
INFO:niftynet:2019-02-12 15:22:11,782: training iter 207, loss=0.7329147458076477 (4.165816s)
INFO:niftynet:2019-02-12 15:22:13,931: training iter 208, loss=0.6236267685890198 (2.148552s)
INFO:niftynet:2019-02-12 15:22:16,362: training iter 209, loss=0.6523840427398682 (2.431152s)
INFO:niftynet:2019-02-12 15:22:27,408: training iter 210, loss=0.5816813707351685 (11.044937s)
INFO:niftynet:2019-02-12 15:22:30,049:     validation iter 210, loss=0.7617892026901245 (2.639715s)
INFO:niftynet:2019-02-12 15:22:32,594: training iter 211, loss=0.5964241623878479 (2.543473s)
INFO:niftynet:2019-02-12 15:22:35,021: training iter 212, loss=0.5550779104232788 (2.426796s)
INFO:niftynet:2019-02-12 15:22:37,999: training iter 213, loss=0.5420711040496826 (2.977647s)
INFO:niftynet:2019-02-12 15:22:41,418: training iter 214, loss=0.512566089630127 (3.418895s)
INFO:niftynet:2019-02-12 15:22:45,905: training iter 215, loss=0.7478306293487549 (4.486425s)
INFO:niftynet:2019-02-12 15:22:48,525: training iter 216, loss=0.4710097908973694 (2.616851s)
INFO:niftynet:2019-02-12 15:22:50,990: training iter 217, loss=0.6172289252281189 (2.464513s)
INFO:niftynet:2019-02-12 15:22:55,142: training iter 218, loss=0.6801702380180359 (4.152250s)
INFO:niftynet:2019-02-12 15:22:58,179: training iter 219, loss=0.5889347195625305 (3.036699s)
INFO:niftynet:2019-02-12 15:23:01,283: training iter 220, loss=0.5458196401596069 (3.101965s)
INFO:niftynet:2019-02-12 15:23:03,355:     validation iter 220, loss=0.7049680948257446 (2.070881s)
INFO:niftynet:2019-02-12 15:23:05,951: training iter 221, loss=0.4868876338005066 (2.594300s)
INFO:niftynet:2019-02-12 15:23:10,175: training iter 222, loss=0.6160399317741394 (4.223490s)
INFO:niftynet:2019-02-12 15:23:12,592: training iter 223, loss=0.7581868171691895 (2.416164s)
INFO:niftynet:2019-02-12 15:23:15,326: training iter 224, loss=0.4871019124984741 (2.734612s)
INFO:niftynet:2019-02-12 15:23:18,203: training iter 225, loss=0.6255497932434082 (2.875845s)
INFO:niftynet:2019-02-12 15:23:20,027: iter 225 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:23:22,994: training iter 226, loss=0.5898327827453613 (2.952685s)
INFO:niftynet:2019-02-12 15:23:25,494: training iter 227, loss=0.5353802442550659 (2.499679s)
INFO:niftynet:2019-02-12 15:23:30,022: training iter 228, loss=0.4755726754665375 (4.527195s)
INFO:niftynet:2019-02-12 15:23:32,912: training iter 229, loss=0.5943745374679565 (2.890340s)
INFO:niftynet:2019-02-12 15:23:35,506: training iter 230, loss=0.6902890205383301 (2.591950s)
INFO:niftynet:2019-02-12 15:23:46,895:     validation iter 230, loss=0.6931415796279907 (11.386650s)
INFO:niftynet:2019-02-12 15:23:49,458: training iter 231, loss=0.7438358068466187 (2.559970s)
INFO:niftynet:2019-02-12 15:23:51,852: training iter 232, loss=0.5806583166122437 (2.393915s)
INFO:niftynet:2019-02-12 15:23:54,338: training iter 233, loss=0.7274991273880005 (2.486183s)
INFO:niftynet:2019-02-12 15:23:57,006: training iter 234, loss=0.6119692325592041 (2.667550s)
INFO:niftynet:2019-02-12 15:23:59,860: training iter 235, loss=0.6579121351242065 (2.853430s)
INFO:niftynet:2019-02-12 15:24:03,566: training iter 236, loss=0.5926181077957153 (3.701046s)
INFO:niftynet:2019-02-12 15:24:06,503: training iter 237, loss=0.701217532157898 (2.937166s)
INFO:niftynet:2019-02-12 15:24:09,581: training iter 238, loss=0.5361725091934204 (3.076873s)
INFO:niftynet:2019-02-12 15:24:12,256: training iter 239, loss=0.5251717567443848 (2.674248s)
INFO:niftynet:2019-02-12 15:24:17,459: training iter 240, loss=0.4884225130081177 (5.202304s)
INFO:niftynet:2019-02-12 15:24:20,173:     validation iter 240, loss=0.6326565146446228 (2.712550s)
INFO:niftynet:2019-02-12 15:24:22,760: training iter 241, loss=0.49886032938957214 (2.576555s)
INFO:niftynet:2019-02-12 15:24:26,215: training iter 242, loss=0.7511302828788757 (3.455297s)
INFO:niftynet:2019-02-12 15:24:28,844: training iter 243, loss=0.7053176164627075 (2.625597s)
INFO:niftynet:2019-02-12 15:24:32,289: training iter 244, loss=0.5295137763023376 (3.444237s)
INFO:niftynet:2019-02-12 15:24:34,899: training iter 245, loss=0.5513715147972107 (2.608816s)
INFO:niftynet:2019-02-12 15:24:38,741: training iter 246, loss=0.5722311735153198 (3.840935s)
INFO:niftynet:2019-02-12 15:24:41,575: training iter 247, loss=0.7511335611343384 (2.833110s)
INFO:niftynet:2019-02-12 15:24:45,418: training iter 248, loss=0.565167248249054 (3.843226s)
INFO:niftynet:2019-02-12 15:24:48,306: training iter 249, loss=0.5546553134918213 (2.886554s)
INFO:niftynet:2019-02-12 15:24:50,818: training iter 250, loss=0.5447875261306763 (2.510987s)
INFO:niftynet:2019-02-12 15:24:52,791: iter 250 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:24:55,148:     validation iter 250, loss=0.6193733811378479 (2.356344s)
INFO:niftynet:2019-02-12 15:24:58,382: training iter 251, loss=0.6759099960327148 (3.231947s)
INFO:niftynet:2019-02-12 15:25:01,084: training iter 252, loss=0.6348569393157959 (2.702425s)
INFO:niftynet:2019-02-12 15:25:03,790: training iter 253, loss=0.5780066251754761 (2.704083s)
INFO:niftynet:2019-02-12 15:25:06,566: training iter 254, loss=0.6204217672348022 (2.775039s)
INFO:niftynet:2019-02-12 15:25:11,734: training iter 255, loss=0.6638815402984619 (5.167708s)
INFO:niftynet:2019-02-12 15:25:15,096: training iter 256, loss=0.5696651339530945 (3.360048s)
INFO:niftynet:2019-02-12 15:25:18,422: training iter 257, loss=0.6349971294403076 (3.325678s)
INFO:niftynet:2019-02-12 15:25:21,345: training iter 258, loss=0.6184976100921631 (2.921601s)
INFO:niftynet:2019-02-12 15:25:24,475: training iter 259, loss=0.6642352938652039 (3.128832s)
INFO:niftynet:2019-02-12 15:25:27,102: training iter 260, loss=0.6519935131072998 (2.626557s)
INFO:niftynet:2019-02-12 15:25:29,383:     validation iter 260, loss=0.5761364698410034 (2.278758s)
INFO:niftynet:2019-02-12 15:25:34,302: training iter 261, loss=0.6192018985748291 (4.916446s)
INFO:niftynet:2019-02-12 15:25:36,756: training iter 262, loss=0.5668140649795532 (2.453247s)
INFO:niftynet:2019-02-12 15:25:39,988: training iter 263, loss=0.4986023008823395 (3.231075s)
INFO:niftynet:2019-02-12 15:25:42,577: training iter 264, loss=0.6928060054779053 (2.589121s)
INFO:niftynet:2019-02-12 15:25:47,811: training iter 265, loss=0.5789179801940918 (5.233863s)
INFO:niftynet:2019-02-12 15:25:50,132: training iter 266, loss=0.46741020679473877 (2.318585s)
INFO:niftynet:2019-02-12 15:25:53,586: training iter 267, loss=0.499755859375 (3.453431s)
INFO:niftynet:2019-02-12 15:25:56,259: training iter 268, loss=0.5634183287620544 (2.671966s)
INFO:niftynet:2019-02-12 15:26:01,721: training iter 269, loss=0.555249035358429 (5.462132s)
INFO:niftynet:2019-02-12 15:26:04,535: training iter 270, loss=0.6403255462646484 (2.813198s)
INFO:niftynet:2019-02-12 15:26:06,963:     validation iter 270, loss=0.4786330759525299 (2.424298s)
INFO:niftynet:2019-02-12 15:26:10,117: training iter 271, loss=0.582468569278717 (3.151570s)
INFO:niftynet:2019-02-12 15:26:12,931: training iter 272, loss=0.5848475694656372 (2.813936s)
INFO:niftynet:2019-02-12 15:26:17,180: training iter 273, loss=0.7207165956497192 (4.248904s)
INFO:niftynet:2019-02-12 15:26:20,030: training iter 274, loss=0.5030673742294312 (2.847955s)
INFO:niftynet:2019-02-12 15:26:22,859: training iter 275, loss=0.5482816100120544 (2.829177s)
INFO:niftynet:2019-02-12 15:26:25,015: iter 275 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:26:27,374: training iter 276, loss=0.5371465086936951 (2.356091s)
INFO:niftynet:2019-02-12 15:26:30,080: training iter 277, loss=0.5366586446762085 (2.706320s)
INFO:niftynet:2019-02-12 15:26:35,451: training iter 278, loss=0.6544903516769409 (5.370332s)
INFO:niftynet:2019-02-12 15:26:38,113: training iter 279, loss=0.6442670822143555 (2.661394s)
INFO:niftynet:2019-02-12 15:26:41,087: training iter 280, loss=0.5539611577987671 (2.974067s)
INFO:niftynet:2019-02-12 15:26:43,648:     validation iter 280, loss=0.5546047687530518 (2.559371s)
INFO:niftynet:2019-02-12 15:26:46,441: training iter 281, loss=0.4764011800289154 (2.789201s)
INFO:niftynet:2019-02-12 15:26:48,718: training iter 282, loss=0.6470324993133545 (2.276976s)
INFO:niftynet:2019-02-12 15:26:51,266: training iter 283, loss=0.6665747165679932 (2.545517s)
INFO:niftynet:2019-02-12 15:26:56,091: training iter 284, loss=0.7207288146018982 (4.824951s)
INFO:niftynet:2019-02-12 15:26:58,979: training iter 285, loss=0.525355875492096 (2.886519s)
INFO:niftynet:2019-02-12 15:27:02,020: training iter 286, loss=0.5424178242683411 (3.038986s)
INFO:niftynet:2019-02-12 15:27:04,692: training iter 287, loss=0.6872546672821045 (2.670922s)
INFO:niftynet:2019-02-12 15:27:07,787: training iter 288, loss=0.4818252921104431 (3.093963s)
INFO:niftynet:2019-02-12 15:27:11,491: training iter 289, loss=0.7542755603790283 (3.702724s)
INFO:niftynet:2019-02-12 15:27:15,845: training iter 290, loss=0.6439124941825867 (4.354056s)
INFO:niftynet:2019-02-12 15:27:18,117:     validation iter 290, loss=0.6416089534759521 (2.269701s)
INFO:niftynet:2019-02-12 15:27:21,535: training iter 291, loss=0.7543363571166992 (3.414840s)
INFO:niftynet:2019-02-12 15:27:24,319: training iter 292, loss=0.6394604444503784 (2.783924s)
INFO:niftynet:2019-02-12 15:27:27,078: training iter 293, loss=0.7073636651039124 (2.754929s)
INFO:niftynet:2019-02-12 15:27:29,951: training iter 294, loss=0.6630957722663879 (2.872276s)
INFO:niftynet:2019-02-12 15:27:33,237: training iter 295, loss=0.5708044767379761 (3.285116s)
INFO:niftynet:2019-02-12 15:27:36,115: training iter 296, loss=0.6223304271697998 (2.874758s)
INFO:niftynet:2019-02-12 15:27:42,492: training iter 297, loss=0.6993470788002014 (6.376552s)
INFO:niftynet:2019-02-12 15:27:45,383: training iter 298, loss=0.6773707270622253 (2.890513s)
INFO:niftynet:2019-02-12 15:27:47,432: training iter 299, loss=0.7616164088249207 (2.048573s)
INFO:niftynet:2019-02-12 15:27:50,445: training iter 300, loss=0.6283524036407471 (3.012359s)
INFO:niftynet:2019-02-12 15:27:52,276: iter 300 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:27:54,528:     validation iter 300, loss=0.6205911040306091 (2.251278s)
INFO:niftynet:2019-02-12 15:27:57,082: training iter 301, loss=0.4782528281211853 (2.550681s)
INFO:niftynet:2019-02-12 15:27:59,524: training iter 302, loss=0.5044946670532227 (2.437239s)
INFO:niftynet:2019-02-12 15:28:02,115: training iter 303, loss=0.41503897309303284 (2.590396s)
INFO:niftynet:2019-02-12 15:28:05,234: training iter 304, loss=0.5852449536323547 (3.119076s)
INFO:niftynet:2019-02-12 15:28:10,096: training iter 305, loss=0.510890781879425 (4.861051s)
INFO:niftynet:2019-02-12 15:28:12,917: training iter 306, loss=0.45776572823524475 (2.820282s)
INFO:niftynet:2019-02-12 15:28:16,349: training iter 307, loss=0.7392951250076294 (3.431001s)
INFO:niftynet:2019-02-12 15:28:20,356: training iter 308, loss=0.6384178400039673 (4.006913s)
INFO:niftynet:2019-02-12 15:28:23,089: training iter 309, loss=0.7490778565406799 (2.732308s)
INFO:niftynet:2019-02-12 15:28:26,635: training iter 310, loss=0.7148669958114624 (3.546348s)
INFO:niftynet:2019-02-12 15:28:28,391:     validation iter 310, loss=0.7356889247894287 (1.752470s)
INFO:niftynet:2019-02-12 15:28:31,555: training iter 311, loss=0.6773350238800049 (3.161152s)
INFO:niftynet:2019-02-12 15:28:35,740: training iter 312, loss=0.6597877144813538 (4.185654s)
INFO:niftynet:2019-02-12 15:28:40,286: training iter 313, loss=0.672946572303772 (4.544825s)
INFO:niftynet:2019-02-12 15:28:43,247: training iter 314, loss=0.6584479808807373 (2.960857s)
INFO:niftynet:2019-02-12 15:28:46,579: training iter 315, loss=0.7323143482208252 (3.330973s)
INFO:niftynet:2019-02-12 15:28:49,338: training iter 316, loss=0.5820485353469849 (2.757588s)
INFO:niftynet:2019-02-12 15:28:54,337: training iter 317, loss=0.44934341311454773 (4.997422s)
INFO:niftynet:2019-02-12 15:28:57,397: training iter 318, loss=0.6106312870979309 (3.060078s)
INFO:niftynet:2019-02-12 15:29:00,299: training iter 319, loss=0.6296989917755127 (2.900771s)
INFO:niftynet:2019-02-12 15:29:02,938: training iter 320, loss=0.6167609691619873 (2.638031s)
INFO:niftynet:2019-02-12 15:29:05,104:     validation iter 320, loss=0.6222782135009766 (2.163572s)
INFO:niftynet:2019-02-12 15:29:07,277: training iter 321, loss=0.6055856943130493 (2.169635s)
INFO:niftynet:2019-02-12 15:29:09,228: training iter 322, loss=0.6433352828025818 (1.938694s)
INFO:niftynet:2019-02-12 15:29:11,989: training iter 323, loss=0.6397977471351624 (2.760745s)
INFO:niftynet:2019-02-12 15:29:23,422: training iter 324, loss=0.5525366067886353 (11.431969s)
INFO:niftynet:2019-02-12 15:29:26,587: training iter 325, loss=0.5400621891021729 (3.164439s)
INFO:niftynet:2019-02-12 15:29:28,956: iter 325 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:29:31,351: training iter 326, loss=0.471987783908844 (2.385753s)
INFO:niftynet:2019-02-12 15:29:34,669: training iter 327, loss=0.6351984739303589 (3.317239s)
INFO:niftynet:2019-02-12 15:29:37,606: training iter 328, loss=0.6438119411468506 (2.937041s)
CRITICAL:niftynet:2019-02-12 15:29:38,165: unrecognised file path format, should be a valid filename,or a sequence of filenames ('/data-nas/brains/MICCAI_BraTS2018/interim/HGG/Brats18_CBICA_AXL_1/origt1ce.nii.gz',)
INFO:niftynet:2019-02-12 15:29:41,585: training iter 329, loss=0.6876447200775146 (3.978004s)
INFO:niftynet:2019-02-12 15:29:44,278: training iter 330, loss=0.5635161399841309 (2.693321s)
INFO:niftynet:2019-02-12 15:29:46,666:     validation iter 330, loss=0.7026976346969604 (2.385895s)
CRITICAL:niftynet:2019-02-12 15:29:48,954: unrecognised file path format, should be a valid filename,or a sequence of filenames ('/data-nas/brains/MICCAI_BraTS2018/interim/HGG/Brats18_TCIA04_437_1/origt1ce.nii.gz',)
INFO:niftynet:2019-02-12 15:29:49,332: training iter 331, loss=0.7089661955833435 (2.663140s)
INFO:niftynet:2019-02-12 15:29:52,078: training iter 332, loss=0.5500444769859314 (2.745284s)
INFO:niftynet:2019-02-12 15:29:52,280: cleaning up...
INFO:niftynet:2019-02-12 15:29:54,216: iter 333 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 15:29:54,218: stopping sampling threads
CRITICAL:niftynet:2019-02-12 15:29:54,434: unrecognised file path format, should be a valid filename,or a sequence of filenames ('/data-nas/brains/MICCAI_BraTS2018/interim/HGG/Brats18_TCIA02_430_1/origt1ce.nii.gz',)
INFO:niftynet:2019-02-12 16:06:15,302: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 16:06:15,302: starting segmentation application
INFO:niftynet:2019-02-12 16:06:15,302: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:06:15,308: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:06:15,313: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:06:15,316: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 16:06:15,320: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 16:06:15,323: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

CRITICAL:niftynet:2019-02-12 16:06:15,446: Could not load file: ('/data-nas/brains/MICCAI_BraTS2018/interim/HGG/Brats18_2013_10_1/origt1_std.nii.gz', '/data-nas/brains/MICCAI_BraTS2018/interim/HGG/Brats18_2013_10_1/origt1ce_std.nii.gz', '/data-nas/brains/MICCAI_BraTS2018/interim/HGG/Brats18_2013_10_1/origt2_std.nii.gz')
INFO:niftynet:2019-02-12 16:23:12,111: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 16:23:12,111: starting segmentation application
INFO:niftynet:2019-02-12 16:23:12,111: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:23:12,117: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:23:12,121: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:23:12,125: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 16:23:12,128: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 16:23:12,131: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 16:23:33,326: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 16:23:33,327: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 16:23:35,813: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 16:23:35,813: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 16:23:35,816: training normalisation histogram references for image:{'T1', 'T2', 'T1ce'}, using 227 subjects
INFO:niftynet:2019-02-12 16:26:24,901: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 16:26:24,901: starting segmentation application
INFO:niftynet:2019-02-12 16:26:24,901: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:26:24,907: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:26:24,911: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:26:24,914: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 16:26:24,918: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 16:26:24,920: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 16:26:36,877: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 16:26:36,877: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 16:26:38,324: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 16:26:38,324: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 16:26:38,327: Looking for the set of unique discrete labels from input label using 227 subjects
INFO:niftynet:2019-02-12 16:27:29,735: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 16:27:30,244: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 16:27:30,278: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 16:27:30,347: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 16:27:30,347: using DenseVNet
INFO:niftynet:2019-02-12 16:27:30,354: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 16:27:30,395: Initialising Dataset from 29 subjects...
INFO:niftynet:2019-02-12 16:27:43,490: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 16:28:50,970: training iter 1, loss=0.8547589778900146 (67.158109s)
INFO:niftynet:2019-02-12 16:28:53,596: training iter 2, loss=0.8034772276878357 (2.626306s)
INFO:niftynet:2019-02-12 16:28:56,132: training iter 3, loss=0.6963022947311401 (2.535693s)
INFO:niftynet:2019-02-12 16:28:59,001: training iter 4, loss=0.7131110429763794 (2.866675s)
INFO:niftynet:2019-02-12 16:29:09,742: training iter 5, loss=0.7147567272186279 (10.735861s)
INFO:niftynet:2019-02-12 16:29:12,105: training iter 6, loss=0.7340347766876221 (2.346820s)
INFO:niftynet:2019-02-12 16:29:14,988: training iter 7, loss=0.6829514503479004 (2.882281s)
INFO:niftynet:2019-02-12 16:29:17,774: training iter 8, loss=0.701488733291626 (2.785282s)
INFO:niftynet:2019-02-12 16:29:20,316: training iter 9, loss=0.6748302578926086 (2.541637s)
INFO:niftynet:2019-02-12 16:29:23,129: training iter 10, loss=0.7184103727340698 (2.810947s)
INFO:niftynet:2019-02-12 16:29:58,620:     validation iter 10, loss=0.6916879415512085 (35.486231s)
INFO:niftynet:2019-02-12 16:30:02,130: training iter 11, loss=0.6790691614151001 (3.498206s)
INFO:niftynet:2019-02-12 16:30:05,194: training iter 12, loss=0.742725133895874 (3.061097s)
INFO:niftynet:2019-02-12 16:30:08,503: training iter 13, loss=0.6588029265403748 (3.308864s)
INFO:niftynet:2019-02-12 16:30:13,283: training iter 14, loss=0.6327755451202393 (4.778682s)
INFO:niftynet:2019-02-12 16:30:18,572: training iter 15, loss=0.7125581502914429 (5.287711s)
INFO:niftynet:2019-02-12 16:30:22,134: training iter 16, loss=0.6483200788497925 (3.558641s)
INFO:niftynet:2019-02-12 16:30:24,171: training iter 17, loss=0.7110462188720703 (2.036378s)
INFO:niftynet:2019-02-12 16:30:27,263: training iter 18, loss=0.6633845567703247 (3.091178s)
INFO:niftynet:2019-02-12 16:30:31,536: training iter 19, loss=0.6157784461975098 (4.271996s)
INFO:niftynet:2019-02-12 16:30:35,477: training iter 20, loss=0.6370881199836731 (3.937808s)
INFO:niftynet:2019-02-12 16:30:38,144:     validation iter 20, loss=0.61046302318573 (2.662359s)
INFO:niftynet:2019-02-12 16:30:40,726: training iter 21, loss=0.618516206741333 (2.578621s)
INFO:niftynet:2019-02-12 16:30:42,605: training iter 22, loss=0.7578938007354736 (1.873937s)
INFO:niftynet:2019-02-12 16:30:44,937: training iter 23, loss=0.7668548822402954 (2.332155s)
INFO:niftynet:2019-02-12 16:30:46,825: training iter 24, loss=0.7017878890037537 (1.887886s)
INFO:niftynet:2019-02-12 16:30:52,258: iter 25 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:30:52,260: training iter 25, loss=0.6725161671638489 (2.647186s)
INFO:niftynet:2019-02-12 16:30:54,712: training iter 26, loss=0.5932828783988953 (2.448812s)
INFO:niftynet:2019-02-12 16:30:57,782: training iter 27, loss=0.743671715259552 (3.069822s)
INFO:niftynet:2019-02-12 16:30:59,805: training iter 28, loss=0.6257681250572205 (2.021940s)
INFO:niftynet:2019-02-12 16:31:01,871: training iter 29, loss=0.6536000967025757 (2.064240s)
INFO:niftynet:2019-02-12 16:31:04,424: training iter 30, loss=0.6601392030715942 (2.551997s)
INFO:niftynet:2019-02-12 16:31:06,816:     validation iter 30, loss=0.6652774214744568 (2.366451s)
INFO:niftynet:2019-02-12 16:31:09,200: training iter 31, loss=0.647529661655426 (2.383295s)
INFO:niftynet:2019-02-12 16:31:11,727: training iter 32, loss=0.6425706148147583 (2.525378s)
INFO:niftynet:2019-02-12 16:31:13,974: training iter 33, loss=0.6554274559020996 (2.247261s)
INFO:niftynet:2019-02-12 16:31:17,425: training iter 34, loss=0.6588090658187866 (3.449515s)
INFO:niftynet:2019-02-12 16:31:20,111: training iter 35, loss=0.5756816267967224 (2.685128s)
INFO:niftynet:2019-02-12 16:31:22,277: training iter 36, loss=0.5549670457839966 (2.162404s)
INFO:niftynet:2019-02-12 16:31:24,631: training iter 37, loss=0.6078993082046509 (2.353481s)
INFO:niftynet:2019-02-12 16:31:27,091: training iter 38, loss=0.6660462021827698 (2.459754s)
INFO:niftynet:2019-02-12 16:31:30,002: training iter 39, loss=0.7624940872192383 (2.910400s)
INFO:niftynet:2019-02-12 16:31:32,481: training iter 40, loss=0.5019736886024475 (2.477465s)
INFO:niftynet:2019-02-12 16:31:34,123:     validation iter 40, loss=0.6776211261749268 (1.636599s)
INFO:niftynet:2019-02-12 16:31:36,526: training iter 41, loss=0.612467348575592 (2.398589s)
INFO:niftynet:2019-02-12 16:31:38,467: training iter 42, loss=0.6365487575531006 (1.939885s)
INFO:niftynet:2019-02-12 16:31:41,060: training iter 43, loss=0.5301835536956787 (2.591490s)
INFO:niftynet:2019-02-12 16:31:43,724: training iter 44, loss=0.5891409516334534 (2.662615s)
INFO:niftynet:2019-02-12 16:31:46,287: training iter 45, loss=0.593111515045166 (2.559864s)
INFO:niftynet:2019-02-12 16:31:48,394: training iter 46, loss=0.6211689114570618 (2.103681s)
INFO:niftynet:2019-02-12 16:31:51,052: training iter 47, loss=0.6072921752929688 (2.657754s)
INFO:niftynet:2019-02-12 16:31:53,524: training iter 48, loss=0.5681236982345581 (2.471713s)
INFO:niftynet:2019-02-12 16:31:56,198: training iter 49, loss=0.5341612100601196 (2.673758s)
INFO:niftynet:2019-02-12 16:32:00,603: iter 50 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:32:00,612: training iter 50, loss=0.7621934413909912 (2.584149s)
INFO:niftynet:2019-02-12 16:32:02,908:     validation iter 50, loss=0.6637138724327087 (2.291559s)
INFO:niftynet:2019-02-12 16:32:05,325: training iter 51, loss=0.7456585168838501 (2.414111s)
INFO:niftynet:2019-02-12 16:32:08,928: training iter 52, loss=0.7575026750564575 (3.602204s)
INFO:niftynet:2019-02-12 16:32:11,314: training iter 53, loss=0.5113873481750488 (2.386315s)
INFO:niftynet:2019-02-12 16:32:13,681: training iter 54, loss=0.6408242583274841 (2.366776s)
INFO:niftynet:2019-02-12 16:32:17,088: training iter 55, loss=0.759348452091217 (3.402672s)
INFO:niftynet:2019-02-12 16:32:20,150: training iter 56, loss=0.6520956158638 (3.061764s)
INFO:niftynet:2019-02-12 16:32:22,784: training iter 57, loss=0.4305797815322876 (2.633133s)
INFO:niftynet:2019-02-12 16:32:25,176: training iter 58, loss=0.6823158860206604 (2.391680s)
INFO:niftynet:2019-02-12 16:32:27,511: training iter 59, loss=0.7572656869888306 (2.334013s)
INFO:niftynet:2019-02-12 16:32:30,665: training iter 60, loss=0.6576701402664185 (3.152702s)
INFO:niftynet:2019-02-12 16:32:32,903:     validation iter 60, loss=0.6816442012786865 (2.234499s)
INFO:niftynet:2019-02-12 16:32:35,530: training iter 61, loss=0.7638403177261353 (2.624051s)
INFO:niftynet:2019-02-12 16:32:37,715: training iter 62, loss=0.7484630346298218 (2.184220s)
INFO:niftynet:2019-02-12 16:32:39,738: training iter 63, loss=0.7146641612052917 (2.022937s)
INFO:niftynet:2019-02-12 16:32:41,874: training iter 64, loss=0.5722662210464478 (2.135721s)
INFO:niftynet:2019-02-12 16:32:44,481: training iter 65, loss=0.5383682250976562 (2.604397s)
INFO:niftynet:2019-02-12 16:32:46,944: training iter 66, loss=0.5093960762023926 (2.460715s)
INFO:niftynet:2019-02-12 16:32:49,352: training iter 67, loss=0.5940080881118774 (2.407082s)
INFO:niftynet:2019-02-12 16:32:51,636: training iter 68, loss=0.6566759347915649 (2.284104s)
INFO:niftynet:2019-02-12 16:32:54,407: training iter 69, loss=0.588438868522644 (2.770222s)
INFO:niftynet:2019-02-12 16:32:57,039: training iter 70, loss=0.5518051385879517 (2.629689s)
INFO:niftynet:2019-02-12 16:32:58,786:     validation iter 70, loss=0.656712532043457 (1.743034s)
INFO:niftynet:2019-02-12 16:33:01,166: training iter 71, loss=0.6602158546447754 (2.377860s)
INFO:niftynet:2019-02-12 16:33:04,000: training iter 72, loss=0.6906416416168213 (2.813350s)
INFO:niftynet:2019-02-12 16:33:06,603: training iter 73, loss=0.5288490056991577 (2.601054s)
INFO:niftynet:2019-02-12 16:33:10,355: training iter 74, loss=0.763696551322937 (3.751899s)
INFO:niftynet:2019-02-12 16:33:14,788: iter 75 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:33:14,790: training iter 75, loss=0.6561790704727173 (2.970882s)
INFO:niftynet:2019-02-12 16:33:16,681: training iter 76, loss=0.5453943014144897 (1.888855s)
INFO:niftynet:2019-02-12 16:33:19,011: training iter 77, loss=0.7511822581291199 (2.329765s)
INFO:niftynet:2019-02-12 16:33:20,747: training iter 78, loss=0.5443056225776672 (1.736034s)
INFO:niftynet:2019-02-12 16:33:22,550: training iter 79, loss=0.6207766532897949 (1.802668s)
INFO:niftynet:2019-02-12 16:33:24,603: training iter 80, loss=0.631228506565094 (2.051552s)
INFO:niftynet:2019-02-12 16:33:26,489:     validation iter 80, loss=0.6044071912765503 (1.861154s)
INFO:niftynet:2019-02-12 16:33:29,763: training iter 81, loss=0.6713494062423706 (3.270656s)
INFO:niftynet:2019-02-12 16:33:32,304: training iter 82, loss=0.7532854080200195 (2.540233s)
INFO:niftynet:2019-02-12 16:33:34,452: training iter 83, loss=0.4366343915462494 (2.148201s)
INFO:niftynet:2019-02-12 16:33:36,657: training iter 84, loss=0.5628277659416199 (2.204378s)
INFO:niftynet:2019-02-12 16:33:39,573: training iter 85, loss=0.5856637954711914 (2.913647s)
INFO:niftynet:2019-02-12 16:33:42,337: training iter 86, loss=0.6255998611450195 (2.760515s)
INFO:niftynet:2019-02-12 16:33:44,799: training iter 87, loss=0.7291642427444458 (2.461005s)
INFO:niftynet:2019-02-12 16:33:47,390: training iter 88, loss=0.7197422385215759 (2.591565s)
INFO:niftynet:2019-02-12 16:33:49,706: training iter 89, loss=0.659011721611023 (2.315532s)
INFO:niftynet:2019-02-12 16:33:52,148: training iter 90, loss=0.7183142900466919 (2.439972s)
INFO:niftynet:2019-02-12 16:34:00,024:     validation iter 90, loss=0.5379624366760254 (7.869603s)
INFO:niftynet:2019-02-12 16:34:02,645: training iter 91, loss=0.6860262155532837 (2.617308s)
INFO:niftynet:2019-02-12 16:34:05,013: training iter 92, loss=0.5934265851974487 (2.364194s)
INFO:niftynet:2019-02-12 16:34:08,197: training iter 93, loss=0.6547451615333557 (3.183171s)
INFO:niftynet:2019-02-12 16:34:10,807: training iter 94, loss=0.6292585134506226 (2.609090s)
INFO:niftynet:2019-02-12 16:34:13,239: training iter 95, loss=0.6669719219207764 (2.430833s)
INFO:niftynet:2019-02-12 16:34:15,558: training iter 96, loss=0.6309559345245361 (2.316103s)
INFO:niftynet:2019-02-12 16:34:17,881: training iter 97, loss=0.44684988260269165 (2.319299s)
INFO:niftynet:2019-02-12 16:34:20,252: training iter 98, loss=0.5286673903465271 (2.370761s)
INFO:niftynet:2019-02-12 16:34:23,569: training iter 99, loss=0.6236647963523865 (3.315773s)
INFO:niftynet:2019-02-12 16:34:28,366: iter 100 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:34:28,370: training iter 100, loss=0.6311804056167603 (2.787030s)
INFO:niftynet:2019-02-12 16:34:30,695:     validation iter 100, loss=0.7449370622634888 (2.315265s)
INFO:niftynet:2019-02-12 16:34:33,166: training iter 101, loss=0.6439615488052368 (2.468279s)
INFO:niftynet:2019-02-12 16:34:35,751: training iter 102, loss=0.44682684540748596 (2.584181s)
INFO:niftynet:2019-02-12 16:34:38,541: training iter 103, loss=0.5805283784866333 (2.789876s)
INFO:niftynet:2019-02-12 16:34:40,724: training iter 104, loss=0.6130528450012207 (2.182161s)
INFO:niftynet:2019-02-12 16:34:43,347: training iter 105, loss=0.39165109395980835 (2.621976s)
INFO:niftynet:2019-02-12 16:34:45,744: training iter 106, loss=0.5975029468536377 (2.394712s)
INFO:niftynet:2019-02-12 16:34:48,874: training iter 107, loss=0.6913086771965027 (3.129767s)
INFO:niftynet:2019-02-12 16:34:51,704: training iter 108, loss=0.6964154243469238 (2.829273s)
INFO:niftynet:2019-02-12 16:34:53,897: training iter 109, loss=0.6342535018920898 (2.193095s)
INFO:niftynet:2019-02-12 16:34:56,576: training iter 110, loss=0.52532958984375 (2.676384s)
INFO:niftynet:2019-02-12 16:34:58,517:     validation iter 110, loss=0.5263289213180542 (1.938847s)
INFO:niftynet:2019-02-12 16:35:01,097: training iter 111, loss=0.5853728652000427 (2.579561s)
INFO:niftynet:2019-02-12 16:35:04,309: training iter 112, loss=0.6949406862258911 (3.212011s)
INFO:niftynet:2019-02-12 16:35:06,373: training iter 113, loss=0.6575844287872314 (2.063796s)
INFO:niftynet:2019-02-12 16:35:09,096: training iter 114, loss=0.5791382789611816 (2.722545s)
INFO:niftynet:2019-02-12 16:35:11,979: training iter 115, loss=0.5659804344177246 (2.881081s)
INFO:niftynet:2019-02-12 16:35:14,377: training iter 116, loss=0.5617592334747314 (2.388715s)
INFO:niftynet:2019-02-12 16:35:17,597: training iter 117, loss=0.6866276860237122 (2.460012s)
INFO:niftynet:2019-02-12 16:35:20,121: training iter 118, loss=0.6080028414726257 (2.523468s)
INFO:niftynet:2019-02-12 16:35:23,010: training iter 119, loss=0.6893069744110107 (2.888684s)
INFO:niftynet:2019-02-12 16:35:25,842: training iter 120, loss=0.6323297023773193 (2.829309s)
INFO:niftynet:2019-02-12 16:35:27,444:     validation iter 120, loss=0.634544849395752 (1.599563s)
INFO:niftynet:2019-02-12 16:35:29,474: training iter 121, loss=0.5793313980102539 (2.028823s)
INFO:niftynet:2019-02-12 16:35:31,712: training iter 122, loss=0.7620627880096436 (2.237530s)
INFO:niftynet:2019-02-12 16:35:34,548: training iter 123, loss=0.5732481479644775 (2.836104s)
INFO:niftynet:2019-02-12 16:35:37,759: training iter 124, loss=0.7611217498779297 (3.210011s)
INFO:niftynet:2019-02-12 16:35:42,359: iter 125 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:35:42,376: training iter 125, loss=0.6208459138870239 (2.720104s)
INFO:niftynet:2019-02-12 16:35:44,796: training iter 126, loss=0.6213104724884033 (2.414105s)
INFO:niftynet:2019-02-12 16:35:47,200: training iter 127, loss=0.7491771578788757 (2.404023s)
INFO:niftynet:2019-02-12 16:35:50,119: training iter 128, loss=0.5262450575828552 (2.918988s)
INFO:niftynet:2019-02-12 16:35:52,572: training iter 129, loss=0.43365710973739624 (2.452188s)
INFO:niftynet:2019-02-12 16:35:55,100: training iter 130, loss=0.7212802171707153 (2.525088s)
INFO:niftynet:2019-02-12 16:35:56,800:     validation iter 130, loss=0.734714925289154 (1.680736s)
INFO:niftynet:2019-02-12 16:35:59,410: training iter 131, loss=0.7212680578231812 (2.609562s)
INFO:niftynet:2019-02-12 16:36:02,174: training iter 132, loss=0.5218650102615356 (2.762230s)
INFO:niftynet:2019-02-12 16:36:04,752: training iter 133, loss=0.5077208280563354 (2.576510s)
INFO:niftynet:2019-02-12 16:36:07,123: training iter 134, loss=0.6574768424034119 (2.371033s)
INFO:niftynet:2019-02-12 16:36:09,825: training iter 135, loss=0.49007877707481384 (2.700155s)
INFO:niftynet:2019-02-12 16:36:12,395: training iter 136, loss=0.7377383708953857 (2.568035s)
INFO:niftynet:2019-02-12 16:36:15,243: training iter 137, loss=0.6268541812896729 (2.847952s)
INFO:niftynet:2019-02-12 16:36:17,788: training iter 138, loss=0.4862689673900604 (2.543565s)
INFO:niftynet:2019-02-12 16:36:20,260: training iter 139, loss=0.6740169525146484 (2.471513s)
INFO:niftynet:2019-02-12 16:36:23,125: training iter 140, loss=0.6735190153121948 (2.862678s)
INFO:niftynet:2019-02-12 16:36:25,121:     validation iter 140, loss=0.5020780563354492 (1.992509s)
INFO:niftynet:2019-02-12 16:36:27,783: training iter 141, loss=0.5207515358924866 (2.658852s)
INFO:niftynet:2019-02-12 16:36:30,028: training iter 142, loss=0.6973819732666016 (2.244925s)
INFO:niftynet:2019-02-12 16:36:33,020: training iter 143, loss=0.6764482259750366 (2.990971s)
INFO:niftynet:2019-02-12 16:36:35,688: training iter 144, loss=0.6845778226852417 (2.664466s)
WARNING:niftynet:2019-02-12 16:36:38,400: User cancelled application
INFO:niftynet:2019-02-12 16:36:38,403: cleaning up...
INFO:niftynet:2019-02-12 16:36:39,994: iter 145 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:36:39,994: stopping sampling threads
WARNING:niftynet:2019-02-12 16:36:44,491: stopped early, incomplete iterations.
INFO:niftynet:2019-02-12 16:36:44,491: SegmentationApplication stopped (time in second 542.10).
INFO:niftynet:2019-02-12 16:38:39,461: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 16:38:39,462: starting segmentation application
INFO:niftynet:2019-02-12 16:38:39,462: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:38:39,468: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:38:39,472: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 16:38:39,475: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 16:38:39,479: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 16:38:39,481: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 16:38:55,819: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 16:38:55,819: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 16:38:57,780: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 16:38:57,780: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 16:38:57,781: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 16:38:57,781: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 16:38:58,720: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 16:38:58,774: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 16:38:58,839: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 16:38:58,840: using DenseVNet
INFO:niftynet:2019-02-12 16:38:58,845: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 16:38:58,883: Initialising Dataset from 29 subjects...
INFO:niftynet:2019-02-12 16:39:02,177: Cross entropy loss function calls tf.nn.sparse_softmax_cross_entropy_with_logits which always performs a softmax internally.
WARNING:niftynet:2019-02-12 16:39:02,250: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/layer/loss_segmentation.py:295: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.
Instructions for updating:
Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:
```python
    sess = tf.Session()
    with sess.as_default():
        tensor = tf.range(10)
        print_op = tf.print(tensor)
        with tf.control_dependencies([print_op]):
          out = tf.add(tensor, tensor)
        sess.run(out)
    ```
Additionally, to use tf.print in python 2.7, users must make sure to import
the following:

  `from __future__ import print_function`

INFO:niftynet:2019-02-12 16:39:12,478: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 16:40:17,140: training iter 1, loss=0.8982380628585815 (64.661372s)
INFO:niftynet:2019-02-12 16:40:19,364: training iter 2, loss=0.38281163573265076 (2.224126s)
INFO:niftynet:2019-02-12 16:40:21,852: training iter 3, loss=0.17025285959243774 (2.486495s)
INFO:niftynet:2019-02-12 16:40:24,573: training iter 4, loss=0.11665138602256775 (2.720202s)
INFO:niftynet:2019-02-12 16:40:37,532: training iter 5, loss=0.10569069534540176 (12.958935s)
INFO:niftynet:2019-02-12 16:40:40,234: training iter 6, loss=0.04253146052360535 (2.700042s)
INFO:niftynet:2019-02-12 16:40:42,923: training iter 7, loss=0.10542412102222443 (2.687927s)
INFO:niftynet:2019-02-12 16:40:45,584: training iter 8, loss=0.13310429453849792 (2.660461s)
INFO:niftynet:2019-02-12 16:40:48,170: training iter 9, loss=0.13849197328090668 (2.584975s)
INFO:niftynet:2019-02-12 16:40:51,063: training iter 10, loss=0.09576711058616638 (2.893041s)
INFO:niftynet:2019-02-12 16:41:26,146:     validation iter 10, loss=0.06362517923116684 (35.081140s)
INFO:niftynet:2019-02-12 16:41:29,903: training iter 11, loss=0.10633639991283417 (3.755197s)
INFO:niftynet:2019-02-12 16:41:34,673: training iter 12, loss=0.047783106565475464 (4.770275s)
INFO:niftynet:2019-02-12 16:41:36,945: training iter 13, loss=0.03366568684577942 (2.271193s)
INFO:niftynet:2019-02-12 16:41:39,714: training iter 14, loss=0.07524450123310089 (2.769240s)
INFO:niftynet:2019-02-12 16:41:43,744: training iter 15, loss=0.04597747325897217 (4.021506s)
INFO:niftynet:2019-02-12 16:41:48,879: training iter 16, loss=0.04676203429698944 (5.132643s)
INFO:niftynet:2019-02-12 16:41:51,366: training iter 17, loss=0.028225749731063843 (2.486341s)
INFO:niftynet:2019-02-12 16:41:55,026: training iter 18, loss=0.022436007857322693 (3.660161s)
INFO:niftynet:2019-02-12 16:41:58,694: training iter 19, loss=0.03269664943218231 (3.667041s)
INFO:niftynet:2019-02-12 16:42:01,781: training iter 20, loss=0.10045907646417618 (3.086185s)
INFO:niftynet:2019-02-12 16:42:04,175:     validation iter 20, loss=0.133172869682312 (2.392099s)
INFO:niftynet:2019-02-12 16:42:06,852: training iter 21, loss=-0.004769608378410339 (2.676035s)
INFO:niftynet:2019-02-12 16:42:09,233: training iter 22, loss=0.009943865239620209 (2.381204s)
INFO:niftynet:2019-02-12 16:42:11,783: training iter 23, loss=0.13047762215137482 (2.549336s)
INFO:niftynet:2019-02-12 16:42:14,356: training iter 24, loss=0.12341582775115967 (2.572642s)
INFO:niftynet:2019-02-12 16:42:17,049: training iter 25, loss=0.08259867876768112 (2.692122s)
INFO:niftynet:2019-02-12 16:42:19,010: iter 25 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:42:21,611: training iter 26, loss=-0.014971062541007996 (2.598850s)
INFO:niftynet:2019-02-12 16:42:23,971: training iter 27, loss=0.08639329671859741 (2.358096s)
INFO:niftynet:2019-02-12 16:42:26,517: training iter 28, loss=0.0623156875371933 (2.545626s)
INFO:niftynet:2019-02-12 16:42:29,238: training iter 29, loss=0.02433861792087555 (2.719873s)
INFO:niftynet:2019-02-12 16:42:31,576: training iter 30, loss=0.06503911316394806 (2.336365s)
INFO:niftynet:2019-02-12 16:42:34,158:     validation iter 30, loss=0.023369453847408295 (2.580653s)
INFO:niftynet:2019-02-12 16:42:36,456: training iter 31, loss=0.18488839268684387 (2.296523s)
INFO:niftynet:2019-02-12 16:42:38,590: training iter 32, loss=0.018644630908966064 (2.134444s)
INFO:niftynet:2019-02-12 16:42:41,824: training iter 33, loss=0.06609068810939789 (3.233605s)
INFO:niftynet:2019-02-12 16:42:44,534: training iter 34, loss=0.08387042582035065 (2.709126s)
INFO:niftynet:2019-02-12 16:42:47,723: training iter 35, loss=0.10760465264320374 (3.189358s)
INFO:niftynet:2019-02-12 16:42:50,388: training iter 36, loss=0.11743194609880447 (2.663426s)
INFO:niftynet:2019-02-12 16:42:52,651: training iter 37, loss=0.0005423054099082947 (2.262461s)
INFO:niftynet:2019-02-12 16:42:54,769: training iter 38, loss=0.059630587697029114 (2.116492s)
INFO:niftynet:2019-02-12 16:42:57,481: training iter 39, loss=0.05947759747505188 (2.711134s)
INFO:niftynet:2019-02-12 16:42:59,935: training iter 40, loss=0.03716889023780823 (2.453310s)
INFO:niftynet:2019-02-12 16:43:02,672:     validation iter 40, loss=0.014799870550632477 (2.735425s)
INFO:niftynet:2019-02-12 16:43:05,013: training iter 41, loss=0.152432382106781 (2.339443s)
INFO:niftynet:2019-02-12 16:43:08,014: training iter 42, loss=-0.0061113834381103516 (3.000579s)
INFO:niftynet:2019-02-12 16:43:10,564: training iter 43, loss=0.12082201987504959 (2.549177s)
INFO:niftynet:2019-02-12 16:43:12,652: training iter 44, loss=0.031927913427352905 (2.086945s)
INFO:niftynet:2019-02-12 16:43:15,244: training iter 45, loss=0.045501917600631714 (2.591831s)
INFO:niftynet:2019-02-12 16:43:17,650: training iter 46, loss=0.032587043941020966 (2.403961s)
INFO:niftynet:2019-02-12 16:43:20,116: training iter 47, loss=0.13974910974502563 (2.465245s)
INFO:niftynet:2019-02-12 16:43:22,671: training iter 48, loss=0.15491914749145508 (2.553529s)
INFO:niftynet:2019-02-12 16:43:25,399: training iter 49, loss=-0.012704379856586456 (2.725947s)
INFO:niftynet:2019-02-12 16:43:28,063: training iter 50, loss=0.07534407079219818 (2.664275s)
INFO:niftynet:2019-02-12 16:43:29,980: iter 50 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:43:32,280:     validation iter 50, loss=0.04220564663410187 (2.298949s)
INFO:niftynet:2019-02-12 16:43:34,341: training iter 51, loss=-0.012280166149139404 (2.059367s)
INFO:niftynet:2019-02-12 16:43:36,778: training iter 52, loss=0.051273494958877563 (2.436474s)
INFO:niftynet:2019-02-12 16:43:39,161: training iter 53, loss=0.045486435294151306 (2.383241s)
INFO:niftynet:2019-02-12 16:43:41,816: training iter 54, loss=0.0855887308716774 (2.654752s)
INFO:niftynet:2019-02-12 16:43:44,653: training iter 55, loss=0.11247354745864868 (2.835538s)
INFO:niftynet:2019-02-12 16:43:47,064: training iter 56, loss=0.0567144900560379 (2.408860s)
INFO:niftynet:2019-02-12 16:43:49,796: training iter 57, loss=0.15391862392425537 (2.732239s)
INFO:niftynet:2019-02-12 16:43:52,158: training iter 58, loss=-0.006088830530643463 (2.361804s)
INFO:niftynet:2019-02-12 16:43:54,611: training iter 59, loss=0.004329487681388855 (2.452618s)
INFO:niftynet:2019-02-12 16:43:58,083: training iter 60, loss=-0.0036220625042915344 (3.470775s)
INFO:niftynet:2019-02-12 16:44:00,458:     validation iter 60, loss=0.020930416882038116 (2.374451s)
INFO:niftynet:2019-02-12 16:44:02,757: training iter 61, loss=0.020474538207054138 (2.296541s)
INFO:niftynet:2019-02-12 16:44:05,226: training iter 62, loss=0.22414562106132507 (2.468871s)
INFO:niftynet:2019-02-12 16:44:07,804: training iter 63, loss=0.06490176916122437 (2.577160s)
INFO:niftynet:2019-02-12 16:44:10,622: training iter 64, loss=0.027928680181503296 (2.818389s)
INFO:niftynet:2019-02-12 16:44:13,305: training iter 65, loss=-2.2158026695251465e-05 (2.682103s)
INFO:niftynet:2019-02-12 16:44:16,342: training iter 66, loss=0.056418612599372864 (3.035819s)
INFO:niftynet:2019-02-12 16:44:19,042: training iter 67, loss=0.041080281138420105 (2.698657s)
INFO:niftynet:2019-02-12 16:44:21,603: training iter 68, loss=0.007985621690750122 (2.560646s)
INFO:niftynet:2019-02-12 16:44:24,374: training iter 69, loss=-0.01326579600572586 (2.770569s)
INFO:niftynet:2019-02-12 16:44:27,176: training iter 70, loss=-0.09405212849378586 (2.800337s)
INFO:niftynet:2019-02-12 16:44:29,396:     validation iter 70, loss=0.059587590396404266 (2.218367s)
INFO:niftynet:2019-02-12 16:44:31,411: training iter 71, loss=-0.0006118863821029663 (2.013389s)
INFO:niftynet:2019-02-12 16:44:34,005: training iter 72, loss=-0.04455428570508957 (2.593349s)
INFO:niftynet:2019-02-12 16:44:36,221: training iter 73, loss=0.004279240965843201 (2.215097s)
INFO:niftynet:2019-02-12 16:44:38,567: training iter 74, loss=0.007480204105377197 (2.345214s)
INFO:niftynet:2019-02-12 16:44:41,080: training iter 75, loss=0.05882905423641205 (2.512610s)
INFO:niftynet:2019-02-12 16:44:42,763: iter 75 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:44:44,630: training iter 76, loss=-0.058490052819252014 (1.865028s)
INFO:niftynet:2019-02-12 16:44:46,473: training iter 77, loss=0.03381448984146118 (1.842584s)
INFO:niftynet:2019-02-12 16:44:48,487: training iter 78, loss=-0.02040640264749527 (2.013815s)
INFO:niftynet:2019-02-12 16:44:50,400: training iter 79, loss=-0.11527904123067856 (1.912469s)
INFO:niftynet:2019-02-12 16:44:52,766: training iter 80, loss=0.014827288687229156 (2.365879s)
INFO:niftynet:2019-02-12 16:44:55,493:     validation iter 80, loss=-0.08176349103450775 (2.725260s)
INFO:niftynet:2019-02-12 16:44:58,381: training iter 81, loss=0.01776893436908722 (2.885790s)
INFO:niftynet:2019-02-12 16:45:00,809: training iter 82, loss=0.35340237617492676 (2.428512s)
INFO:niftynet:2019-02-12 16:45:03,080: training iter 83, loss=-0.08067232370376587 (2.270038s)
INFO:niftynet:2019-02-12 16:45:05,084: training iter 84, loss=0.09949004650115967 (2.000217s)
INFO:niftynet:2019-02-12 16:45:07,880: training iter 85, loss=0.036577530205249786 (2.794413s)
INFO:niftynet:2019-02-12 16:45:10,696: training iter 86, loss=-0.0007718205451965332 (2.814942s)
INFO:niftynet:2019-02-12 16:45:12,824: training iter 87, loss=-0.01080448180437088 (2.126180s)
INFO:niftynet:2019-02-12 16:45:15,283: training iter 88, loss=0.08749887347221375 (2.458738s)
INFO:niftynet:2019-02-12 16:45:17,640: training iter 89, loss=-0.032066360116004944 (2.356660s)
INFO:niftynet:2019-02-12 16:45:20,647: training iter 90, loss=-0.0839853584766388 (3.006379s)
INFO:niftynet:2019-02-12 16:45:26,470:     validation iter 90, loss=0.03517010062932968 (5.822137s)
INFO:niftynet:2019-02-12 16:45:28,797: training iter 91, loss=0.0018450170755386353 (2.324826s)
INFO:niftynet:2019-02-12 16:45:31,308: training iter 92, loss=0.03722894936800003 (2.509534s)
INFO:niftynet:2019-02-12 16:45:34,334: training iter 93, loss=0.01601903885602951 (3.024362s)
INFO:niftynet:2019-02-12 16:45:36,690: training iter 94, loss=0.035059764981269836 (2.355270s)
INFO:niftynet:2019-02-12 16:45:39,766: training iter 95, loss=0.05358424782752991 (3.075245s)
INFO:niftynet:2019-02-12 16:45:42,509: training iter 96, loss=-0.06593453139066696 (2.740492s)
INFO:niftynet:2019-02-12 16:45:44,644: training iter 97, loss=0.039487823843955994 (2.134672s)
INFO:niftynet:2019-02-12 16:45:47,067: training iter 98, loss=0.015396520495414734 (2.423443s)
INFO:niftynet:2019-02-12 16:45:49,218: training iter 99, loss=-0.03148414194583893 (2.150480s)
INFO:niftynet:2019-02-12 16:45:52,258: training iter 100, loss=0.044140227138996124 (3.039305s)
INFO:niftynet:2019-02-12 16:45:55,022: iter 100 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:45:57,217:     validation iter 100, loss=0.04935562610626221 (2.170991s)
INFO:niftynet:2019-02-12 16:45:59,812: training iter 101, loss=-0.032751522958278656 (2.593310s)
INFO:niftynet:2019-02-12 16:46:02,232: training iter 102, loss=-0.041700974106788635 (2.419611s)
INFO:niftynet:2019-02-12 16:46:04,773: training iter 103, loss=0.002702362835407257 (2.540477s)
INFO:niftynet:2019-02-12 16:46:07,159: training iter 104, loss=-0.07060138881206512 (2.384303s)
INFO:niftynet:2019-02-12 16:46:10,310: training iter 105, loss=0.0026041343808174133 (3.149955s)
INFO:niftynet:2019-02-12 16:46:13,234: training iter 106, loss=0.007787443697452545 (2.923043s)
INFO:niftynet:2019-02-12 16:46:15,772: training iter 107, loss=0.02927251160144806 (2.537129s)
INFO:niftynet:2019-02-12 16:46:18,199: training iter 108, loss=-0.08123002201318741 (2.427304s)
INFO:niftynet:2019-02-12 16:46:20,794: training iter 109, loss=-0.04911894351243973 (2.593244s)
INFO:niftynet:2019-02-12 16:46:24,033: training iter 110, loss=0.02038690447807312 (3.237650s)
INFO:niftynet:2019-02-12 16:46:26,395:     validation iter 110, loss=0.03632713854312897 (2.359184s)
INFO:niftynet:2019-02-12 16:46:28,771: training iter 111, loss=-0.03543524444103241 (2.373238s)
INFO:niftynet:2019-02-12 16:46:31,144: training iter 112, loss=-0.05344463884830475 (2.373092s)
INFO:niftynet:2019-02-12 16:46:33,660: training iter 113, loss=-0.07591177523136139 (2.514341s)
INFO:niftynet:2019-02-12 16:46:36,690: training iter 114, loss=-0.05424794554710388 (3.029891s)
INFO:niftynet:2019-02-12 16:46:39,851: training iter 115, loss=0.05894637852907181 (3.159575s)
INFO:niftynet:2019-02-12 16:46:41,981: training iter 116, loss=-0.013423174619674683 (2.124087s)
INFO:niftynet:2019-02-12 16:46:44,388: training iter 117, loss=0.045324020087718964 (2.406821s)
INFO:niftynet:2019-02-12 16:46:46,819: training iter 118, loss=-0.08534756302833557 (2.429501s)
INFO:niftynet:2019-02-12 16:46:49,544: training iter 119, loss=0.011937953531742096 (2.723727s)
INFO:niftynet:2019-02-12 16:46:52,097: training iter 120, loss=0.009618103504180908 (2.551309s)
INFO:niftynet:2019-02-12 16:46:54,653:     validation iter 120, loss=0.06519605219364166 (2.555014s)
INFO:niftynet:2019-02-12 16:46:57,704: training iter 121, loss=0.012457318603992462 (3.049678s)
INFO:niftynet:2019-02-12 16:47:00,274: training iter 122, loss=-0.000843144953250885 (2.568377s)
INFO:niftynet:2019-02-12 16:47:02,963: training iter 123, loss=0.042321979999542236 (2.689410s)
INFO:niftynet:2019-02-12 16:47:05,244: training iter 124, loss=-0.0034829899668693542 (2.278854s)
INFO:niftynet:2019-02-12 16:47:07,808: training iter 125, loss=-0.01919356733560562 (2.563955s)
INFO:niftynet:2019-02-12 16:47:09,940: iter 125 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:47:12,365: training iter 126, loss=0.04272247850894928 (2.424151s)
INFO:niftynet:2019-02-12 16:47:14,758: training iter 127, loss=0.004115335643291473 (2.391990s)
INFO:niftynet:2019-02-12 16:47:17,655: training iter 128, loss=0.08914199471473694 (2.896318s)
INFO:niftynet:2019-02-12 16:47:20,445: training iter 129, loss=0.07957753539085388 (2.789731s)
INFO:niftynet:2019-02-12 16:47:22,909: training iter 130, loss=0.11610815674066544 (2.462269s)
WARNING:niftynet:2019-02-12 16:47:25,098: User cancelled application
INFO:niftynet:2019-02-12 16:47:25,098: cleaning up...
INFO:niftynet:2019-02-12 16:47:26,775: iter 130 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 16:47:26,775: stopping sampling threads
WARNING:niftynet:2019-02-12 16:47:29,627: stopped early, incomplete iterations.
INFO:niftynet:2019-02-12 16:47:29,628: SegmentationApplication stopped (time in second 498.95).
INFO:niftynet:2019-02-12 17:03:23,611: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 17:03:23,611: starting segmentation application
INFO:niftynet:2019-02-12 17:03:23,611: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 17:03:23,617: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 17:03:23,621: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 17:03:23,624: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 17:03:23,628: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 17:03:23,630: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 17:03:49,542: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 17:03:49,542: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 17:03:49,542: Image reader: loading 227 subjects from sections ('label',) as input [sampler]
INFO:niftynet:2019-02-12 17:03:52,929: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 17:03:52,929: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 17:03:52,929: Image reader: loading 29 subjects from sections ('label',) as input [sampler]
INFO:niftynet:2019-02-12 17:03:52,930: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 17:03:52,931: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 17:03:54,879: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7), 'sampler': (1, 128, 128, 128, 1, 1), 'sampler_location': (1, 7)} 
INFO:niftynet:2019-02-12 17:03:54,932: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7), 'sampler': (1, 128, 128, 128, 1, 1), 'sampler_location': (1, 7)} 
WARNING:niftynet:2019-02-12 17:03:54,996: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 17:03:54,996: using DenseVNet
INFO:niftynet:2019-02-12 17:03:55,001: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 17:03:55,044: Initialising Dataset from 29 subjects...
INFO:niftynet:2019-02-12 17:03:58,302: Cross entropy loss function calls tf.nn.sparse_softmax_cross_entropy_with_logits which always performs a softmax internally.
INFO:niftynet:2019-02-12 17:04:08,194: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 17:05:22,311: training iter 1, loss=1.2656385898590088 (74.116614s)
INFO:niftynet:2019-02-12 17:05:23,256: training iter 2, loss=0.7671595811843872 (0.944582s)
INFO:niftynet:2019-02-12 17:05:23,950: training iter 3, loss=0.4117751121520996 (0.693991s)
INFO:niftynet:2019-02-12 17:05:24,605: training iter 4, loss=0.4608023166656494 (0.654771s)
INFO:niftynet:2019-02-12 17:05:34,442: training iter 5, loss=0.4034005403518677 (9.836264s)
INFO:niftynet:2019-02-12 17:05:35,189: training iter 6, loss=0.3655287027359009 (0.745996s)
INFO:niftynet:2019-02-12 17:05:35,968: training iter 7, loss=0.3144620656967163 (0.777933s)
INFO:niftynet:2019-02-12 17:05:36,688: training iter 8, loss=0.29920628666877747 (0.720308s)
INFO:niftynet:2019-02-12 17:05:40,151: training iter 9, loss=0.37048816680908203 (3.462704s)
INFO:niftynet:2019-02-12 17:05:41,124: training iter 10, loss=0.34137266874313354 (0.971047s)
INFO:niftynet:2019-02-12 17:06:19,278:     validation iter 10, loss=0.31904080510139465 (38.152376s)
INFO:niftynet:2019-02-12 17:06:20,470: training iter 11, loss=0.4126494526863098 (1.189987s)
INFO:niftynet:2019-02-12 17:06:25,037: training iter 12, loss=0.384512722492218 (4.566082s)
INFO:niftynet:2019-02-12 17:06:27,115: training iter 13, loss=0.336935430765152 (2.077180s)
INFO:niftynet:2019-02-12 17:06:27,844: training iter 14, loss=0.3293105363845825 (0.727982s)
INFO:niftynet:2019-02-12 17:06:28,580: training iter 15, loss=0.3582862615585327 (0.735123s)
INFO:niftynet:2019-02-12 17:06:29,222: training iter 16, loss=0.2546430230140686 (0.638781s)
INFO:niftynet:2019-02-12 17:06:29,839: training iter 17, loss=0.3035627603530884 (0.617415s)
INFO:niftynet:2019-02-12 17:06:30,521: training iter 18, loss=0.5842838883399963 (0.663803s)
INFO:niftynet:2019-02-12 17:06:31,184: training iter 19, loss=0.43603432178497314 (0.661451s)
INFO:niftynet:2019-02-12 17:06:32,088: training iter 20, loss=0.4002336263656616 (0.902230s)
INFO:niftynet:2019-02-12 17:06:36,710:     validation iter 20, loss=0.3575408160686493 (4.619135s)
INFO:niftynet:2019-02-12 17:06:38,847: training iter 21, loss=0.3146883249282837 (2.133851s)
INFO:niftynet:2019-02-12 17:06:45,086: training iter 22, loss=0.3461846709251404 (6.238927s)
INFO:niftynet:2019-02-12 17:06:46,039: training iter 23, loss=0.38403141498565674 (0.950139s)
INFO:niftynet:2019-02-12 17:06:51,273: training iter 24, loss=0.3043506145477295 (5.234037s)
INFO:niftynet:2019-02-12 17:06:52,071: training iter 25, loss=0.30337876081466675 (0.797588s)
INFO:niftynet:2019-02-12 17:06:54,059: iter 25 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:06:54,706: training iter 26, loss=0.26948416233062744 (0.646480s)
INFO:niftynet:2019-02-12 17:06:55,429: training iter 27, loss=0.3571721911430359 (0.722832s)
INFO:niftynet:2019-02-12 17:06:56,033: training iter 28, loss=0.3173457086086273 (0.603889s)
INFO:niftynet:2019-02-12 17:06:56,715: training iter 29, loss=0.28077763319015503 (0.680998s)
INFO:niftynet:2019-02-12 17:06:57,526: training iter 30, loss=0.34357744455337524 (0.811463s)
INFO:niftynet:2019-02-12 17:06:58,213:     validation iter 30, loss=0.28573620319366455 (0.683322s)
INFO:niftynet:2019-02-12 17:06:58,915: training iter 31, loss=0.34881263971328735 (0.699979s)
INFO:niftynet:2019-02-12 17:06:59,614: training iter 32, loss=0.26023805141448975 (0.692263s)
INFO:niftynet:2019-02-12 17:07:03,234: training iter 33, loss=0.2738523483276367 (3.618643s)
INFO:niftynet:2019-02-12 17:07:05,719: training iter 34, loss=0.25667712092399597 (2.484953s)
INFO:niftynet:2019-02-12 17:07:08,086: training iter 35, loss=0.2995065748691559 (2.366257s)
INFO:niftynet:2019-02-12 17:07:12,172: training iter 36, loss=0.30094996094703674 (4.084515s)
INFO:niftynet:2019-02-12 17:07:14,089: training iter 37, loss=0.30685579776763916 (1.915828s)
INFO:niftynet:2019-02-12 17:07:17,525: training iter 38, loss=0.3265090882778168 (3.435469s)
INFO:niftynet:2019-02-12 17:07:21,130: training iter 39, loss=0.32564473152160645 (3.604822s)
INFO:niftynet:2019-02-12 17:07:24,086: training iter 40, loss=0.3464568853378296 (2.953942s)
INFO:niftynet:2019-02-12 17:07:24,696:     validation iter 40, loss=0.3071119487285614 (0.607554s)
INFO:niftynet:2019-02-12 17:07:25,390: training iter 41, loss=0.31047800183296204 (0.692287s)
INFO:niftynet:2019-02-12 17:07:27,149: training iter 42, loss=0.5029242038726807 (1.758723s)
INFO:niftynet:2019-02-12 17:07:32,081: training iter 43, loss=0.3968501091003418 (4.931552s)
INFO:niftynet:2019-02-12 17:07:33,926: training iter 44, loss=0.31441211700439453 (1.817287s)
INFO:niftynet:2019-02-12 17:07:37,668: training iter 45, loss=0.4677669405937195 (3.741493s)
INFO:niftynet:2019-02-12 17:07:38,656: training iter 46, loss=0.3205944895744324 (0.985056s)
INFO:niftynet:2019-02-12 17:07:42,417: training iter 47, loss=0.33737480640411377 (3.759499s)
INFO:niftynet:2019-02-12 17:07:43,778: training iter 48, loss=0.3968951106071472 (1.341915s)
INFO:niftynet:2019-02-12 17:07:48,248: training iter 49, loss=0.3617470860481262 (4.468157s)
INFO:niftynet:2019-02-12 17:07:50,438: training iter 50, loss=0.37561124563217163 (2.189498s)
INFO:niftynet:2019-02-12 17:07:52,570: iter 50 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:07:53,252:     validation iter 50, loss=0.3111168444156647 (0.681077s)
INFO:niftynet:2019-02-12 17:07:53,916: training iter 51, loss=0.30142295360565186 (0.661891s)
INFO:niftynet:2019-02-12 17:07:54,522: training iter 52, loss=0.27328675985336304 (0.605922s)
INFO:niftynet:2019-02-12 17:07:57,846: training iter 53, loss=0.30404502153396606 (3.322752s)
INFO:niftynet:2019-02-12 17:08:01,382: training iter 54, loss=0.2694385051727295 (3.536470s)
INFO:niftynet:2019-02-12 17:08:03,287: training iter 55, loss=0.34775957465171814 (1.903454s)
INFO:niftynet:2019-02-12 17:08:05,373: training iter 56, loss=0.2848328948020935 (2.084004s)
INFO:niftynet:2019-02-12 17:08:08,970: training iter 57, loss=0.26984095573425293 (3.595930s)
INFO:niftynet:2019-02-12 17:08:10,326: training iter 58, loss=0.26019883155822754 (1.355767s)
INFO:niftynet:2019-02-12 17:08:14,815: training iter 59, loss=0.47669801115989685 (4.488012s)
INFO:niftynet:2019-02-12 17:08:16,924: training iter 60, loss=0.29810118675231934 (2.108073s)
INFO:niftynet:2019-02-12 17:08:17,479:     validation iter 60, loss=0.2753250002861023 (0.553321s)
INFO:niftynet:2019-02-12 17:08:20,879: training iter 61, loss=0.4147798717021942 (3.398887s)
INFO:niftynet:2019-02-12 17:08:21,687: training iter 62, loss=0.3788785934448242 (0.807667s)
INFO:niftynet:2019-02-12 17:08:26,098: training iter 63, loss=0.3911530375480652 (4.410561s)
INFO:niftynet:2019-02-12 17:08:28,570: training iter 64, loss=0.2800149619579315 (2.467279s)
INFO:niftynet:2019-02-12 17:08:34,369: training iter 65, loss=0.3388451337814331 (5.798524s)
INFO:niftynet:2019-02-12 17:08:35,014: training iter 66, loss=0.2838934361934662 (0.642465s)
INFO:niftynet:2019-02-12 17:08:39,903: training iter 67, loss=0.31307512521743774 (4.889047s)
INFO:niftynet:2019-02-12 17:08:40,616: training iter 68, loss=0.3839261531829834 (0.712557s)
INFO:niftynet:2019-02-12 17:08:42,812: training iter 69, loss=0.3157309293746948 (2.195825s)
INFO:niftynet:2019-02-12 17:08:47,620: training iter 70, loss=0.30264079570770264 (4.806774s)
INFO:niftynet:2019-02-12 17:08:48,223:     validation iter 70, loss=0.31276530027389526 (0.600474s)
INFO:niftynet:2019-02-12 17:08:48,788: training iter 71, loss=0.297448068857193 (0.563424s)
INFO:niftynet:2019-02-12 17:08:50,714: training iter 72, loss=0.2911681532859802 (1.925326s)
INFO:niftynet:2019-02-12 17:08:51,797: training iter 73, loss=0.30508995056152344 (1.080892s)
INFO:niftynet:2019-02-12 17:08:56,855: training iter 74, loss=0.32466793060302734 (5.058476s)
INFO:niftynet:2019-02-12 17:08:59,825: training iter 75, loss=0.2831191420555115 (2.969132s)
INFO:niftynet:2019-02-12 17:09:02,037: iter 75 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:09:02,693: training iter 76, loss=0.3049357831478119 (0.653154s)
INFO:niftynet:2019-02-12 17:09:07,308: training iter 77, loss=0.3029291033744812 (4.614333s)
INFO:niftynet:2019-02-12 17:09:08,839: training iter 78, loss=0.33579668402671814 (1.530789s)
INFO:niftynet:2019-02-12 17:09:13,132: training iter 79, loss=0.32196784019470215 (4.291009s)
INFO:niftynet:2019-02-12 17:09:15,114: training iter 80, loss=0.3018701672554016 (1.981176s)
INFO:niftynet:2019-02-12 17:09:15,700:     validation iter 80, loss=0.3281063735485077 (0.584684s)
INFO:niftynet:2019-02-12 17:09:17,799: training iter 81, loss=0.2908248007297516 (2.097671s)
INFO:niftynet:2019-02-12 17:09:20,709: training iter 82, loss=0.273163378238678 (2.909554s)
INFO:niftynet:2019-02-12 17:09:21,319: training iter 83, loss=0.3179766535758972 (0.609527s)
INFO:niftynet:2019-02-12 17:09:23,617: training iter 84, loss=0.29576388001441956 (2.296841s)
INFO:niftynet:2019-02-12 17:09:28,963: training iter 85, loss=0.3007259964942932 (5.344126s)
INFO:niftynet:2019-02-12 17:09:32,469: training iter 86, loss=0.2521858811378479 (3.504290s)
INFO:niftynet:2019-02-12 17:09:33,124: training iter 87, loss=0.2755066156387329 (0.654320s)
INFO:niftynet:2019-02-12 17:09:36,066: training iter 88, loss=0.25581878423690796 (2.941061s)
INFO:niftynet:2019-02-12 17:09:40,133: training iter 89, loss=0.26379120349884033 (4.066555s)
INFO:niftynet:2019-02-12 17:09:41,072: training iter 90, loss=0.27344539761543274 (0.938615s)
INFO:niftynet:2019-02-12 17:09:47,752:     validation iter 90, loss=0.30889391899108887 (6.677408s)
INFO:niftynet:2019-02-12 17:09:48,422: training iter 91, loss=0.33132871985435486 (0.667095s)
INFO:niftynet:2019-02-12 17:09:49,091: training iter 92, loss=0.3869122862815857 (0.668133s)
INFO:niftynet:2019-02-12 17:09:55,323: training iter 93, loss=0.3623012602329254 (6.231912s)
INFO:niftynet:2019-02-12 17:09:56,036: training iter 94, loss=0.34775209426879883 (0.712382s)
INFO:niftynet:2019-02-12 17:09:58,728: training iter 95, loss=0.247488334774971 (2.691766s)
INFO:niftynet:2019-02-12 17:09:59,260: training iter 96, loss=0.3254934549331665 (0.529942s)
INFO:niftynet:2019-02-12 17:10:10,561: training iter 97, loss=0.33150535821914673 (11.300520s)
INFO:niftynet:2019-02-12 17:10:11,391: training iter 98, loss=0.30877256393432617 (0.826304s)
INFO:niftynet:2019-02-12 17:10:11,933: training iter 99, loss=0.26886457204818726 (0.539285s)
INFO:niftynet:2019-02-12 17:10:12,704: training iter 100, loss=0.3096507787704468 (0.770009s)
INFO:niftynet:2019-02-12 17:10:14,164: cleaning up...
INFO:niftynet:2019-02-12 17:10:14,165: stopping sampling threads
INFO:niftynet:2019-02-12 17:18:35,089: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 17:18:35,089: starting segmentation application
INFO:niftynet:2019-02-12 17:18:35,090: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 17:18:35,096: [T1ce] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1ce.csv, skipped filenames search
INFO:niftynet:2019-02-12 17:18:35,100: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 17:18:35,104: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 17:18:35,108: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 17:18:35,110: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T1ce', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 17:18:58,234: Image reader: loading 227 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 17:18:58,234: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 17:18:58,234: Image reader: loading 227 subjects from sections ('label',) as input [sampler]
INFO:niftynet:2019-02-12 17:19:00,827: Image reader: loading 29 subjects from sections ('T1', 'T1ce', 'T2') as input [image]
INFO:niftynet:2019-02-12 17:19:00,827: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 17:19:00,831: Image reader: loading 29 subjects from sections ('label',) as input [sampler]
INFO:niftynet:2019-02-12 17:19:00,834: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 17:19:00,834: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 17:19:02,944: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7), 'sampler': (1, 128, 128, 128, 1, 1), 'sampler_location': (1, 7)} 
INFO:niftynet:2019-02-12 17:19:02,991: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 3), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7), 'sampler': (1, 128, 128, 128, 1, 1), 'sampler_location': (1, 7)} 
WARNING:niftynet:2019-02-12 17:19:03,104: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 17:19:03,104: using DenseVNet
INFO:niftynet:2019-02-12 17:19:03,111: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 17:19:03,168: Initialising Dataset from 29 subjects...
INFO:niftynet:2019-02-12 17:19:06,365: Cross entropy loss function calls tf.nn.sparse_softmax_cross_entropy_with_logits which always performs a softmax internally.
INFO:niftynet:2019-02-12 17:19:16,302: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 17:20:27,888: training iter 1, loss=1.7262828350067139 (71.585729s)
INFO:niftynet:2019-02-12 17:20:29,170: training iter 2, loss=1.0544140338897705 (1.280256s)
INFO:niftynet:2019-02-12 17:20:29,837: training iter 3, loss=0.6328345537185669 (0.665911s)
INFO:niftynet:2019-02-12 17:20:30,789: training iter 4, loss=0.3955276608467102 (0.951080s)
INFO:niftynet:2019-02-12 17:20:40,970: training iter 5, loss=0.42032113671302795 (10.180411s)
INFO:niftynet:2019-02-12 17:20:41,666: training iter 6, loss=0.3294913172721863 (0.694671s)
INFO:niftynet:2019-02-12 17:20:42,355: training iter 7, loss=0.2831050157546997 (0.688615s)
INFO:niftynet:2019-02-12 17:20:43,039: training iter 8, loss=0.34848231077194214 (0.683698s)
INFO:niftynet:2019-02-12 17:20:43,690: training iter 9, loss=0.29563775658607483 (0.638987s)
INFO:niftynet:2019-02-12 17:20:45,092: training iter 10, loss=0.39101603627204895 (1.397149s)
INFO:niftynet:2019-02-12 17:21:25,000:     validation iter 10, loss=0.27052193880081177 (39.893238s)
INFO:niftynet:2019-02-12 17:21:25,847: training iter 11, loss=0.29364749789237976 (0.844950s)
INFO:niftynet:2019-02-12 17:21:26,691: training iter 12, loss=0.3626329302787781 (0.842247s)
INFO:niftynet:2019-02-12 17:21:32,959: training iter 13, loss=0.4919126629829407 (6.267799s)
INFO:niftynet:2019-02-12 17:21:33,640: training iter 14, loss=0.3176485300064087 (0.679503s)
INFO:niftynet:2019-02-12 17:21:34,475: training iter 15, loss=0.3869851529598236 (0.834272s)
INFO:niftynet:2019-02-12 17:21:35,110: training iter 16, loss=0.319469153881073 (0.633373s)
INFO:niftynet:2019-02-12 17:21:35,788: training iter 17, loss=0.3430206775665283 (0.675696s)
INFO:niftynet:2019-02-12 17:21:36,409: training iter 18, loss=0.3669864535331726 (0.619765s)
INFO:niftynet:2019-02-12 17:21:37,073: training iter 19, loss=0.3152790069580078 (0.663530s)
INFO:niftynet:2019-02-12 17:21:37,953: training iter 20, loss=0.3344465494155884 (0.879146s)
INFO:niftynet:2019-02-12 17:21:38,682:     validation iter 20, loss=0.2689793109893799 (0.725847s)
INFO:niftynet:2019-02-12 17:21:40,565: training iter 21, loss=0.2900632917881012 (1.880144s)
INFO:niftynet:2019-02-12 17:21:45,140: training iter 22, loss=0.2636565864086151 (4.574824s)
INFO:niftynet:2019-02-12 17:21:46,110: training iter 23, loss=0.315573513507843 (0.967198s)
INFO:niftynet:2019-02-12 17:21:48,136: training iter 24, loss=0.30225926637649536 (2.025404s)
INFO:niftynet:2019-02-12 17:21:52,041: training iter 25, loss=0.3051999509334564 (3.905214s)
INFO:niftynet:2019-02-12 17:21:55,003: iter 25 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:22:00,129: training iter 26, loss=0.3642710745334625 (5.125932s)
INFO:niftynet:2019-02-12 17:22:00,856: training iter 27, loss=0.3674333691596985 (0.726840s)
INFO:niftynet:2019-02-12 17:22:01,587: training iter 28, loss=0.3836214542388916 (0.730463s)
INFO:niftynet:2019-02-12 17:22:02,285: training iter 29, loss=0.2862806022167206 (0.698041s)
INFO:niftynet:2019-02-12 17:22:05,114: training iter 30, loss=0.32874059677124023 (2.828786s)
INFO:niftynet:2019-02-12 17:22:05,710:     validation iter 30, loss=0.26726844906806946 (0.594213s)
INFO:niftynet:2019-02-12 17:22:06,390: training iter 31, loss=0.2781729996204376 (0.673542s)
INFO:niftynet:2019-02-12 17:22:09,403: training iter 32, loss=0.2645333707332611 (3.012616s)
INFO:niftynet:2019-02-12 17:22:10,152: training iter 33, loss=0.2509010434150696 (0.748073s)
INFO:niftynet:2019-02-12 17:22:13,770: training iter 34, loss=0.28353068232536316 (3.617776s)
INFO:niftynet:2019-02-12 17:22:20,183: training iter 35, loss=0.31193462014198303 (6.411635s)
INFO:niftynet:2019-02-12 17:22:20,819: training iter 36, loss=0.3830711245536804 (0.631987s)
INFO:niftynet:2019-02-12 17:22:22,174: training iter 37, loss=0.2801920175552368 (1.353622s)
INFO:niftynet:2019-02-12 17:22:24,277: training iter 38, loss=0.29662826657295227 (2.103177s)
INFO:niftynet:2019-02-12 17:22:31,701: training iter 39, loss=0.3075486421585083 (7.424095s)
INFO:niftynet:2019-02-12 17:22:33,509: training iter 40, loss=0.33292803168296814 (1.807083s)
INFO:niftynet:2019-02-12 17:22:34,400:     validation iter 40, loss=0.36576324701309204 (0.504328s)
INFO:niftynet:2019-02-12 17:22:35,105: training iter 41, loss=0.35508209466934204 (0.703586s)
INFO:niftynet:2019-02-12 17:22:37,861: training iter 42, loss=0.3225780725479126 (2.754909s)
INFO:niftynet:2019-02-12 17:22:40,779: training iter 43, loss=0.2595212459564209 (2.917512s)
INFO:niftynet:2019-02-12 17:22:43,802: training iter 44, loss=0.27863383293151855 (3.022922s)
INFO:niftynet:2019-02-12 17:22:47,161: training iter 45, loss=0.2770029604434967 (3.358955s)
INFO:niftynet:2019-02-12 17:22:49,934: training iter 46, loss=0.38061755895614624 (2.770909s)
INFO:niftynet:2019-02-12 17:22:51,887: training iter 47, loss=0.24467509984970093 (1.951873s)
INFO:niftynet:2019-02-12 17:22:56,410: training iter 48, loss=0.2727954089641571 (4.522324s)
INFO:niftynet:2019-02-12 17:22:58,068: training iter 49, loss=0.2725343108177185 (1.657884s)
INFO:niftynet:2019-02-12 17:23:01,487: training iter 50, loss=0.3036022484302521 (3.417974s)
INFO:niftynet:2019-02-12 17:23:04,051: iter 50 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:23:04,647:     validation iter 50, loss=0.3531914949417114 (0.595508s)
INFO:niftynet:2019-02-12 17:23:05,313: training iter 51, loss=0.3997533917427063 (0.662459s)
INFO:niftynet:2019-02-12 17:23:07,028: training iter 52, loss=0.27806568145751953 (1.715349s)
INFO:niftynet:2019-02-12 17:23:10,338: training iter 53, loss=0.27669042348861694 (3.309652s)
INFO:niftynet:2019-02-12 17:23:14,116: training iter 54, loss=0.2565164268016815 (3.777042s)
INFO:niftynet:2019-02-12 17:23:17,566: training iter 55, loss=0.2977393567562103 (0.773104s)
INFO:niftynet:2019-02-12 17:23:18,212: training iter 56, loss=0.30124378204345703 (0.643852s)
INFO:niftynet:2019-02-12 17:23:19,321: training iter 57, loss=0.32931703329086304 (1.108977s)
INFO:niftynet:2019-02-12 17:23:24,770: training iter 58, loss=0.28080132603645325 (5.448845s)
INFO:niftynet:2019-02-12 17:23:30,988: training iter 59, loss=0.2696940302848816 (6.215576s)
INFO:niftynet:2019-02-12 17:23:31,716: training iter 60, loss=0.2575138807296753 (0.728265s)
INFO:niftynet:2019-02-12 17:23:32,331:     validation iter 60, loss=0.3024221658706665 (0.612565s)
INFO:niftynet:2019-02-12 17:23:32,984: training iter 61, loss=0.384122759103775 (0.650628s)
INFO:niftynet:2019-02-12 17:23:36,134: training iter 62, loss=0.2340766191482544 (3.149660s)
INFO:niftynet:2019-02-12 17:23:39,737: training iter 63, loss=0.3058192729949951 (3.595602s)
INFO:niftynet:2019-02-12 17:23:43,538: training iter 64, loss=0.3334415555000305 (3.798148s)
INFO:niftynet:2019-02-12 17:23:44,358: training iter 65, loss=0.31765252351760864 (0.819748s)
INFO:niftynet:2019-02-12 17:23:46,593: training iter 66, loss=0.30894935131073 (2.232141s)
INFO:niftynet:2019-02-12 17:23:50,529: training iter 67, loss=0.33462560176849365 (3.934455s)
INFO:niftynet:2019-02-12 17:23:57,103: training iter 68, loss=0.3206440806388855 (6.572893s)
INFO:niftynet:2019-02-12 17:23:57,732: training iter 69, loss=0.26882606744766235 (0.628716s)
INFO:niftynet:2019-02-12 17:23:58,404: training iter 70, loss=0.2644117474555969 (0.672020s)
INFO:niftynet:2019-02-12 17:23:58,898:     validation iter 70, loss=0.31472063064575195 (0.492640s)
INFO:niftynet:2019-02-12 17:24:00,755: training iter 71, loss=0.27377498149871826 (1.848850s)
INFO:niftynet:2019-02-12 17:24:05,512: training iter 72, loss=0.36138540506362915 (4.755570s)
INFO:niftynet:2019-02-12 17:24:09,722: training iter 73, loss=0.3290146589279175 (4.208552s)
INFO:niftynet:2019-02-12 17:24:11,317: training iter 74, loss=0.30368801951408386 (1.594466s)
INFO:niftynet:2019-02-12 17:24:12,005: training iter 75, loss=0.3206026256084442 (0.687340s)
INFO:niftynet:2019-02-12 17:24:13,361: iter 75 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:24:15,374: training iter 76, loss=0.36343568563461304 (2.012333s)
INFO:niftynet:2019-02-12 17:24:21,247: training iter 77, loss=0.28799688816070557 (5.870750s)
INFO:niftynet:2019-02-12 17:24:22,056: training iter 78, loss=0.31275421380996704 (0.807909s)
INFO:niftynet:2019-02-12 17:24:24,789: training iter 79, loss=0.30485594272613525 (2.730818s)
INFO:niftynet:2019-02-12 17:24:26,052: training iter 80, loss=0.2877168357372284 (1.262507s)
INFO:niftynet:2019-02-12 17:24:26,667:     validation iter 80, loss=0.29450997710227966 (0.613278s)
INFO:niftynet:2019-02-12 17:24:33,241: training iter 81, loss=0.3592303693294525 (6.573518s)
INFO:niftynet:2019-02-12 17:24:33,879: training iter 82, loss=0.3524845838546753 (0.637242s)
INFO:niftynet:2019-02-12 17:24:37,388: training iter 83, loss=0.2888721227645874 (3.508620s)
INFO:niftynet:2019-02-12 17:24:38,180: training iter 84, loss=0.2815554141998291 (0.789241s)
INFO:niftynet:2019-02-12 17:24:40,668: training iter 85, loss=0.2654011845588684 (2.487258s)
INFO:niftynet:2019-02-12 17:24:44,210: training iter 86, loss=0.26945143938064575 (3.540945s)
INFO:niftynet:2019-02-12 17:24:48,720: training iter 87, loss=0.2651538848876953 (4.509541s)
INFO:niftynet:2019-02-12 17:24:49,534: training iter 88, loss=0.2787444591522217 (0.813702s)
INFO:niftynet:2019-02-12 17:24:50,177: training iter 89, loss=0.34026792645454407 (0.641846s)
INFO:niftynet:2019-02-12 17:24:54,412: training iter 90, loss=0.3314754366874695 (4.233972s)
INFO:niftynet:2019-02-12 17:25:00,583:     validation iter 90, loss=0.3074690103530884 (6.169524s)
INFO:niftynet:2019-02-12 17:25:01,259: training iter 91, loss=0.29370811581611633 (0.674301s)
INFO:niftynet:2019-02-12 17:25:01,876: training iter 92, loss=0.2813515067100525 (0.617098s)
INFO:niftynet:2019-02-12 17:25:03,038: training iter 93, loss=0.3327471613883972 (1.161201s)
INFO:niftynet:2019-02-12 17:25:08,065: training iter 94, loss=0.2934916019439697 (5.023816s)
INFO:niftynet:2019-02-12 17:25:09,592: training iter 95, loss=0.34877631068229675 (1.526284s)
INFO:niftynet:2019-02-12 17:25:11,484: training iter 96, loss=0.3512362241744995 (1.890670s)
INFO:niftynet:2019-02-12 17:25:25,271: training iter 97, loss=0.3094562888145447 (13.786691s)
INFO:niftynet:2019-02-12 17:25:25,953: training iter 98, loss=0.27930107712745667 (0.681996s)
INFO:niftynet:2019-02-12 17:25:26,544: training iter 99, loss=0.3518117070198059 (0.590364s)
INFO:niftynet:2019-02-12 17:25:27,215: training iter 100, loss=0.3029983639717102 (0.670196s)
INFO:niftynet:2019-02-12 17:25:28,543: iter 100 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:25:29,164:     validation iter 100, loss=0.28229644894599915 (0.620505s)
INFO:niftynet:2019-02-12 17:25:31,695: training iter 101, loss=0.3077097535133362 (2.528333s)
INFO:niftynet:2019-02-12 17:25:39,095: training iter 102, loss=0.25415393710136414 (7.399240s)
INFO:niftynet:2019-02-12 17:25:39,906: training iter 103, loss=0.303225576877594 (0.809143s)
INFO:niftynet:2019-02-12 17:25:40,486: training iter 104, loss=0.2778303027153015 (0.579900s)
INFO:niftynet:2019-02-12 17:25:41,238: training iter 105, loss=0.26593929529190063 (0.751032s)
INFO:niftynet:2019-02-12 17:25:48,313: training iter 106, loss=0.26348498463630676 (7.073937s)
INFO:niftynet:2019-02-12 17:25:52,077: training iter 107, loss=0.26070189476013184 (3.762980s)
INFO:niftynet:2019-02-12 17:25:52,684: training iter 108, loss=0.24898552894592285 (0.606744s)
INFO:niftynet:2019-02-12 17:25:53,272: training iter 109, loss=0.3358217179775238 (0.587900s)
INFO:niftynet:2019-02-12 17:26:00,088: training iter 110, loss=0.3149029016494751 (6.815276s)
INFO:niftynet:2019-02-12 17:26:00,655:     validation iter 110, loss=0.35912179946899414 (0.565171s)
INFO:niftynet:2019-02-12 17:26:04,988: training iter 111, loss=0.2682461738586426 (4.331435s)
INFO:niftynet:2019-02-12 17:26:05,642: training iter 112, loss=0.46310603618621826 (0.653360s)
INFO:niftynet:2019-02-12 17:26:06,344: training iter 113, loss=0.2923600971698761 (0.701544s)
INFO:niftynet:2019-02-12 17:26:11,637: training iter 114, loss=0.30477428436279297 (5.292346s)
INFO:niftynet:2019-02-12 17:26:12,969: training iter 115, loss=0.3278706669807434 (1.331297s)
INFO:niftynet:2019-02-12 17:26:17,047: training iter 116, loss=0.34818369150161743 (4.076530s)
INFO:niftynet:2019-02-12 17:26:17,820: training iter 117, loss=0.284286767244339 (0.772449s)
INFO:niftynet:2019-02-12 17:26:22,893: training iter 118, loss=0.2883073091506958 (5.073014s)
INFO:niftynet:2019-02-12 17:26:24,049: training iter 119, loss=0.2948927879333496 (1.154580s)
INFO:niftynet:2019-02-12 17:26:29,315: training iter 120, loss=0.2927197515964508 (5.261792s)
INFO:niftynet:2019-02-12 17:26:29,892:     validation iter 120, loss=0.23784399032592773 (0.576183s)
INFO:niftynet:2019-02-12 17:26:30,544: training iter 121, loss=0.2721811830997467 (0.649059s)
INFO:niftynet:2019-02-12 17:26:32,477: training iter 122, loss=0.36119142174720764 (1.933213s)
INFO:niftynet:2019-02-12 17:26:35,699: training iter 123, loss=0.32624131441116333 (3.221117s)
INFO:niftynet:2019-02-12 17:26:37,496: training iter 124, loss=0.2528926432132721 (1.770119s)
INFO:niftynet:2019-02-12 17:26:41,111: training iter 125, loss=0.3356531262397766 (3.614348s)
INFO:niftynet:2019-02-12 17:26:42,910: iter 125 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:26:43,528: training iter 126, loss=0.4669750928878784 (0.616190s)
INFO:niftynet:2019-02-12 17:26:45,985: training iter 127, loss=0.33246204257011414 (2.456579s)
INFO:niftynet:2019-02-12 17:26:48,574: training iter 128, loss=0.2904202938079834 (2.588820s)
INFO:niftynet:2019-02-12 17:26:51,434: training iter 129, loss=0.3287190794944763 (2.851057s)
INFO:niftynet:2019-02-12 17:26:55,563: training iter 130, loss=0.3092272877693176 (4.127660s)
INFO:niftynet:2019-02-12 17:26:56,117:     validation iter 130, loss=0.33345818519592285 (0.552154s)
INFO:niftynet:2019-02-12 17:26:56,745: training iter 131, loss=0.2750909924507141 (0.626705s)
INFO:niftynet:2019-02-12 17:27:00,816: training iter 132, loss=0.3455120623111725 (4.070899s)
INFO:niftynet:2019-02-12 17:27:03,951: training iter 133, loss=0.29454702138900757 (3.134630s)
INFO:niftynet:2019-02-12 17:27:04,641: training iter 134, loss=0.4810197353363037 (0.686588s)
INFO:niftynet:2019-02-12 17:27:08,634: training iter 135, loss=0.27222323417663574 (3.990918s)
INFO:niftynet:2019-02-12 17:27:13,071: training iter 136, loss=0.3237646818161011 (4.434701s)
INFO:niftynet:2019-02-12 17:27:13,805: training iter 137, loss=0.3205135464668274 (0.733459s)
INFO:niftynet:2019-02-12 17:27:16,611: training iter 138, loss=0.3169522285461426 (2.805757s)
INFO:niftynet:2019-02-12 17:27:19,013: training iter 139, loss=0.24227690696716309 (2.401307s)
INFO:niftynet:2019-02-12 17:27:21,406: training iter 140, loss=0.3037217855453491 (2.391547s)
INFO:niftynet:2019-02-12 17:27:22,027:     validation iter 140, loss=0.28830820322036743 (0.619179s)
INFO:niftynet:2019-02-12 17:27:24,393: training iter 141, loss=0.3505280315876007 (2.359589s)
INFO:niftynet:2019-02-12 17:27:26,300: training iter 142, loss=0.3287981152534485 (1.905240s)
INFO:niftynet:2019-02-12 17:27:31,159: training iter 143, loss=0.29403921961784363 (4.853553s)
INFO:niftynet:2019-02-12 17:27:33,294: training iter 144, loss=0.28242430090904236 (2.134706s)
INFO:niftynet:2019-02-12 17:27:34,130: training iter 145, loss=0.2752825617790222 (0.835700s)
INFO:niftynet:2019-02-12 17:27:37,650: training iter 146, loss=0.2931143343448639 (3.517109s)
INFO:niftynet:2019-02-12 17:27:43,162: training iter 147, loss=0.33392226696014404 (5.511426s)
INFO:niftynet:2019-02-12 17:27:44,881: training iter 148, loss=0.3690114915370941 (1.717706s)
INFO:niftynet:2019-02-12 17:27:45,967: training iter 149, loss=0.2966609597206116 (1.086027s)
INFO:niftynet:2019-02-12 17:27:51,400: training iter 150, loss=0.32030802965164185 (5.432493s)
INFO:niftynet:2019-02-12 17:27:53,131: iter 150 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:27:53,690:     validation iter 150, loss=0.2826216220855713 (0.556380s)
INFO:niftynet:2019-02-12 17:27:55,283: training iter 151, loss=0.31576377153396606 (1.591973s)
INFO:niftynet:2019-02-12 17:27:57,343: training iter 152, loss=0.3235092759132385 (2.059609s)
INFO:niftynet:2019-02-12 17:27:57,933: training iter 153, loss=0.33263394236564636 (0.589174s)
INFO:niftynet:2019-02-12 17:28:01,563: training iter 154, loss=0.32351022958755493 (3.628863s)
INFO:niftynet:2019-02-12 17:28:07,279: training iter 155, loss=0.33822402358055115 (5.715774s)
INFO:niftynet:2019-02-12 17:28:11,273: training iter 156, loss=0.3336503505706787 (3.990076s)
INFO:niftynet:2019-02-12 17:28:12,783: training iter 157, loss=0.2737741470336914 (1.509550s)
INFO:niftynet:2019-02-12 17:28:13,368: training iter 158, loss=0.2978423833847046 (0.584702s)
INFO:niftynet:2019-02-12 17:28:16,376: training iter 159, loss=0.3198867440223694 (3.006920s)
INFO:niftynet:2019-02-12 17:28:17,141: training iter 160, loss=0.2633868455886841 (0.763068s)
INFO:niftynet:2019-02-12 17:28:17,803:     validation iter 160, loss=0.3499406576156616 (0.642271s)
INFO:niftynet:2019-02-12 17:28:26,269: training iter 161, loss=0.2525441646575928 (8.464727s)
INFO:niftynet:2019-02-12 17:28:26,894: training iter 162, loss=0.28776711225509644 (0.624801s)
INFO:niftynet:2019-02-12 17:28:29,012: training iter 163, loss=0.2340102195739746 (2.115451s)
INFO:niftynet:2019-02-12 17:28:29,667: training iter 164, loss=0.2884237468242645 (0.653649s)
INFO:niftynet:2019-02-12 17:28:33,756: training iter 165, loss=0.29191991686820984 (4.086950s)
INFO:niftynet:2019-02-12 17:28:35,817: training iter 166, loss=0.338106632232666 (2.058744s)
INFO:niftynet:2019-02-12 17:28:41,897: training iter 167, loss=0.25113385915756226 (6.079158s)
INFO:niftynet:2019-02-12 17:28:42,509: training iter 168, loss=0.24106453359127045 (0.612160s)
INFO:niftynet:2019-02-12 17:28:44,877: training iter 169, loss=0.25422948598861694 (2.366977s)
INFO:niftynet:2019-02-12 17:28:49,223: training iter 170, loss=0.35907867550849915 (4.344548s)
INFO:niftynet:2019-02-12 17:28:49,751:     validation iter 170, loss=0.36154454946517944 (0.527117s)
INFO:niftynet:2019-02-12 17:28:55,094: training iter 171, loss=0.24714377522468567 (5.340777s)
INFO:niftynet:2019-02-12 17:28:55,811: training iter 172, loss=0.5343376398086548 (0.716604s)
INFO:niftynet:2019-02-12 17:28:56,382: training iter 173, loss=0.2907743752002716 (0.570840s)
INFO:niftynet:2019-02-12 17:29:01,274: training iter 174, loss=0.28270402550697327 (4.891302s)
INFO:niftynet:2019-02-12 17:29:02,025: training iter 175, loss=0.27919071912765503 (0.750544s)
INFO:niftynet:2019-02-12 17:29:03,505: iter 175 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:29:06,243: training iter 176, loss=0.3115496039390564 (2.736065s)
INFO:niftynet:2019-02-12 17:29:06,900: training iter 177, loss=0.30000758171081543 (0.656187s)
INFO:niftynet:2019-02-12 17:29:14,819: training iter 178, loss=0.3181988596916199 (7.918286s)
INFO:niftynet:2019-02-12 17:29:15,450: training iter 179, loss=0.30339211225509644 (0.631252s)
INFO:niftynet:2019-02-12 17:29:18,903: training iter 180, loss=0.31493526697158813 (3.452203s)
INFO:niftynet:2019-02-12 17:29:19,441:     validation iter 180, loss=0.263400137424469 (0.535912s)
INFO:niftynet:2019-02-12 17:29:20,105: training iter 181, loss=0.32177144289016724 (0.657744s)
INFO:niftynet:2019-02-12 17:29:24,882: training iter 182, loss=0.32165566086769104 (4.776420s)
INFO:niftynet:2019-02-12 17:29:27,750: training iter 183, loss=0.2802777886390686 (2.154085s)
INFO:niftynet:2019-02-12 17:29:30,891: training iter 184, loss=0.2635476589202881 (3.140265s)
INFO:niftynet:2019-02-12 17:29:33,358: training iter 185, loss=0.31719136238098145 (2.465721s)
INFO:niftynet:2019-02-12 17:29:36,026: training iter 186, loss=0.27277183532714844 (2.666405s)
INFO:niftynet:2019-02-12 17:29:38,212: training iter 187, loss=0.28465735912323 (2.185107s)
INFO:niftynet:2019-02-12 17:29:43,132: training iter 188, loss=0.4171541929244995 (4.919915s)
INFO:niftynet:2019-02-12 17:29:43,766: training iter 189, loss=0.31418538093566895 (0.634275s)
INFO:niftynet:2019-02-12 17:29:47,663: training iter 190, loss=0.29664015769958496 (3.894881s)
INFO:niftynet:2019-02-12 17:29:48,328:     validation iter 190, loss=0.23547370731830597 (0.662445s)
INFO:niftynet:2019-02-12 17:29:49,940: training iter 191, loss=0.2955816388130188 (1.610819s)
INFO:niftynet:2019-02-12 17:29:51,899: training iter 192, loss=0.313052237033844 (1.958033s)
INFO:niftynet:2019-02-12 17:29:57,127: training iter 193, loss=0.28161194920539856 (5.227517s)
INFO:niftynet:2019-02-12 17:29:57,714: training iter 194, loss=0.3236520290374756 (0.587221s)
INFO:niftynet:2019-02-12 17:29:59,023: training iter 195, loss=0.30757367610931396 (1.307794s)
INFO:niftynet:2019-02-12 17:30:01,814: training iter 196, loss=0.3341904580593109 (2.789674s)
INFO:niftynet:2019-02-12 17:30:07,038: training iter 197, loss=0.296597957611084 (5.223850s)
INFO:niftynet:2019-02-12 17:30:10,587: training iter 198, loss=0.31280604004859924 (3.548384s)
INFO:niftynet:2019-02-12 17:30:11,235: training iter 199, loss=0.2757887840270996 (0.646080s)
INFO:niftynet:2019-02-12 17:30:12,080: training iter 200, loss=0.24815446138381958 (0.837728s)
INFO:niftynet:2019-02-12 17:30:13,617: iter 200 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:30:14,226:     validation iter 200, loss=0.29654526710510254 (0.609003s)
INFO:niftynet:2019-02-12 17:30:19,037: training iter 201, loss=0.2828938364982605 (4.809066s)
INFO:niftynet:2019-02-12 17:30:20,708: training iter 202, loss=0.3134862184524536 (1.670427s)
INFO:niftynet:2019-02-12 17:30:23,627: training iter 203, loss=0.2571724057197571 (2.918385s)
INFO:niftynet:2019-02-12 17:30:24,623: training iter 204, loss=0.25908732414245605 (0.996132s)
INFO:niftynet:2019-02-12 17:30:31,239: training iter 205, loss=0.24534738063812256 (6.615597s)
INFO:niftynet:2019-02-12 17:30:32,435: training iter 206, loss=0.3075414001941681 (1.193571s)
INFO:niftynet:2019-02-12 17:30:33,841: training iter 207, loss=0.279157817363739 (1.404984s)
INFO:niftynet:2019-02-12 17:30:34,486: training iter 208, loss=0.24558624625205994 (0.645210s)
INFO:niftynet:2019-02-12 17:30:36,082: training iter 209, loss=0.3349125385284424 (1.595112s)
INFO:niftynet:2019-02-12 17:30:50,709: training iter 210, loss=0.328880250453949 (14.626971s)
INFO:niftynet:2019-02-12 17:30:51,230:     validation iter 210, loss=0.3178326487541199 (0.518481s)
INFO:niftynet:2019-02-12 17:30:51,831: training iter 211, loss=0.2654834985733032 (0.599957s)
INFO:niftynet:2019-02-12 17:30:52,403: training iter 212, loss=0.2808575928211212 (0.571099s)
INFO:niftynet:2019-02-12 17:30:53,002: training iter 213, loss=0.24948053061962128 (0.598851s)
INFO:niftynet:2019-02-12 17:30:57,002: training iter 214, loss=0.38438400626182556 (3.999365s)
INFO:niftynet:2019-02-12 17:31:03,729: training iter 215, loss=0.2753579616546631 (6.725744s)
INFO:niftynet:2019-02-12 17:31:04,306: training iter 216, loss=0.2852902114391327 (0.575549s)
INFO:niftynet:2019-02-12 17:31:04,859: training iter 217, loss=0.25659388303756714 (0.551238s)
INFO:niftynet:2019-02-12 17:31:06,686: training iter 218, loss=0.29333147406578064 (1.820978s)
INFO:niftynet:2019-02-12 17:31:14,158: training iter 219, loss=0.23162566125392914 (7.471095s)
INFO:niftynet:2019-02-12 17:31:15,929: training iter 220, loss=0.2699962258338928 (1.770314s)
INFO:niftynet:2019-02-12 17:31:16,568:     validation iter 220, loss=0.2625524401664734 (0.636665s)
INFO:niftynet:2019-02-12 17:31:17,182: training iter 221, loss=0.3649917244911194 (0.611159s)
INFO:niftynet:2019-02-12 17:31:17,727: training iter 222, loss=0.23979049921035767 (0.544647s)
INFO:niftynet:2019-02-12 17:31:26,929: training iter 223, loss=0.2910267412662506 (9.201625s)
INFO:niftynet:2019-02-12 17:31:27,796: training iter 224, loss=0.34716832637786865 (0.865960s)
INFO:niftynet:2019-02-12 17:31:28,615: training iter 225, loss=0.28054341673851013 (0.818475s)
INFO:niftynet:2019-02-12 17:31:29,885: iter 225 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:31:30,437: training iter 226, loss=0.30357468128204346 (0.551542s)
INFO:niftynet:2019-02-12 17:31:35,256: training iter 227, loss=0.2784050703048706 (4.818802s)
INFO:niftynet:2019-02-12 17:31:41,228: training iter 228, loss=0.312819242477417 (5.972234s)
INFO:niftynet:2019-02-12 17:31:41,900: training iter 229, loss=0.3208094537258148 (0.671074s)
INFO:niftynet:2019-02-12 17:31:42,643: training iter 230, loss=0.30937084555625916 (0.742702s)
INFO:niftynet:2019-02-12 17:31:48,308:     validation iter 230, loss=0.29629337787628174 (5.663087s)
INFO:niftynet:2019-02-12 17:31:49,456: training iter 231, loss=0.3075200319290161 (1.105619s)
INFO:niftynet:2019-02-12 17:31:51,658: training iter 232, loss=0.2948060631752014 (2.201463s)
INFO:niftynet:2019-02-12 17:31:55,362: training iter 233, loss=0.2954395115375519 (3.702582s)
INFO:niftynet:2019-02-12 17:31:56,058: training iter 234, loss=0.2638140320777893 (0.695168s)
INFO:niftynet:2019-02-12 17:32:01,214: training iter 235, loss=0.28445082902908325 (5.155776s)
INFO:niftynet:2019-02-12 17:32:03,393: training iter 236, loss=0.3148246705532074 (2.177071s)
INFO:niftynet:2019-02-12 17:32:04,531: training iter 237, loss=0.2756873071193695 (1.137757s)
INFO:niftynet:2019-02-12 17:32:05,743: training iter 238, loss=0.2342195212841034 (1.211296s)
INFO:niftynet:2019-02-12 17:32:11,737: training iter 239, loss=0.3174706697463989 (5.989993s)
INFO:niftynet:2019-02-12 17:32:16,508: training iter 240, loss=0.2698594033718109 (4.769350s)
INFO:niftynet:2019-02-12 17:32:17,019:     validation iter 240, loss=0.27701157331466675 (0.510133s)
INFO:niftynet:2019-02-12 17:32:17,633: training iter 241, loss=0.32492774724960327 (0.611669s)
INFO:niftynet:2019-02-12 17:32:18,272: training iter 242, loss=0.2721794545650482 (0.638525s)
INFO:niftynet:2019-02-12 17:32:22,588: training iter 243, loss=0.2939257025718689 (4.315091s)
INFO:niftynet:2019-02-12 17:32:25,150: training iter 244, loss=0.31753969192504883 (2.559514s)
INFO:niftynet:2019-02-12 17:32:30,177: training iter 245, loss=0.2860502600669861 (5.026425s)
INFO:niftynet:2019-02-12 17:32:30,826: training iter 246, loss=0.33320167660713196 (0.647861s)
INFO:niftynet:2019-02-12 17:32:32,866: training iter 247, loss=0.2937195897102356 (2.039885s)
INFO:niftynet:2019-02-12 17:32:36,779: training iter 248, loss=0.2395504117012024 (3.912033s)
INFO:niftynet:2019-02-12 17:32:39,707: training iter 249, loss=0.343652606010437 (2.927244s)
INFO:niftynet:2019-02-12 17:32:43,132: training iter 250, loss=0.4121816158294678 (3.424124s)
INFO:niftynet:2019-02-12 17:32:44,823: iter 250 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:32:45,342:     validation iter 250, loss=0.2636009454727173 (0.515153s)
INFO:niftynet:2019-02-12 17:32:45,892: training iter 251, loss=0.2919086515903473 (0.548651s)
INFO:niftynet:2019-02-12 17:32:46,427: training iter 252, loss=0.2693289518356323 (0.534561s)
INFO:niftynet:2019-02-12 17:32:50,977: training iter 253, loss=0.25973644852638245 (4.550004s)
INFO:niftynet:2019-02-12 17:32:57,173: training iter 254, loss=0.27726709842681885 (6.195430s)
INFO:niftynet:2019-02-12 17:32:57,881: training iter 255, loss=0.28628039360046387 (0.707672s)
INFO:niftynet:2019-02-12 17:33:00,626: training iter 256, loss=0.27398058772087097 (2.744141s)
INFO:niftynet:2019-02-12 17:33:02,113: training iter 257, loss=0.2887722849845886 (1.485920s)
INFO:niftynet:2019-02-12 17:33:05,450: training iter 258, loss=0.33668166399002075 (3.310005s)
INFO:niftynet:2019-02-12 17:33:09,667: training iter 259, loss=0.29576992988586426 (4.216764s)
INFO:niftynet:2019-02-12 17:33:13,379: training iter 260, loss=0.2966309189796448 (3.711128s)
INFO:niftynet:2019-02-12 17:33:13,963:     validation iter 260, loss=0.26757383346557617 (0.580789s)
INFO:niftynet:2019-02-12 17:33:14,519: training iter 261, loss=0.2901218831539154 (0.554295s)
INFO:niftynet:2019-02-12 17:33:15,110: training iter 262, loss=0.3503631353378296 (0.589864s)
INFO:niftynet:2019-02-12 17:33:22,590: training iter 263, loss=0.3107805550098419 (7.479033s)
INFO:niftynet:2019-02-12 17:33:23,178: training iter 264, loss=0.2896908223628998 (0.587098s)
INFO:niftynet:2019-02-12 17:33:26,446: training iter 265, loss=0.43381309509277344 (3.267548s)
INFO:niftynet:2019-02-12 17:33:27,115: training iter 266, loss=0.27694419026374817 (0.667938s)
INFO:niftynet:2019-02-12 17:33:29,031: training iter 267, loss=0.27345287799835205 (1.915325s)
INFO:niftynet:2019-02-12 17:33:34,675: training iter 268, loss=0.26557421684265137 (5.642773s)
INFO:niftynet:2019-02-12 17:33:36,015: training iter 269, loss=0.28916293382644653 (1.340350s)
INFO:niftynet:2019-02-12 17:33:38,593: training iter 270, loss=0.2695804238319397 (2.577206s)
INFO:niftynet:2019-02-12 17:33:39,204:     validation iter 270, loss=0.3501787781715393 (0.609610s)
INFO:niftynet:2019-02-12 17:33:40,094: training iter 271, loss=0.2889491319656372 (0.888264s)
INFO:niftynet:2019-02-12 17:33:44,734: training iter 272, loss=0.3318289518356323 (4.639086s)
INFO:niftynet:2019-02-12 17:33:46,916: training iter 273, loss=0.3160104751586914 (2.181976s)
INFO:niftynet:2019-02-12 17:33:50,963: training iter 274, loss=0.2803265452384949 (4.044935s)
INFO:niftynet:2019-02-12 17:33:53,404: training iter 275, loss=0.3139496445655823 (2.439567s)
INFO:niftynet:2019-02-12 17:33:55,245: iter 275 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:33:57,645: training iter 276, loss=0.25868526101112366 (2.398668s)
INFO:niftynet:2019-02-12 17:33:58,287: training iter 277, loss=0.29583612084388733 (0.641535s)
INFO:niftynet:2019-02-12 17:34:00,574: training iter 278, loss=0.30093419551849365 (2.285012s)
INFO:niftynet:2019-02-12 17:34:05,168: training iter 279, loss=0.2896641492843628 (4.593646s)
INFO:niftynet:2019-02-12 17:34:09,741: training iter 280, loss=0.2602882385253906 (4.571719s)
INFO:niftynet:2019-02-12 17:34:10,325:     validation iter 280, loss=0.2585967779159546 (0.580014s)
INFO:niftynet:2019-02-12 17:34:10,990: training iter 281, loss=0.27690428495407104 (0.661822s)
INFO:niftynet:2019-02-12 17:34:11,608: training iter 282, loss=0.2696729898452759 (0.618061s)
INFO:niftynet:2019-02-12 17:34:16,340: training iter 283, loss=0.3118085563182831 (4.730417s)
INFO:niftynet:2019-02-12 17:34:18,082: training iter 284, loss=0.2881748676300049 (1.741714s)
INFO:niftynet:2019-02-12 17:34:20,779: training iter 285, loss=0.27504631876945496 (2.696767s)
INFO:niftynet:2019-02-12 17:34:24,051: training iter 286, loss=0.24376964569091797 (3.270701s)
INFO:niftynet:2019-02-12 17:34:25,790: training iter 287, loss=0.3219042420387268 (1.736608s)
INFO:niftynet:2019-02-12 17:34:29,028: training iter 288, loss=0.2947605848312378 (3.236387s)
INFO:niftynet:2019-02-12 17:34:30,211: training iter 289, loss=0.25450581312179565 (1.182333s)
INFO:niftynet:2019-02-12 17:34:35,400: training iter 290, loss=0.30384373664855957 (5.188499s)
INFO:niftynet:2019-02-12 17:34:35,905:     validation iter 290, loss=0.24392743408679962 (0.503388s)
INFO:niftynet:2019-02-12 17:34:36,942: training iter 291, loss=0.26584333181381226 (1.035457s)
INFO:niftynet:2019-02-12 17:34:41,024: training iter 292, loss=0.25036436319351196 (4.079542s)
INFO:niftynet:2019-02-12 17:34:42,283: training iter 293, loss=0.28805166482925415 (1.256485s)
INFO:niftynet:2019-02-12 17:34:48,321: training iter 294, loss=0.2607760429382324 (6.037411s)
INFO:niftynet:2019-02-12 17:34:49,756: training iter 295, loss=0.28996285796165466 (1.434468s)
INFO:niftynet:2019-02-12 17:34:51,594: training iter 296, loss=0.35058295726776123 (1.836040s)
INFO:niftynet:2019-02-12 17:34:52,544: training iter 297, loss=0.2911040782928467 (0.949344s)
INFO:niftynet:2019-02-12 17:34:57,784: training iter 298, loss=0.37430620193481445 (5.239880s)
INFO:niftynet:2019-02-12 17:34:59,992: training iter 299, loss=0.2584075331687927 (2.207561s)
INFO:niftynet:2019-02-12 17:35:01,713: training iter 300, loss=0.2957726716995239 (1.720471s)
INFO:niftynet:2019-02-12 17:35:03,744: iter 300 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:35:04,373:     validation iter 300, loss=0.25173187255859375 (0.628274s)
INFO:niftynet:2019-02-12 17:35:05,912: training iter 301, loss=0.2951100170612335 (1.534188s)
INFO:niftynet:2019-02-12 17:35:09,270: training iter 302, loss=0.29934871196746826 (3.358071s)
INFO:niftynet:2019-02-12 17:35:12,894: training iter 303, loss=0.27397969365119934 (3.623595s)
INFO:niftynet:2019-02-12 17:35:16,297: training iter 304, loss=0.2864895462989807 (3.399737s)
INFO:niftynet:2019-02-12 17:35:17,281: training iter 305, loss=0.2681519389152527 (0.819352s)
INFO:niftynet:2019-02-12 17:35:21,321: training iter 306, loss=0.28019434213638306 (4.038230s)
INFO:niftynet:2019-02-12 17:35:23,875: training iter 307, loss=0.3058188855648041 (2.553741s)
INFO:niftynet:2019-02-12 17:35:25,466: training iter 308, loss=0.2740732729434967 (1.590667s)
INFO:niftynet:2019-02-12 17:35:29,324: training iter 309, loss=0.294560968875885 (3.856979s)
INFO:niftynet:2019-02-12 17:35:30,769: training iter 310, loss=0.29683083295822144 (1.438886s)
INFO:niftynet:2019-02-12 17:35:31,459:     validation iter 310, loss=0.2900606393814087 (0.679481s)
INFO:niftynet:2019-02-12 17:35:36,340: training iter 311, loss=0.29353660345077515 (4.879259s)
INFO:niftynet:2019-02-12 17:35:39,421: training iter 312, loss=0.26496946811676025 (3.080358s)
INFO:niftynet:2019-02-12 17:35:42,962: training iter 313, loss=0.273853063583374 (3.538407s)
INFO:niftynet:2019-02-12 17:35:43,672: training iter 314, loss=0.24000896513462067 (0.708223s)
INFO:niftynet:2019-02-12 17:35:47,580: training iter 315, loss=0.3179614543914795 (3.906697s)
INFO:niftynet:2019-02-12 17:35:52,324: training iter 316, loss=0.26158636808395386 (4.739589s)
INFO:niftynet:2019-02-12 17:35:53,491: training iter 317, loss=0.25072044134140015 (0.642888s)
INFO:niftynet:2019-02-12 17:35:54,107: training iter 318, loss=0.268276572227478 (0.615759s)
INFO:niftynet:2019-02-12 17:35:58,232: training iter 319, loss=0.48259925842285156 (2.229207s)
INFO:niftynet:2019-02-12 17:36:00,741: training iter 320, loss=0.256508469581604 (2.508097s)
INFO:niftynet:2019-02-12 17:36:01,291:     validation iter 320, loss=0.26279664039611816 (0.547872s)
INFO:niftynet:2019-02-12 17:36:04,631: training iter 321, loss=0.3226920962333679 (3.338966s)
INFO:niftynet:2019-02-12 17:36:05,175: training iter 322, loss=0.26149851083755493 (0.543123s)
INFO:niftynet:2019-02-12 17:36:07,304: training iter 323, loss=0.2820720076560974 (2.128518s)
INFO:niftynet:2019-02-12 17:36:19,340: training iter 324, loss=0.2824490964412689 (12.035789s)
INFO:niftynet:2019-02-12 17:36:20,036: training iter 325, loss=0.27604109048843384 (0.696369s)
INFO:niftynet:2019-02-12 17:36:21,480: iter 325 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:36:22,043: training iter 326, loss=0.2967228293418884 (0.562478s)
INFO:niftynet:2019-02-12 17:36:22,602: training iter 327, loss=0.3107231855392456 (0.558152s)
INFO:niftynet:2019-02-12 17:36:31,490: training iter 328, loss=0.27359703183174133 (8.886786s)
INFO:niftynet:2019-02-12 17:36:33,302: training iter 329, loss=0.31688183546066284 (1.808923s)
INFO:niftynet:2019-02-12 17:36:34,041: training iter 330, loss=0.28508779406547546 (0.737965s)
INFO:niftynet:2019-02-12 17:36:34,604:     validation iter 330, loss=0.29419001936912537 (0.561212s)
INFO:niftynet:2019-02-12 17:36:35,273: training iter 331, loss=0.28882360458374023 (0.666668s)
INFO:niftynet:2019-02-12 17:36:39,649: training iter 332, loss=0.26118049025535583 (4.375162s)
INFO:niftynet:2019-02-12 17:36:45,801: training iter 333, loss=0.21704843640327454 (6.152009s)
INFO:niftynet:2019-02-12 17:36:47,968: training iter 334, loss=0.26851266622543335 (2.166171s)
INFO:niftynet:2019-02-12 17:36:48,765: training iter 335, loss=0.2611519694328308 (0.797261s)
INFO:niftynet:2019-02-12 17:36:49,594: training iter 336, loss=0.37513571977615356 (0.826519s)
INFO:niftynet:2019-02-12 17:36:56,691: training iter 337, loss=0.25556695461273193 (7.095999s)
INFO:niftynet:2019-02-12 17:36:58,399: training iter 338, loss=0.22939500212669373 (1.706726s)
INFO:niftynet:2019-02-12 17:37:00,402: training iter 339, loss=0.2769811153411865 (2.003181s)
INFO:niftynet:2019-02-12 17:37:01,251: training iter 340, loss=0.2508259117603302 (0.847374s)
INFO:niftynet:2019-02-12 17:37:01,830:     validation iter 340, loss=0.2982267439365387 (0.576093s)
INFO:niftynet:2019-02-12 17:37:07,058: training iter 341, loss=0.2378307282924652 (5.226995s)
INFO:niftynet:2019-02-12 17:37:10,685: training iter 342, loss=0.24627314507961273 (3.626496s)
INFO:niftynet:2019-02-12 17:37:11,943: training iter 343, loss=0.32228976488113403 (1.255938s)
INFO:niftynet:2019-02-12 17:37:12,495: training iter 344, loss=0.28202033042907715 (0.551284s)
INFO:niftynet:2019-02-12 17:37:18,376: training iter 345, loss=0.2794579565525055 (5.880831s)
INFO:niftynet:2019-02-12 17:37:22,699: training iter 346, loss=0.3520037531852722 (4.321799s)
INFO:niftynet:2019-02-12 17:37:24,492: training iter 347, loss=0.2957204282283783 (1.791498s)
INFO:niftynet:2019-02-12 17:37:25,205: training iter 348, loss=0.2862328886985779 (0.712993s)
INFO:niftynet:2019-02-12 17:37:28,605: training iter 349, loss=0.30422836542129517 (3.399378s)
INFO:niftynet:2019-02-12 17:37:33,255: training iter 350, loss=0.2850648760795593 (4.648774s)
INFO:niftynet:2019-02-12 17:37:35,024: iter 350 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:37:35,595:     validation iter 350, loss=0.29590165615081787 (0.568258s)
INFO:niftynet:2019-02-12 17:37:38,115: training iter 351, loss=0.304884135723114 (2.517872s)
INFO:niftynet:2019-02-12 17:37:38,775: training iter 352, loss=0.2833622694015503 (0.660034s)
INFO:niftynet:2019-02-12 17:37:39,964: training iter 353, loss=0.31007909774780273 (1.188763s)
INFO:niftynet:2019-02-12 17:37:40,582: training iter 354, loss=0.31324341893196106 (0.617139s)
INFO:niftynet:2019-02-12 17:37:48,969: training iter 355, loss=0.28384995460510254 (8.386770s)
INFO:niftynet:2019-02-12 17:37:51,938: training iter 356, loss=0.26342734694480896 (2.967526s)
INFO:niftynet:2019-02-12 17:37:52,573: training iter 357, loss=0.253398060798645 (0.633361s)
INFO:niftynet:2019-02-12 17:37:53,261: training iter 358, loss=0.2639002799987793 (0.688560s)
INFO:niftynet:2019-02-12 17:37:59,690: training iter 359, loss=0.3129377067089081 (6.428228s)
INFO:niftynet:2019-02-12 17:38:03,534: training iter 360, loss=0.24288469552993774 (3.842150s)
INFO:niftynet:2019-02-12 17:38:04,102:     validation iter 360, loss=0.22291691601276398 (0.567225s)
INFO:niftynet:2019-02-12 17:38:04,748: training iter 361, loss=0.26189282536506653 (0.643967s)
INFO:niftynet:2019-02-12 17:38:05,329: training iter 362, loss=0.24966827034950256 (0.580638s)
INFO:niftynet:2019-02-12 17:38:08,084: training iter 363, loss=0.31178081035614014 (2.752318s)
INFO:niftynet:2019-02-12 17:38:13,474: training iter 364, loss=0.27390775084495544 (5.390221s)
INFO:niftynet:2019-02-12 17:38:15,896: training iter 365, loss=0.27552470564842224 (2.420922s)
INFO:niftynet:2019-02-12 17:38:17,723: training iter 366, loss=0.2802693843841553 (1.816067s)
INFO:niftynet:2019-02-12 17:38:19,794: training iter 367, loss=0.5032120943069458 (2.070697s)
INFO:niftynet:2019-02-12 17:38:22,312: training iter 368, loss=0.3080822229385376 (2.516613s)
INFO:niftynet:2019-02-12 17:38:27,813: training iter 369, loss=0.25678831338882446 (5.499407s)
INFO:niftynet:2019-02-12 17:38:29,222: training iter 370, loss=0.31991344690322876 (1.409210s)
INFO:niftynet:2019-02-12 17:38:29,869:     validation iter 370, loss=0.3370567560195923 (0.644162s)
INFO:niftynet:2019-02-12 17:38:30,577: training iter 371, loss=0.3068942427635193 (0.705858s)
INFO:niftynet:2019-02-12 17:38:33,284: training iter 372, loss=0.24249592423439026 (2.706742s)
INFO:niftynet:2019-02-12 17:38:37,427: training iter 373, loss=0.25033313035964966 (4.141180s)
INFO:niftynet:2019-02-12 17:38:40,275: training iter 374, loss=0.2935155928134918 (2.847219s)
INFO:niftynet:2019-02-12 17:38:44,216: training iter 375, loss=0.30863258242607117 (3.940124s)
INFO:niftynet:2019-02-12 17:38:45,638: iter 375 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:38:46,303: training iter 376, loss=0.2719497084617615 (0.664526s)
INFO:niftynet:2019-02-12 17:38:48,013: training iter 377, loss=0.23757971823215485 (1.708765s)
INFO:niftynet:2019-02-12 17:38:53,263: training iter 378, loss=0.3387877345085144 (5.250273s)
INFO:niftynet:2019-02-12 17:38:53,894: training iter 379, loss=0.322512149810791 (0.629155s)
INFO:niftynet:2019-02-12 17:38:57,950: training iter 380, loss=0.27795132994651794 (4.052520s)
INFO:niftynet:2019-02-12 17:39:02,275:     validation iter 380, loss=0.25441214442253113 (4.323581s)
INFO:niftynet:2019-02-12 17:39:02,981: training iter 381, loss=0.2636868953704834 (0.705264s)
INFO:niftynet:2019-02-12 17:39:03,837: training iter 382, loss=0.30640947818756104 (0.855020s)
INFO:niftynet:2019-02-12 17:39:05,374: training iter 383, loss=0.32358479499816895 (1.534899s)
INFO:niftynet:2019-02-12 17:39:11,607: training iter 384, loss=0.31479519605636597 (6.232994s)
INFO:niftynet:2019-02-12 17:39:13,506: training iter 385, loss=0.30754703283309937 (1.899118s)
INFO:niftynet:2019-02-12 17:39:14,624: training iter 386, loss=0.2369796633720398 (1.115685s)
INFO:niftynet:2019-02-12 17:39:19,221: training iter 387, loss=0.308973491191864 (4.593883s)
INFO:niftynet:2019-02-12 17:39:22,342: training iter 388, loss=0.2892124652862549 (3.119627s)
INFO:niftynet:2019-02-12 17:39:23,700: training iter 389, loss=0.2665083706378937 (1.355823s)
INFO:niftynet:2019-02-12 17:39:27,292: training iter 390, loss=0.299124538898468 (3.592148s)
INFO:niftynet:2019-02-12 17:39:27,941:     validation iter 390, loss=0.2830754816532135 (0.647294s)
INFO:niftynet:2019-02-12 17:39:28,600: training iter 391, loss=0.2769049108028412 (0.647080s)
INFO:niftynet:2019-02-12 17:39:32,664: training iter 392, loss=0.2923005223274231 (4.063383s)
INFO:niftynet:2019-02-12 17:39:35,970: training iter 393, loss=0.24061158299446106 (3.305120s)
INFO:niftynet:2019-02-12 17:39:40,032: training iter 394, loss=0.275077223777771 (4.062277s)
INFO:niftynet:2019-02-12 17:39:40,781: training iter 395, loss=0.31275397539138794 (0.748220s)
INFO:niftynet:2019-02-12 17:39:42,290: training iter 396, loss=0.28231263160705566 (1.507356s)
INFO:niftynet:2019-02-12 17:39:46,135: training iter 397, loss=0.2763580083847046 (3.843099s)
INFO:niftynet:2019-02-12 17:39:49,729: training iter 398, loss=0.32733210921287537 (3.592558s)
INFO:niftynet:2019-02-12 17:39:52,205: training iter 399, loss=0.24718818068504333 (2.475627s)
INFO:niftynet:2019-02-12 17:39:53,755: training iter 400, loss=0.23891165852546692 (1.549005s)
INFO:niftynet:2019-02-12 17:39:55,560: iter 400 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:39:56,101:     validation iter 400, loss=0.28035929799079895 (0.540476s)
INFO:niftynet:2019-02-12 17:39:57,615: training iter 401, loss=0.25819477438926697 (1.512261s)
INFO:niftynet:2019-02-12 17:39:59,738: training iter 402, loss=0.26494476199150085 (2.122882s)
INFO:niftynet:2019-02-12 17:40:05,083: training iter 403, loss=0.3129611015319824 (5.344787s)
INFO:niftynet:2019-02-12 17:40:06,127: training iter 404, loss=0.3147464692592621 (1.043821s)
INFO:niftynet:2019-02-12 17:40:10,035: training iter 405, loss=0.46305251121520996 (3.907327s)
INFO:niftynet:2019-02-12 17:40:12,127: training iter 406, loss=0.30877238512039185 (2.091681s)
INFO:niftynet:2019-02-12 17:40:15,522: training iter 407, loss=0.36337611079216003 (3.394753s)
INFO:niftynet:2019-02-12 17:40:18,108: training iter 408, loss=0.35001176595687866 (2.514207s)
INFO:niftynet:2019-02-12 17:40:23,026: training iter 409, loss=0.29615360498428345 (4.917127s)
INFO:niftynet:2019-02-12 17:40:23,906: training iter 410, loss=0.2996048927307129 (0.875782s)
INFO:niftynet:2019-02-12 17:40:24,486:     validation iter 410, loss=0.3597928285598755 (0.578577s)
INFO:niftynet:2019-02-12 17:40:25,823: training iter 411, loss=0.2674921154975891 (1.333033s)
INFO:niftynet:2019-02-12 17:40:27,736: training iter 412, loss=0.26209306716918945 (1.912683s)
INFO:niftynet:2019-02-12 17:40:32,681: training iter 413, loss=0.2732451856136322 (4.944731s)
INFO:niftynet:2019-02-12 17:40:34,933: training iter 414, loss=0.29175299406051636 (2.251009s)
INFO:niftynet:2019-02-12 17:40:36,997: training iter 415, loss=0.3269713521003723 (2.060584s)
INFO:niftynet:2019-02-12 17:40:40,166: training iter 416, loss=0.26950740814208984 (3.167612s)
INFO:niftynet:2019-02-12 17:40:43,101: training iter 417, loss=0.24539083242416382 (2.934320s)
INFO:niftynet:2019-02-12 17:40:45,099: training iter 418, loss=0.28365465998649597 (1.997380s)
INFO:niftynet:2019-02-12 17:40:47,526: training iter 419, loss=0.3141639530658722 (2.426601s)
INFO:niftynet:2019-02-12 17:40:49,519: training iter 420, loss=0.3505869507789612 (1.992269s)
INFO:niftynet:2019-02-12 17:40:50,088:     validation iter 420, loss=0.29915228486061096 (0.567471s)
INFO:niftynet:2019-02-12 17:40:53,331: training iter 421, loss=0.22178228199481964 (3.242027s)
INFO:niftynet:2019-02-12 17:40:56,527: training iter 422, loss=0.2535133957862854 (3.195625s)
INFO:niftynet:2019-02-12 17:40:58,575: training iter 423, loss=0.2685322165489197 (2.047961s)
INFO:niftynet:2019-02-12 17:41:02,158: training iter 424, loss=0.38295120000839233 (3.582371s)
INFO:niftynet:2019-02-12 17:41:05,485: training iter 425, loss=0.2907082736492157 (3.325662s)
INFO:niftynet:2019-02-12 17:41:07,550: iter 425 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:41:09,437: training iter 426, loss=0.25405359268188477 (1.876285s)
INFO:niftynet:2019-02-12 17:41:11,154: training iter 427, loss=0.29853177070617676 (1.717098s)
INFO:niftynet:2019-02-12 17:41:14,886: training iter 428, loss=0.3179970979690552 (3.730504s)
INFO:niftynet:2019-02-12 17:41:17,331: training iter 429, loss=0.2707427740097046 (2.444611s)
INFO:niftynet:2019-02-12 17:41:20,982: training iter 430, loss=0.26709264516830444 (3.649526s)
INFO:niftynet:2019-02-12 17:41:21,536:     validation iter 430, loss=0.25447654724121094 (0.552288s)
INFO:niftynet:2019-02-12 17:41:22,324: training iter 431, loss=0.23447883129119873 (0.786102s)
INFO:niftynet:2019-02-12 17:41:26,906: training iter 432, loss=0.2848087549209595 (4.582053s)
INFO:niftynet:2019-02-12 17:41:27,574: training iter 433, loss=0.2914459705352783 (0.667562s)
INFO:niftynet:2019-02-12 17:41:32,136: training iter 434, loss=0.26684263348579407 (4.561364s)
INFO:niftynet:2019-02-12 17:41:33,854: training iter 435, loss=0.25618216395378113 (1.717524s)
INFO:niftynet:2019-02-12 17:41:35,706: training iter 436, loss=0.27293580770492554 (1.850969s)
INFO:niftynet:2019-02-12 17:41:46,672: training iter 437, loss=0.2758815884590149 (10.942346s)
INFO:niftynet:2019-02-12 17:41:49,248: training iter 438, loss=0.49114781618118286 (2.576358s)
INFO:niftynet:2019-02-12 17:41:49,869: training iter 439, loss=0.2788407504558563 (0.620337s)
INFO:niftynet:2019-02-12 17:41:50,588: training iter 440, loss=0.32733145356178284 (0.717163s)
INFO:niftynet:2019-02-12 17:41:51,100:     validation iter 440, loss=0.288697212934494 (0.504024s)
INFO:niftynet:2019-02-12 17:41:56,178: training iter 441, loss=0.32136261463165283 (5.076979s)
INFO:niftynet:2019-02-12 17:42:02,319: training iter 442, loss=0.24526533484458923 (6.118002s)
INFO:niftynet:2019-02-12 17:42:03,026: training iter 443, loss=0.291123628616333 (0.706298s)
INFO:niftynet:2019-02-12 17:42:03,614: training iter 444, loss=0.2350732684135437 (0.587704s)
INFO:niftynet:2019-02-12 17:42:06,561: training iter 445, loss=0.2954533100128174 (2.946698s)
INFO:niftynet:2019-02-12 17:42:09,644: training iter 446, loss=0.3176887035369873 (3.080899s)
INFO:niftynet:2019-02-12 17:42:15,392: training iter 447, loss=0.30056631565093994 (5.747475s)
INFO:niftynet:2019-02-12 17:42:15,975: training iter 448, loss=0.29215097427368164 (0.583037s)
INFO:niftynet:2019-02-12 17:42:19,835: training iter 449, loss=0.2794755697250366 (3.858319s)
INFO:niftynet:2019-02-12 17:42:20,654: training iter 450, loss=0.2889038026332855 (0.818612s)
INFO:niftynet:2019-02-12 17:42:22,572: iter 450 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:42:23,118:     validation iter 450, loss=0.3386852741241455 (0.544846s)
INFO:niftynet:2019-02-12 17:42:27,237: training iter 451, loss=0.286385178565979 (4.116592s)
INFO:niftynet:2019-02-12 17:42:27,906: training iter 452, loss=0.28932809829711914 (0.668671s)
INFO:niftynet:2019-02-12 17:42:29,576: training iter 453, loss=0.2722355127334595 (1.669829s)
INFO:niftynet:2019-02-12 17:42:33,530: training iter 454, loss=0.31336671113967896 (3.953500s)
INFO:niftynet:2019-02-12 17:42:36,751: training iter 455, loss=0.2355339378118515 (3.220418s)
INFO:niftynet:2019-02-12 17:42:38,298: training iter 456, loss=0.2665627896785736 (1.542122s)
INFO:niftynet:2019-02-12 17:42:39,202: training iter 457, loss=0.27070343494415283 (0.903442s)
INFO:niftynet:2019-02-12 17:42:44,567: training iter 458, loss=0.2695315480232239 (5.333613s)
INFO:niftynet:2019-02-12 17:42:47,092: training iter 459, loss=0.29411640763282776 (2.524678s)
INFO:niftynet:2019-02-12 17:42:48,299: training iter 460, loss=0.25144296884536743 (1.206896s)
INFO:niftynet:2019-02-12 17:42:48,939:     validation iter 460, loss=0.30002766847610474 (0.638828s)
INFO:niftynet:2019-02-12 17:42:50,032: training iter 461, loss=0.23201468586921692 (1.091467s)
INFO:niftynet:2019-02-12 17:42:56,583: training iter 462, loss=0.20748037099838257 (6.549315s)
INFO:niftynet:2019-02-12 17:42:58,588: training iter 463, loss=0.2654985189437866 (2.005555s)
INFO:niftynet:2019-02-12 17:42:59,832: training iter 464, loss=0.40145379304885864 (1.243092s)
INFO:niftynet:2019-02-12 17:43:02,604: training iter 465, loss=0.3189219832420349 (2.771818s)
INFO:niftynet:2019-02-12 17:43:08,394: training iter 466, loss=0.2723713517189026 (5.788727s)
INFO:niftynet:2019-02-12 17:43:08,980: training iter 467, loss=0.28277820348739624 (0.584725s)
INFO:niftynet:2019-02-12 17:43:12,630: training iter 468, loss=0.31628116965293884 (3.649183s)
INFO:niftynet:2019-02-12 17:43:13,968: training iter 469, loss=0.2476511150598526 (1.324211s)
INFO:niftynet:2019-02-12 17:43:16,945: training iter 470, loss=0.24362434446811676 (2.971983s)
INFO:niftynet:2019-02-12 17:43:17,558:     validation iter 470, loss=0.3078031539916992 (0.611421s)
INFO:niftynet:2019-02-12 17:43:21,175: training iter 471, loss=0.33904439210891724 (3.615621s)
INFO:niftynet:2019-02-12 17:43:22,510: training iter 472, loss=0.30013757944107056 (1.332995s)
INFO:niftynet:2019-02-12 17:43:25,301: training iter 473, loss=0.3022703528404236 (2.790992s)
INFO:niftynet:2019-02-12 17:43:29,141: training iter 474, loss=0.3195664882659912 (3.839479s)
INFO:niftynet:2019-02-12 17:43:33,909: training iter 475, loss=0.31782716512680054 (4.767330s)
INFO:niftynet:2019-02-12 17:43:35,451: iter 475 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:43:36,050: training iter 476, loss=0.30258268117904663 (0.597393s)
INFO:niftynet:2019-02-12 17:43:36,650: training iter 477, loss=0.2147843986749649 (0.598898s)
INFO:niftynet:2019-02-12 17:43:39,816: training iter 478, loss=0.26729488372802734 (3.165869s)
INFO:niftynet:2019-02-12 17:43:43,635: training iter 479, loss=0.28486672043800354 (3.811299s)
INFO:niftynet:2019-02-12 17:43:46,947: training iter 480, loss=0.31704604625701904 (3.311537s)
INFO:niftynet:2019-02-12 17:43:47,606:     validation iter 480, loss=0.4409582018852234 (0.657667s)
INFO:niftynet:2019-02-12 17:43:48,441: training iter 481, loss=0.2793024480342865 (0.833436s)
INFO:niftynet:2019-02-12 17:43:49,067: training iter 482, loss=0.22073447704315186 (0.625102s)
INFO:niftynet:2019-02-12 17:43:56,282: training iter 483, loss=0.35708025097846985 (7.215287s)
INFO:niftynet:2019-02-12 17:43:59,346: training iter 484, loss=0.2426515817642212 (3.062665s)
INFO:niftynet:2019-02-12 17:44:01,021: training iter 485, loss=0.2881300449371338 (1.675214s)
INFO:niftynet:2019-02-12 17:44:02,263: training iter 486, loss=0.31932127475738525 (1.239680s)
INFO:niftynet:2019-02-12 17:44:02,961: training iter 487, loss=0.25227129459381104 (0.696882s)
INFO:niftynet:2019-02-12 17:44:09,926: training iter 488, loss=0.30860671401023865 (6.964147s)
INFO:niftynet:2019-02-12 17:44:12,841: training iter 489, loss=0.3212127089500427 (2.913911s)
INFO:niftynet:2019-02-12 17:44:15,617: training iter 490, loss=0.2559410035610199 (2.775114s)
INFO:niftynet:2019-02-12 17:44:16,249:     validation iter 490, loss=0.27799010276794434 (0.631232s)
INFO:niftynet:2019-02-12 17:44:16,907: training iter 491, loss=0.2601550817489624 (0.656578s)
INFO:niftynet:2019-02-12 17:44:19,738: training iter 492, loss=0.3150908350944519 (2.827625s)
INFO:niftynet:2019-02-12 17:44:24,503: training iter 493, loss=0.28483879566192627 (4.764029s)
INFO:niftynet:2019-02-12 17:44:25,180: training iter 494, loss=0.2704029977321625 (0.675776s)
INFO:niftynet:2019-02-12 17:44:27,880: training iter 495, loss=0.2910999655723572 (2.698219s)
INFO:niftynet:2019-02-12 17:44:29,705: training iter 496, loss=0.2605580687522888 (1.823436s)
INFO:niftynet:2019-02-12 17:44:33,784: training iter 497, loss=0.4620475172996521 (4.077742s)
INFO:niftynet:2019-02-12 17:44:36,968: training iter 498, loss=0.31321609020233154 (3.183384s)
INFO:niftynet:2019-02-12 17:44:39,931: training iter 499, loss=0.29005324840545654 (2.962259s)
INFO:niftynet:2019-02-12 17:44:40,710: training iter 500, loss=0.4023255407810211 (0.778088s)
INFO:niftynet:2019-02-12 17:44:42,618: iter 500 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:44:43,152:     validation iter 500, loss=0.2619521915912628 (0.531669s)
INFO:niftynet:2019-02-12 17:44:44,352: training iter 501, loss=0.29536938667297363 (1.196513s)
INFO:niftynet:2019-02-12 17:44:48,405: training iter 502, loss=0.24403399229049683 (4.052817s)
INFO:niftynet:2019-02-12 17:44:53,987: training iter 503, loss=0.3769204318523407 (5.582507s)
INFO:niftynet:2019-02-12 17:44:54,683: training iter 504, loss=0.28468209505081177 (0.694599s)
INFO:niftynet:2019-02-12 17:44:55,933: training iter 505, loss=0.2747056484222412 (1.246248s)
INFO:niftynet:2019-02-12 17:44:59,310: training iter 506, loss=0.2633698880672455 (3.375602s)
INFO:niftynet:2019-02-12 17:45:01,777: training iter 507, loss=0.32310640811920166 (2.454728s)
INFO:niftynet:2019-02-12 17:45:06,093: training iter 508, loss=0.32341885566711426 (4.315815s)
INFO:niftynet:2019-02-12 17:45:06,747: training iter 509, loss=0.2682134509086609 (0.652679s)
INFO:niftynet:2019-02-12 17:45:10,871: training iter 510, loss=0.3019522428512573 (4.123535s)
INFO:niftynet:2019-02-12 17:45:11,444:     validation iter 510, loss=0.3386700749397278 (0.572396s)
INFO:niftynet:2019-02-12 17:45:12,633: training iter 511, loss=0.29626286029815674 (1.187240s)
INFO:niftynet:2019-02-12 17:45:17,185: training iter 512, loss=0.23111209273338318 (4.551729s)
INFO:niftynet:2019-02-12 17:45:19,325: training iter 513, loss=0.26836520433425903 (2.137965s)
INFO:niftynet:2019-02-12 17:45:19,989: training iter 514, loss=0.3453940749168396 (0.663560s)
INFO:niftynet:2019-02-12 17:45:24,074: training iter 515, loss=0.2767539620399475 (4.084943s)
INFO:niftynet:2019-02-12 17:45:29,133: training iter 516, loss=0.31820595264434814 (5.058071s)
INFO:niftynet:2019-02-12 17:45:30,662: training iter 517, loss=0.2629240155220032 (1.527916s)
INFO:niftynet:2019-02-12 17:45:31,285: training iter 518, loss=0.2669844925403595 (0.621563s)
INFO:niftynet:2019-02-12 17:45:32,326: training iter 519, loss=0.31818869709968567 (1.040958s)
INFO:niftynet:2019-02-12 17:45:38,338: training iter 520, loss=0.3316994905471802 (5.987785s)
INFO:niftynet:2019-02-12 17:45:42,151:     validation iter 520, loss=0.262953519821167 (3.807081s)
INFO:niftynet:2019-02-12 17:45:44,414: training iter 521, loss=0.31171053647994995 (2.260458s)
INFO:niftynet:2019-02-12 17:45:45,190: training iter 522, loss=0.29869285225868225 (0.775854s)
INFO:niftynet:2019-02-12 17:45:45,781: training iter 523, loss=0.30826371908187866 (0.588471s)
INFO:niftynet:2019-02-12 17:45:50,650: training iter 524, loss=0.2630384862422943 (4.868350s)
INFO:niftynet:2019-02-12 17:45:54,872: training iter 525, loss=0.31489789485931396 (4.219878s)
INFO:niftynet:2019-02-12 17:45:56,612: iter 525 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:45:57,427: training iter 526, loss=0.30110815167427063 (0.814345s)
INFO:niftynet:2019-02-12 17:45:58,135: training iter 527, loss=0.255770206451416 (0.706595s)
INFO:niftynet:2019-02-12 17:45:59,083: training iter 528, loss=0.3383268713951111 (0.947749s)
WARNING:niftynet:2019-02-12 17:46:08,176: User cancelled application
INFO:niftynet:2019-02-12 17:46:08,178: cleaning up...
INFO:niftynet:2019-02-12 17:46:09,849: iter 529 saved: /home/sathiesh/niftynet_brain/models/brats/models/model.ckpt
INFO:niftynet:2019-02-12 17:46:09,849: stopping sampling threads
WARNING:niftynet:2019-02-12 17:46:21,860: stopped early, incomplete iterations.
INFO:niftynet:2019-02-12 17:46:21,860: SegmentationApplication stopped (time in second 1627.25).
INFO:niftynet:2019-02-12 18:20:00,664: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 18:20:00,664: starting segmentation application
INFO:niftynet:2019-02-12 18:20:00,664: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 18:20:00,670: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 18:20:00,674: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/binary_labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 18:20:00,678: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 18:20:00,680: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 18:20:27,801: Image reader: loading 227 subjects from sections ('T1', 'T2') as input [image]
INFO:niftynet:2019-02-12 18:20:27,802: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 18:20:30,691: Image reader: loading 29 subjects from sections ('T1', 'T2') as input [image]
INFO:niftynet:2019-02-12 18:20:30,692: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 18:20:30,693: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 18:20:30,694: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 18:20:32,211: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 2), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 18:20:32,264: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 2), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
WARNING:niftynet:2019-02-12 18:20:32,377: From /home/sathiesh/niftynet_brain/NiftyNet/niftynet/engine/application_initializer.py:106: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
INFO:niftynet:2019-02-12 18:20:32,378: using DenseVNet
INFO:niftynet:2019-02-12 18:20:32,385: Initialising Dataset from 227 subjects...
INFO:niftynet:2019-02-12 18:20:32,436: Initialising Dataset from 29 subjects...
INFO:niftynet:2019-02-12 18:20:45,372: Parameters from random initialisations ...
INFO:niftynet:2019-02-12 18:21:37,086: training iter 1, loss=0.5983183979988098 (51.393713s)
INFO:niftynet:2019-02-12 18:21:39,778: training iter 2, loss=0.36823979020118713 (2.692304s)
INFO:niftynet:2019-02-12 18:21:42,596: training iter 3, loss=0.4821213483810425 (2.817845s)
INFO:niftynet:2019-02-12 18:21:44,954: training iter 4, loss=0.4162382185459137 (2.347293s)
INFO:niftynet:2019-02-12 18:21:56,310: training iter 5, loss=0.4793062210083008 (11.354772s)
INFO:niftynet:2019-02-12 18:21:58,990: training iter 6, loss=0.3509295582771301 (2.670575s)
INFO:niftynet:2019-02-12 18:22:01,367: training iter 7, loss=0.27041172981262207 (2.360395s)
INFO:niftynet:2019-02-12 18:22:03,779: training iter 8, loss=0.3585248589515686 (2.409201s)
INFO:niftynet:2019-02-12 18:22:06,326: training iter 9, loss=0.44085395336151123 (2.546835s)
INFO:niftynet:2019-02-12 18:22:08,862: training iter 10, loss=0.47447121143341064 (2.534202s)
INFO:niftynet:2019-02-12 18:22:29,672:     validation iter 10, loss=0.42512208223342896 (20.803552s)
INFO:niftynet:2019-02-12 18:22:31,693: training iter 11, loss=0.30250170826911926 (2.021453s)
INFO:niftynet:2019-02-12 18:22:33,627: training iter 12, loss=0.25826066732406616 (1.933685s)
INFO:niftynet:2019-02-12 18:22:35,828: training iter 13, loss=0.4787195920944214 (2.199844s)
INFO:niftynet:2019-02-12 18:22:38,250: training iter 14, loss=0.46841371059417725 (2.421696s)
INFO:niftynet:2019-02-12 18:22:44,499: training iter 15, loss=0.37529227137565613 (6.246454s)
INFO:niftynet:2019-02-12 18:22:46,778: training iter 16, loss=0.4303472936153412 (2.271332s)
INFO:niftynet:2019-02-12 18:22:48,831: training iter 17, loss=0.4561629891395569 (2.023904s)
INFO:niftynet:2019-02-12 18:22:51,102: training iter 18, loss=0.28730446100234985 (2.270928s)
INFO:niftynet:2019-02-12 18:22:53,795: training iter 19, loss=0.514297366142273 (2.692365s)
INFO:niftynet:2019-02-12 18:22:56,573: training iter 20, loss=0.473124235868454 (2.777161s)
INFO:niftynet:2019-02-12 18:22:58,730:     validation iter 20, loss=0.4537620544433594 (2.151177s)
INFO:niftynet:2019-02-12 18:23:01,035: training iter 21, loss=0.46909838914871216 (2.302093s)
INFO:niftynet:2019-02-12 18:23:03,105: training iter 22, loss=0.3721272051334381 (2.069029s)
INFO:niftynet:2019-02-12 18:23:05,311: training iter 23, loss=0.42799365520477295 (2.205295s)
INFO:niftynet:2019-02-12 18:23:07,361: training iter 24, loss=0.26361173391342163 (2.049773s)
INFO:niftynet:2019-02-12 18:23:09,814: training iter 25, loss=0.4384191632270813 (2.450632s)
INFO:niftynet:2019-02-12 18:23:09,958: cleaning up...
INFO:niftynet:2019-02-12 18:25:18,816: set CUDA_VISIBLE_DEVICES to 0
INFO:niftynet:2019-02-12 18:25:18,816: starting segmentation application
INFO:niftynet:2019-02-12 18:25:18,816: [T1] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t1.csv, skipped filenames search
INFO:niftynet:2019-02-12 18:25:18,822: [T2] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/orig_t2.csv, skipped filenames search
INFO:niftynet:2019-02-12 18:25:18,826: [label] using existing csv file /home/sathiesh/niftynet_brain/csv/brats/binary_labels.csv, skipped filenames search
WARNING:niftynet:2019-02-12 18:25:18,830: Loading from existing partitioning file /home/sathiesh/niftynet_brain/models/brats/dataset_split.csv, ignoring partitioning ratios.
INFO:niftynet:2019-02-12 18:25:18,832: 

Number of subjects 285, input section names: ['subject_id', 'T1', 'T2', 'label']
Dataset partitioning:
-- training 227 cases (79.65%),
-- validation 29 cases (10.18%),
-- inference 29 cases (10.18%).

INFO:niftynet:2019-02-12 18:25:33,439: Image reader: loading 227 subjects from sections ('T1', 'T2') as input [image]
INFO:niftynet:2019-02-12 18:25:33,439: Image reader: loading 227 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 18:25:34,942: Image reader: loading 29 subjects from sections ('T1', 'T2') as input [image]
INFO:niftynet:2019-02-12 18:25:34,942: Image reader: loading 29 subjects from sections ('label',) as input [label]
INFO:niftynet:2019-02-12 18:25:34,944: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 18:25:34,944: label mapping ready for label:('label',), 4 classes
INFO:niftynet:2019-02-12 18:25:36,053: initialised uniform sampler {'image': (1, 128, 128, 128, 1, 2), 'image_location': (1, 7), 'label': (1, 128, 128, 128, 1, 1), 'label_location': (1, 7)} 
INFO:niftynet:2019-02-12 18:25:36,091: initialised uniform sampler